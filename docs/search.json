[
  {
    "objectID": "10_Practicle_forecasting_issues.html",
    "href": "10_Practicle_forecasting_issues.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Practicle forecasting issues\n\n1+2\n\n[1] 3"
  },
  {
    "objectID": "12_data_appendix.html",
    "href": "12_data_appendix.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Data\nYou can find a overview of the data that is included in the textbook package fpp3 by running the following code:\n\ndata(package = \"fpp3\")\n\n\n\n# A tibble: 13 × 2\n   Item              Title                                               \n   <chr>             <chr>                                               \n 1 aus_accommodation Australian accommodation data                       \n 2 aus_airpassengers Air Transport Passengers Australia                  \n 3 aus_arrivals      International Arrivals to Australia                 \n 4 bank_calls        Call volume for a large North American bank         \n 5 boston_marathon   Boston marathon winning times since 1897            \n 6 canadian_gas      Monthly Canadian gas production                     \n 7 guinea_rice       Rice production (Guinea)                            \n 8 insurance         Insurance quotations and advertising expenditure    \n 9 prices            Price series for various commodities                \n10 souvenirs         Sales for a souvenir shop                           \n11 us_change         Percentage changes in economic variables in the USA.\n12 us_employment     US monthly employment data                          \n13 us_gasoline       US finished motor gasoline product supplied.        \n\n\nThese are avaiable when the fpp3 package is loaded, i.e.\n\nlibrary(fpp3)\nbank_calls\n\n# A tsibble: 27,716 x 2 [5m] <UTC>\n   DateTime            Calls\n   <dttm>              <dbl>\n 1 2003-03-03 07:00:00   111\n 2 2003-03-03 07:05:00   113\n 3 2003-03-03 07:10:00    76\n 4 2003-03-03 07:15:00    82\n 5 2003-03-03 07:20:00    91\n 6 2003-03-03 07:25:00    87\n 7 2003-03-03 07:30:00    75\n 8 2003-03-03 07:35:00    89\n 9 2003-03-03 07:40:00    99\n10 2003-03-03 07:45:00   125\n# … with 27,706 more rows\n\n\nTo load a specific data set explicitly in your R environment:\n\ndata(\"bank_calls\")\n\nOther examples used in the videos and content on this website is available for download at github.com/holleland/BAN430/tree/master/data. You should also be able to load them directly into R using the raw link:\n\n# CPI Norway\nread.csv(\n  \"https://raw.githubusercontent.com/holleland/BAN430/master/data/CPI_norway.csv\", sep = \";\") %>% head()\n\n     X Y.avg2   Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov\n1 2022      . 117.8 119.1 119.8 121.2 121.5 122.6 124.2 123.9 125.6     .     .\n2 2021  116.1 114.1 114.9 114.6 115.0 114.9 115.3 116.3 116.3 117.5 117.2 118.1\n3 2020  112.2 111.3 111.2 111.2 111.7 111.9 112.1 112.9 112.5 112.9 113.2 112.4\n4 2019  110.8 109.3 110.2 110.4 110.8 110.5 110.6 111.4 110.6 111.1 111.3 111.6\n5 2018  108.4 106.0 107.0 107.3 107.7 107.8 108.5 109.3 108.9 109.5 109.3 109.8\n6 2017  105.5 104.3 104.7 105.0 105.2 105.4 105.8 106.1 105.3 105.9 106.0 106.1\n    Dec\n1     .\n2 118.9\n3 112.9\n4 111.3\n5 109.8\n6 106.1\n\n\nFor excel files it is a bit less convenient, because you will need to download the file. But you can let R do that for you (if you insist).\n\nloadExcel_url <- function(url) {\n    temp_file <- tempfile(fileext = \".xlsx\")\n    download.file(url = url, destfile = temp_file, mode = \"wb\", quiet = TRUE)\n    readxl::read_excel(temp_file)\n}\nloadExcel_url(\"https://github.com/holleland/BAN430/blob/master/data/NorwayEmployment_15-74years_bySex.xlsx?raw=true\")\n\n# A tibble: 214 × 3\n   Sex   Quarter `Employed persons (1 000 persons)`\n   <chr> <chr>                                <dbl>\n 1 Male  1996K1                                1133\n 2 Male  1996K2                                1152\n 3 Male  1996K3                                1171\n 4 Male  1996K4                                1161\n 5 Male  1997K1                                1164\n 6 Male  1997K2                                1189\n 7 Male  1997K3                                1200\n 8 Male  1997K4                                1189\n 9 Male  1998K1                                1195\n10 Male  1998K2                                1214\n# … with 204 more rows\n\n\nThe code above will save the file temporary in your computers temporary folder and load it into R from there. You could also adjust the code so that it stores the file in your working directory by adjusting the function.\n\ntemp_file <- paste0(getwd(), \"/NorwayEmployment.xlsx\")\n\nBut the easiest will maybe be to just download the files manually from github and save them in a data folder of your own."
  },
  {
    "objectID": "1_1_readingData.html",
    "href": "1_1_readingData.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "This is a book created from markdown and executable code."
  },
  {
    "objectID": "1_1_readingData.html#exercises",
    "href": "1_1_readingData.html#exercises",
    "title": "BAN430 Forecasting",
    "section": "Exercises",
    "text": "Exercises\nIs 1+1 the same as 2?\n\n\nSolution\n\n\n1+1 == 2"
  },
  {
    "objectID": "1_installingrandrstudio.html",
    "href": "1_installingrandrstudio.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "R and Rstudio\nIn this course, we will be using R and Rstudio to e.g. visualize time series, estimate model parameters, forecast, etc. It is therefore essential to have some basic knowledge of how to write an R script and how to read in data and do some simple data manipulation for preparing the data for different time series analysis. Hopefully, most of you have some experience with R and Rstudio before. If you have, this will be a short recap, if not this will be a very short introduction covering the most basic operations.\n\nInstalling R and Rstudio\n\nInstall R:\n\nGo to: cran.uib.no\nPress download R for Linux/MacOS/Windows\nPress base\nDownload R-4.x.x for Linux/MacOS/Windows\nRun the installation using default options\n\nInstall Rstudio\n\nGo to: rstudio.com\nSelect Rstudio desktop\nPress Download Rstudio desktop\nSelect the Rstudio desktop with open source licence, which is free\n\nSelect the version for your operating system\nRun the installation using default settings\n\nOpen Rstudio and check that it works (it should start without any error messages).\nInstall the R-package of the book “fpp3”.\n\nIn Rstudio, select Tools -> Install packages -> write “fpp3” and make sure install dependencies is marked. Press Install. You can also run the following code in the console\n\n\n\ninstall.packages(\"fpp3\", dependencies = TRUE)\n\n\n\nOther useful packages to install are\n\ninstall.packages(\"tidyverse\", dependencies = TRUE)\ninstall.packages(\"readxl\")\n\n(will add other packages as we go)"
  },
  {
    "objectID": "1_randrstudio.html",
    "href": "1_randrstudio.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "In this course, we will be using R and Rstudio to e.g. visualize time series, estimate model parameters, forecast, etc. It is therefore essential to have some basic knowledge of how to write an R script and how to read in data and do some simple data manipulation for preparing the data for different time series analysis. Hopefully, most of you have some experience with R and Rstudio before. If you have, this will be a short recap, if not this will be a very short introduction covering the most basic operations.\n\n\n\nInstall R:\n\nGo to: cran.uib.no\nPress download R for Linux/MacOS/Windows\nPress base\nDownload R-4.x.x for Linux/MacOS/Windows\nRun the installation using default options\n\nInstall Rstudio\n\nGo to: rstudio.com\nSelect Rstudio desktop\nPress Download Rstudio desktop\nSelect the Rstudio desktop with open source licence, which is free\n\nSelect the version for your operating system\nRun the installation using default settings\n\nOpen Rstudio and check that it works (it should start without any error messages).\nInstall the R-package of the book “fpp3”.\n\nIn Rstudio, select Tools -> Install packages -> write “fpp3” and make sure install dependencies is marked. Press Install. You can also run the following code in the console\n\n\n\ninstall.packages(\"fpp3\", dependencies = TRUE)\n\n\n\nOther useful packages to install are\n\ninstall.packages(\"tidyverse\", dependencies = TRUE)\ninstall.packages(\"readxl\")\n\n(will add other packages as we go)\n\n\n\nIf R and Rstudio are completely new tools for you, this section will probably not be detailed enough to get you started, but fear not. There are lots of good and useful online material for learning basic R. One possibility is to work through the first section of (chapter 1-8) of the book R for data science by @wickham2016r available online. There is also a free Coursera course on R programming, recommended by the textbook authors.\nWe will mostly be using the tidyverse approach to doing data manipulation. This is in line with what you learn in courses like BAN400 R programming for Data Science or BAN420 Introduction to R and also with what the authors of the textbook does (@hyndman2018).\nSay you are given an .xlsx file (MS excel format) of daily prices of an US 10 year Treasury bond. The excel file contains several sheets with the\n\nClosing ask price (“Ask”)\nClosing bid price (“Bid”)\nClosing mid price (“Mid”)\n\nEach contains two columns: date and price. In the figure below we have taken a screen shot of the Mid sheet.\n\n\n\nUS 10-year Treasury bonds index collected from the Refinitiv Eikon data base.\n\n\nYou are interesting in reading in the closing mid price. To read in this data, you may use the following code.\n\nlibrary(fpp3)   # loading textbook package\nlibrary(tidyverse)\nlibrary(readxl) # loading package for reading excel files\ndat <- read_excel(\"data/US10YTRR.xlsx\", sheet = \"Mid\")  \nhead(dat) # printing out the first 6 rows\n\n# A tibble: 6 × 2\n  date                price\n  <dttm>              <dbl>\n1 2022-08-30 00:00:00  96.9\n2 2022-08-29 00:00:00  96.9\n3 2022-08-26 00:00:00  97.6\n4 2022-08-25 00:00:00  97.6\n5 2022-08-24 00:00:00  96.9\n6 2022-08-23 00:00:00  97.4\n\n\nThe sheet argument specifies which sheet in the excel file we want to read. The read_excel function is also quite smart so it recognizes that the date column is a date and automatically format it accordingly. It is however perhaps not so useful to also include the time of the day (all is 00:00:00), so let us remove this part.\n\ndat %>% \n  mutate(date = as.Date(date))\n\n# A tibble: 8,804 × 2\n   date       price\n   <date>     <dbl>\n 1 2022-08-30  96.9\n 2 2022-08-29  96.9\n 3 2022-08-26  97.6\n 4 2022-08-25  97.6\n 5 2022-08-24  96.9\n 6 2022-08-23  97.4\n 7 2022-08-22  97.7\n 8 2022-08-19  98.1\n 9 2022-08-18  98.8\n10 2022-08-17  98.7\n# … with 8,794 more rows\n\n\nHere I have used the mutate function. This is a function we use to either mutate an existing column or create a new one. In this case we mutated the date column transforming it to a “Date” object. We could also be intersted in adding a column for which year the observation is from.\n\ndat %>% \n  mutate(date = as.Date(date),\n         year = year(date))\n\n# A tibble: 8,804 × 3\n   date       price  year\n   <date>     <dbl> <dbl>\n 1 2022-08-30  96.9  2022\n 2 2022-08-29  96.9  2022\n 3 2022-08-26  97.6  2022\n 4 2022-08-25  97.6  2022\n 5 2022-08-24  96.9  2022\n 6 2022-08-23  97.4  2022\n 7 2022-08-22  97.7  2022\n 8 2022-08-19  98.1  2022\n 9 2022-08-18  98.8  2022\n10 2022-08-17  98.7  2022\n# … with 8,794 more rows\n\n\nHere we have used the year function from the lubridate package, which is loaded with the fpp3 package. The operator %>% is used to add operations to the data manipulation pipeline in the given order. We start with the data object (a tibble) and add a mutate operation to that where we first transform the date column and add a year column. Now that we are pleased with our pipeline, let us save this to the dat object.\n\ndat <- dat %>% \n  mutate(date = as.Date(date),\n         year = year(date))\ndat %>% glimpse()\n\nRows: 8,804\nColumns: 3\n$ date  <date> 2022-08-30, 2022-08-29, 2022-08-26, 2022-08-25, 2022-08-24, 202…\n$ price <dbl> 96.92969, 96.92188, 97.62500, 97.60156, 96.94531, 97.38281, 97.6…\n$ year  <dbl> 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022…\n\n\nThe glimpse function summarizes the tibble/data frame.\n\n\nNow, the data ranges from/to\n\nrange(dat$date)\n\n[1] \"1987-08-03\" \"2022-08-30\"\n\n\nbut say you only want to use data from 2010 onwards. To do this, we use the filter function. This function is useful for selecting rows that fulfil some condition, in this case year >= 2010. Let us make a pipeline for this\n\ndat %>% \n  filter(year >= 2010)\n\n# A tibble: 3,178 × 3\n   date       price  year\n   <date>     <dbl> <dbl>\n 1 2022-08-30  96.9  2022\n 2 2022-08-29  96.9  2022\n 3 2022-08-26  97.6  2022\n 4 2022-08-25  97.6  2022\n 5 2022-08-24  96.9  2022\n 6 2022-08-23  97.4  2022\n 7 2022-08-22  97.7  2022\n 8 2022-08-19  98.1  2022\n 9 2022-08-18  98.8  2022\n10 2022-08-17  98.7  2022\n# … with 3,168 more rows\n\n\nSince 2022 is not a complete year (in the data), you also don’t want observations after 2021. Then you can add this as an extra condition.\n\ndat %>% \n  filter(year >= 2010, year <=2021)\n\n# A tibble: 3,012 × 3\n   date       price  year\n   <date>     <dbl> <dbl>\n 1 2021-12-31  98.8  2021\n 2 2021-12-30  98.8  2021\n 3 2021-12-29  98.4  2021\n 4 2021-12-28  99.0  2021\n 5 2021-12-27  99.1  2021\n 6 2021-12-23  98.9  2021\n 7 2021-12-22  99.3  2021\n 8 2021-12-21  99.2  2021\n 9 2021-12-20  99.5  2021\n10 2021-12-17  99.7  2021\n# … with 3,002 more rows\n\n\nAlternatively, you can use the between function\n\ndat %>% \n  filter(between(year, 2010, 2021))\n\nwhich will produce the same result. Another useful function is called select. While filter is used on the rows of your data, select is for columns. Say we don’t need the year column after having filtered out the years we don’t want. We can then either select the columns we want to keep\n\ndat %>% \n  filter(between(year, 2010, 2021)) %>%\n  select(date, price)\n\nor remove the columns we do not want\n\ndat %>% \n  filter(between(year, 2010, 2021)) %>%\n  select(-year)\n\n# A tibble: 3,012 × 2\n   date       price\n   <date>     <dbl>\n 1 2021-12-31  98.8\n 2 2021-12-30  98.8\n 3 2021-12-29  98.4\n 4 2021-12-28  99.0\n 5 2021-12-27  99.1\n 6 2021-12-23  98.9\n 7 2021-12-22  99.3\n 8 2021-12-21  99.2\n 9 2021-12-20  99.5\n10 2021-12-17  99.7\n# … with 3,002 more rows\n\n\n\n\n\nSay we are interested in calculating the yearly mean price. In the tidyverse pipeline this means we want to group our observations according to year and summarize by year the mean of the observations. We will filter to avoid having the first and last years that are incomplete.\n\ndat %>% \n  filter(between(year, 1988, 2021)) %>%\n  group_by(year) %>%\n  summarize(meanPrice = mean(price))\n\n# A tibble: 34 × 2\n    year meanPrice\n   <dbl>     <dbl>\n 1  1988      99.7\n 2  1989     101. \n 3  1990     100. \n 4  1991     100. \n 5  1992     100. \n 6  1993     102. \n 7  1994      98.3\n 8  1995     102. \n 9  1996      99.8\n10  1997      99.9\n# … with 24 more rows\n\n\nThis pipeline could be read as first we take out observations prior to 1988 and after 2021, then we group the observations according to year and summarize the mean price by year. Note that this operation will delete any columns that are not in the group_by or being calculated in the summarize.\n\n\n\nPlotting a data frame is convenient to do using the ggplot2 package. This will (when used appropriately) produce beautiful figures. Let us plot the time series at hand. The ggplot2 follows the same logic with a pipeline, but instead of the %>% operator, we add elements to the figure using +. We need to specify the data object and the name of the x and y columns to be plotted. Everything in the figure that is to vary based on values in the data frame needs to be wrapped in a aes (aesthetic) function (here the x and y arguments). By adding the geom_line() we insert a line.\n\nggplot(data = dat, \n       aes(x=date, y = price)) +\n  geom_line()\n\n\n\n\nWe could instead add geom_point()\n\n ggplot(data = dat,\n        aes(x=date, y = price)) +\n   geom_point()\n\n\n\n\nor do both\n\n ggplot(data = dat,\n        aes(x=date, y = price)) +\n   geom_line() +\n   geom_point()\n\n\n\n\nWe can change the colors and decrease the size of the points:\n\n ggplot(data = dat,\n        aes(x=date, y = price)) +\n   geom_line(color = \"blue\") +\n   geom_point(color = \"green\", size = .2)\n\n\n\n\nOr maybe we do not want to use the default theme: –>\n\n ggplot(data = dat,\n        aes(x=date, y = price)) +\n   geom_line(color = \"blue\") +\n   geom_point(color = \"green\", size = .2) +\n   theme_bw()\n\n\n\n\nWe can also include the plotting in our data manipulation pipeline. For instance, lets summarize the data by year and plot the resulting yearly time series.\n\ndat %>%\n  filter(between(year, 1988, 2021)) %>%\n  group_by(year) %>%\n  summarize(meanPrice = mean(price)) %>%\n  # adding plotting to pipeline:\n  ggplot(aes(x=year, y = meanPrice)) +\n  geom_line(color = \"blue\") +\n  geom_point(color = \"green\") +\n  theme_bw()\n\n\n\n\n\n\n\nWe cannot illustrate all aspects here, but you will learn new elements by studying examples throughout the course. This recap is mostly for remembering the basics of data manipulation in R and simple plotting. As you will see in the continuation, the coding is not much more complex then what you have seen here and the fpp3 package uses the same type of logic and syntax as the tidyverse. There will however be some new functions specific for time series analysis that you will need to learn.\n\n\n\n\nSet working directory.\nLoad the data.\nFilter away observations prior to 2010.\nRemove columns except .. and time\nSummarize data to monthly means\nMake a plot with time on x-axis and monthly means on y-axis\nSave the figure to file."
  },
  {
    "objectID": "1_rrecap.html",
    "href": "1_rrecap.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "If R and Rstudio are completely new tools for you, this section will probably not be detailed enough to get you started, but fear not. There are lots of good and useful online material for learning basic R. One possibility is to work through the first section (chapter 1-8) of the book R for data science by Wickham and Grolemund available online. There is also a free Coursera course on R programming, recommended by the textbook authors. The course BAN420 is recommended for taking this course. If you have not completed that course, you will find the material as videos on the BAN420 website. I recommend going through this material if you do not know it already. What follows below is a (very) short version of parts of Tuesday-Wednesday.\nWe will be using the tidyverse approach to doing data manipulation. This is in line with what you learn in courses like BAN400 R programming for Data Science or BAN420 Introduction to R and also with what the authors of the textbook does (Hyndman and Athanasopoulos,2021).\nOn canvas, you will find an excel file called US10YTRR.xlsx in the R recap folder. If you have experience with R, try to solve the following exercise without looking at the suggested solution.\n\n\nYou are given an .xlsx file (MS excel format) of daily prices of an US 10 year Treasury bond. The excel file contains several sheets with the\n\nClosing ask price (“Ask”)\nClosing bid price (“Bid”)\nClosing mid price (“Mid”)\n\nEach contains two columns: date and price. In the figure below we have taken a screen shot of the Mid sheet.\n\n\n\nUS 10-year Treasury bonds index collected from the Refinitiv Eikon data base.\n\n\nDo the following:\n\nSet working directory.\nLoad the data for the Mid price.\nFormat the date column in Date format.\nAdd a column for which year the observation is from.\nFilter away observations prior to 2010 and after 2021.\nRemove columns except date and price.\nSummarize data to monthly mean prices (hint: use tsibble::yearmonth function - this might be new to you!).\nMake a plot with months on x-axis and monthly means on y-axis.\nSave the figure to file.\n\nIf you get stuck, check out the (“hidden”) suggested solutions below.\n\n\nShow suggested solutions\n\nFirst, you need to set your working directory to the folder where you have saved the downloaded file and where you want to save the final figure. This can be done using the user interface in RStudio (“Session” -> “Set working directory” -> …) or using the command\n\nsetwd(\"../path/to/the/folder/\")\n\nYou are interested in reading in the closing mid price. To read in this data, you may use the following code.\n\nlibrary(fpp3)   # loading textbook package\nlibrary(tidyverse)\nlibrary(readxl) # loading package for reading excel files\ndat <- read_excel(\"data/US10YTRR.xlsx\", sheet = \"Mid\")  \nhead(dat) # printing out the first 6 rows\n\n# A tibble: 6 × 2\n  date                price\n  <dttm>              <dbl>\n1 2022-08-30 00:00:00  96.9\n2 2022-08-29 00:00:00  96.9\n3 2022-08-26 00:00:00  97.6\n4 2022-08-25 00:00:00  97.6\n5 2022-08-24 00:00:00  96.9\n6 2022-08-23 00:00:00  97.4\n\n\nThe sheet argument specifies which sheet in the excel file we want to read. The read_excel function is also quite smart so it recognizes that the date column is a date and automatically format it accordingly. It is however perhaps not so useful to also include the time of the day (all is 00:00:00), so let us remove this part.\n\ndat %>% \n  mutate(date = as.Date(date))\n\n# A tibble: 8,804 × 2\n   date       price\n   <date>     <dbl>\n 1 2022-08-30  96.9\n 2 2022-08-29  96.9\n 3 2022-08-26  97.6\n 4 2022-08-25  97.6\n 5 2022-08-24  96.9\n 6 2022-08-23  97.4\n 7 2022-08-22  97.7\n 8 2022-08-19  98.1\n 9 2022-08-18  98.8\n10 2022-08-17  98.7\n# … with 8,794 more rows\n\n\nHere I have used the mutate function. This is a function we use to either mutate (change) an existing column or create a new one. In this case we mutated the date column transforming it to a “Date” object/format. We could also be interested in adding a column for which year the observation is from.\n\ndat %>% \n  mutate(date = as.Date(date),\n         year = year(date))\n\n# A tibble: 8,804 × 3\n   date       price  year\n   <date>     <dbl> <dbl>\n 1 2022-08-30  96.9  2022\n 2 2022-08-29  96.9  2022\n 3 2022-08-26  97.6  2022\n 4 2022-08-25  97.6  2022\n 5 2022-08-24  96.9  2022\n 6 2022-08-23  97.4  2022\n 7 2022-08-22  97.7  2022\n 8 2022-08-19  98.1  2022\n 9 2022-08-18  98.8  2022\n10 2022-08-17  98.7  2022\n# … with 8,794 more rows\n\n\nHere we have used the year function from the lubridate package, which is loaded with the fpp3 package. The operator %>% is used to add operations to the data manipulation pipeline in the given order. We start with the data object (a tibble) and add a mutate operation to that where we first transform the date column and add a year column. Now that we are pleased with our pipeline, let us save this to the dat object.\n\ndat <- dat %>% \n  mutate(date = as.Date(date),\n         year = year(date))\ndat %>% glimpse()\n\nRows: 8,804\nColumns: 3\n$ date  <date> 2022-08-30, 2022-08-29, 2022-08-26, 2022-08-25, 2022-08-24, 202…\n$ price <dbl> 96.92969, 96.92188, 97.62500, 97.60156, 96.94531, 97.38281, 97.6…\n$ year  <dbl> 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022…\n\n\nThe glimpse function summarizes the tibble/data frame.\nfilter and select\nNow, the data ranges from/to\n\nrange(dat$date)\n\n[1] \"1987-08-03\" \"2022-08-30\"\n\n\nbut you only want to use data from 2010 onwards. To do this, we use the filter function. This function is useful for selecting rows that fulfill some condition, in this case year >= 2010. Let us make a pipeline for this\n\ndat %>% \n  filter(year >= 2010)\n\n# A tibble: 3,178 × 3\n   date       price  year\n   <date>     <dbl> <dbl>\n 1 2022-08-30  96.9  2022\n 2 2022-08-29  96.9  2022\n 3 2022-08-26  97.6  2022\n 4 2022-08-25  97.6  2022\n 5 2022-08-24  96.9  2022\n 6 2022-08-23  97.4  2022\n 7 2022-08-22  97.7  2022\n 8 2022-08-19  98.1  2022\n 9 2022-08-18  98.8  2022\n10 2022-08-17  98.7  2022\n# … with 3,168 more rows\n\n\nSince 2022 is not a complete year (in the data), you also do not want observations after 2021. Then you can add this as an extra condition.\n\ndat %>% \n  filter(year >= 2010, year <=2021)\n\n# A tibble: 3,012 × 3\n   date       price  year\n   <date>     <dbl> <dbl>\n 1 2021-12-31  98.8  2021\n 2 2021-12-30  98.8  2021\n 3 2021-12-29  98.4  2021\n 4 2021-12-28  99.0  2021\n 5 2021-12-27  99.1  2021\n 6 2021-12-23  98.9  2021\n 7 2021-12-22  99.3  2021\n 8 2021-12-21  99.2  2021\n 9 2021-12-20  99.5  2021\n10 2021-12-17  99.7  2021\n# … with 3,002 more rows\n\n\nAlternatively, you can use the between function\n\ndat %>% \n  filter(between(year, 2010, 2021))\n\nwhich will produce the same result. Another useful function is called select. While filter is used on the rows of your data, select is for columns. Say we don’t need the year column after having filtered out the years we don’t want. We can then either select the columns we want to keep\n\ndat %>% \n  filter(between(year, 2010, 2021)) %>%\n  select(date, price)\n\nor remove the columns we do not want\n\ndat %>% \n  filter(between(year, 2010, 2021)) %>%\n  select(-year)\n\n# A tibble: 3,012 × 2\n   date       price\n   <date>     <dbl>\n 1 2021-12-31  98.8\n 2 2021-12-30  98.8\n 3 2021-12-29  98.4\n 4 2021-12-28  99.0\n 5 2021-12-27  99.1\n 6 2021-12-23  98.9\n 7 2021-12-22  99.3\n 8 2021-12-21  99.2\n 9 2021-12-20  99.5\n10 2021-12-17  99.7\n# … with 3,002 more rows\n\n\ngroup_by and summarize\nWe are interested in calculating the monthly mean price. In the tidyverse pipeline this means we want to group our observations according to month and year and summarize by month and year the mean of the observations.\n\n(monthlyMeans <- dat %>% \n  filter(between(year,2010,2021)) %>%\n  mutate(monthyear = tsibble::yearmonth(date)) %>%\n  group_by(monthyear) %>%\n  summarize(meanPrice = mean(price)))\n\n# A tibble: 144 × 2\n   monthyear meanPrice\n       <mth>     <dbl>\n 1  2010 Jan      97.3\n 2  2010 Feb      98.7\n 3  2010 Mar      99.2\n 4  2010 Apr      98.4\n 5  2010 May     101. \n 6  2010 Jun     103. \n 7  2010 Jul     104. \n 8  2010 Aug     102. \n 9  2010 Sep      99.9\n10  2010 Oct     101. \n# … with 134 more rows\n\n\nThis pipeline could be read as first we take out observations prior to 1988 and after 2021, then we group the observations according to year and summarize the mean price by year. Note that this operation will delete any columns that are not in the group_by or being calculated in the summarize.\nggplot\nPlotting a data frame is convenient to do using the ggplot2 package. This will (when used appropriately) produce beautiful figures. Let us plot the time series at hand. The ggplot2 follows the same logic with a pipeline, but instead of the %>% operator, we add elements to the figure using +. We need to specify the data object and the name of the x and y columns to be plotted. Everything in the figure that is to vary based on values in the data frame needs to be wrapped in a aes (aesthetic) function (here the x and y arguments). By adding the geom_line() we insert a line.\n\nggplot(data = monthlyMeans, \n       aes(x=monthyear, y = meanPrice)) +\n  geom_line()\n\n\n\n\nWe could instead add geom_point()\n\n ggplot(data = monthlyMeans, \n       aes(x=monthyear, y = meanPrice)) +\n   geom_point()\n\n\n\n\nor do both\n\n ggplot(data = monthlyMeans, \n       aes(x=monthyear, y = meanPrice)) +\n   geom_line() +\n   geom_point()\n\n\n\n\nWe can change the colors and decrease the size of the points:\n\n ggplot(data =monthlyMeans, \n       aes(x=monthyear, y = meanPrice)) +\n   geom_line(color = \"blue\") +\n   geom_point(color = \"green\", size = .2)\n\n\n\n\nOr maybe we do not want to use the default theme: –>\n\n ggplot(data = monthlyMeans, \n       aes(x=monthyear, y = meanPrice)) +\n   geom_line(color = \"blue\") +\n   geom_point(color = \"green\", size = .2) +\n   theme_bw()\n\n\n\n\nWe can also include the plotting in our data manipulation pipeline. For instance, lets summarize the data by year and plot the resulting yearly time series.\n\ndat %>%\n  filter(between(year, 1988, 2021)) %>%\n  group_by(year) %>%\n  summarize(meanPrice = mean(price)) %>%\n  # adding plotting to pipeline:\n  ggplot(aes(x=year, y = meanPrice)) +\n  geom_line(color = \"blue\") +\n  geom_point(color = \"green\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe cannot illustrate all aspects here, but you will learn new elements by studying examples throughout the course. This recap is mostly for remembering the basics of data manipulation in R and simple plotting. As you will see in the continuation, the coding is not much more complex then what you have seen here and the fpp3 package uses the same type of logic and syntax as the tidyverse. There will however be some new functions specific for time series analysis that you will need to learn."
  },
  {
    "objectID": "2_timeseriesgraphics.html",
    "href": "2_timeseriesgraphics.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "In this chapter, we will cover some key concepts of time series that you need to know. You should build up an intuition of what a stationary time series is, and what autocorrelation is. White noise is a simplest time series process you can think of and the building block of ARIMA models. Here we try to give a short introduction to these concepts. Some of these have already been introduced in MET4 at NHH. So if you have taken that course, this may serve as a short repetition.\n\n\nBelow is a video from Aric LaBarr’s youtube series explaing time series concepts in less than 5 minutes. In the video below he explains what time series data is - as opposed to cross sectional data. As he points out, we also some times have a combination which we can e.g. aggregate to cross sectional data or time series data.\n\n\n\n\n\nIt is quite common in statistics in general, but also in time series analysis, to distinguish between capital and non-capital letters. While Y_t denotes the stochastic variable Y at time t, y_t is the realization of said stochastic variable Y_t, i.e. an observed value. Y_t is a specific variable, while \\{Y_t\\} is the entire time series. While a stochastic variable has for instance an expectation, a variance, a dependence structure and a distribution, an observation is just a number.\nLet \\mu_t=\\mu_Y(t)=\\mathrm{E}(Y_t) denote the mean function of Y_t. Assuming that E(Y_t^2)<\\infty, the covariance function of \\{Y_t\\} is denoted \\gamma(r,s)=\\gamma_Y(r,s)= \\mathrm{Cov}(Y_r,Y_s)=\\mathrm{E}(Y_r-\\mu_r)(Y_s-\\mu_s). If the covariance function does not depend on the specific values of r and s, but rather the distance between them h = |r-s|, we write \\gamma(r,s)=\\gamma(h). Likewise, if \\mu_t does not depend on t, we write \\mu_t=\\mu. This will be used in the section about stationarity below. Note that the variance of Y_t is given by \\gamma(t,t) = \\mathrm{Cov}(Y_t,Y_t) = \\mathrm{Var}(Y_t) or \\gamma(0) in the case where the covariance does not depend on t.\n\n\n\nThe tsibble object will be important in this course. Since we are working in the tidyverse, things will be easier if we commit to it. Converting your data to a time series tibble (tsibble) is essential. The tsibble extends the tidy tibble data frame by introducing temporal structure. Read the chapter 2.1 about tsibble objects in the textbook. Here you find several coding examples.\n\n\n\nWe return to Labarr’s time series in 5 minutes series on youtube explaining stationarity. This is a key concept in time series. The video by Labarr gives a nice intuition for the concept. It separates between strong and weak stationarity. In this course, the weak stationarity condition will be the one we focus on, but it is nice to know that there are other definitions of stationarity as well. For a time series to be (weakly) stationary the mean and variance of the time series variable Y_t should not depend on t. More formally, a time series is weakly stationary if\n\nThe expectation is constant: \\mu_t = \\mu\nThe variance is constant and finite: \\sigma_t^2 = \\sigma^2 < \\infty\nThe covariance between two lagged variables only depend on the lag: \\gamma(r,s) = \\gamma(h), where h=|r-s|.\n\n\n\nIn the video, Labarr shows some examples of non-stationary time series which can be transformed to stationary time series by differencing. We will come back to this later in the course, when we talk about transformations and again when we study ARIMA models. We will also study examples of stationary time / non-stationary time series in the voluntary homework. You can also read about the stationarity in the textbook, but for now, you may stop after the paragraph on stationarity. The book is less specific about their definition, but we will stick the weakly stationary definition above.\n\n\n\n\n\nA white noise series is a time series of uncorrelated observations with mean zero and finite variance. We will often write it as Z_t where Z_t \\sim \\mathrm{WN}(0,\\sigma^2). The standard is that the series is uncorrelated, but we may require it to be independent (stronger assumption) and very often normally distributed. In that case we call it iid Gaussian white noise (iid = independent and identically distributed). Notation for this may be Z_t \\sim \\text{iid}\\, \\mathrm{WN}(0,\\sigma^2). Let us generate a white noise series in R.\n\nlibrary(fpp3)\nset.seed(123) # To produce the same output\nwn <- tsibble(\n  t = 1:100,\n  Z = rnorm(100, sd = 3), # Draws from N(0, 3^2) distribution\n  index = \"t\"\n) \nwn %>% \n  autoplot() + \n  labs(title = \"Gaussian White noise\",\n       subtitle = \"iid WN(0,9)\")\n\nPlot variable not specified, automatically selected `.vars = Z`\n\n\n\n\n\nWe can then plot the acf of the series:\n\nwn %>%\n  ACF(Z) %>%\n  autoplot() + \n  labs(title=\"White noise ACF\")\n\n\n\n\nAs you can see, all the correlations fall within the confidence bands. The series is uncorrelated. You can also find a similar example in the textbook."
  },
  {
    "objectID": "3_adjustments.html",
    "href": "3_adjustments.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "In the example in the video above, we are not interested in a proxy for working days per month, and to avoid the effect of this we use the mean (average) production per working day within each month instead of total production per month. The code to generate the example can be found below:\n\n\nCode:\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(fpp3)\n\n# ggplot theme:\ntheme_set(\n  theme_bw() + \n    theme(panel.grid.minor = element_blank(),\n          panel.grid.major = element_blank())\n)\n\n# Daily production: \ndat <- tibble(\n  date = seq(as.Date(\"2015-01-01\"), as.Date(\"2019-12-31\"), by = \"1 day\"),\n  price = pi\n) %>% \n  #Removing the weekends:\n  filter(wday(date, week_start = 1) %in% 1:5) %>%  \n  #Note: We do not remove public holidays, and the worker never takes a day off\n  mutate(YearMonth = yearmonth(date))\n\n# -- TOTAL PRODUCTION FIGURE --\ndat %>%\n  group_by(YearMonth) %>%\n  summarize(`Total production` = sum(price)) %>%\n  as_tsibble(index = \"YearMonth\") %>% \n  ggplot(aes(x = YearMonth, \n             y = `Total production`)) + \n  geom_point(color = \"skyblue\") + \n  geom_line(color = \"skyblue\") + \n  scale_y_continuous(breaks = seq(60, 100, 5),\n                     labels = paste0(\"$\",seq(60, 100, 5),\"k\"), \n                     limits = c(60,75))\n# -- MEAN PRODUCTION FIGURE --\ndat %>% group_by(YearMonth) %>%\n  summarize(`Mean production` = mean(price)) %>%\n  as_tsibble(index = \"YearMonth\") %>% \n  ggplot(aes(x = YearMonth, \n             y = `Mean production`)) + \n  geom_point(color = \"skyblue\") + \n  geom_line(color=\"skyblue\")+ \n  scale_y_continuous(breaks = seq(3, 4, .02), \n                     labels = paste0(\"$\",seq(3, 4, .02),\"k\"))\n\n\n\n\n\nAdjusting for population size is usually a good idea when studying a quantity that is affected by it. The most obvious example is to study GDP (Gross Domestic Product) per capita instead of GDP.\n\n\n\n\nCode:\n# --- Setting up the data --\nscandinaviaUSA <- global_economy %>% \n  filter(Country %in% c(\"Norway\",\"Sweden\",\"Denmark\", \"United States\"))\nscandinaviaUSA %>% head()\n\n# --- GDP by country in $US --\nscandinaviaUSA %>%\n  autoplot(GDP)+\n  labs(title= \"GDP\", y = \"$US\")\n\n# --- Population by country  --\nscandinaviaUSA %>%\n  autoplot(Population)+\n  labs(title= \"Population\", y = \"Number of people\")\n\n# --- Population by country  (log-scale on y-axis) --\nscandinaviaUSA %>% \n  autoplot(Population)+\n  scale_y_log10()+\n  labs(title= \"Population (log-scale)\",y = \"Number of people\")\n\n# --- GDP per capita by country ---\nscandinaviaUSA %>%\n  autoplot(GDP/Population) +\n  labs(title= \"GDP per capita = GDP / Population\", y = \"$US\")\n\n\n\n\n\nAdjusting for inflation is a simple way of taking into account that 5$ in 1950 would get you much more than 5$ would today. This compensation is usually done by a consumer price index, which is standardized to a specific year (in the video below we show examples with 2010 and 2015 as reference years).\nLet Y_t denote the raw time series and Y_t^\\star the inflation adjusted. Let \\text{CPI}_t denote a relevant consumer price index defined to be 100 in the reference year t^\\star. Then Y_t^\\star = Y_t \\cdot \\frac{100}{\\text{CPI}_t}. More generally, we can choose the reference year t^\\star and write this as Y_t^\\star = Y_t \\cdot \\frac{\\text{CPI}_{t^\\star}}{\\text{CPI}_t}.\nThe inflation adjusted series is then measured in the unit “year t^\\star-money”.\n\n\n\n\nCode:\n# --- Inflation adjusted GDP per capita by country ---\nscandinaviaUSA %>%\n  autoplot(GDP/Population *100 / CPI) +\n  labs(title= \"GDP per capita = GDP / Population\", y = \"$US\")\n\n# --- CPI FOR NORWAY (data from Statistics Norway)---\nCPI <- read.csv(\"data/CPI_norway.csv\", sep = \";\") %>% as_tibble() %>%\n  select(1:2) %>%\n  rename(Year = X,CPI = Y.avg2) %>% \n  mutate(Year = as.numeric(Year), CPI = as.numeric(CPI))%>%\n  filter(Year < 2022) %>%\n  as_tsibble(index = Year)\n\n# --- CPI figure ---\nCPI %>% \n  autoplot(CPI, color = \"blue\", lwd = 1.2) +\n  labs(title= \"Consumer Price Index\", y = \"NOK\",\n       subtitle = \"Data source: Statistics Norway\")+\n  geom_hline(yintercept = 100, lty = 2) + geom_vline(xintercept = 2015, lty = 2)+\n  scale_x_continuous(breaks = seq(1925,2025,10))+\n  scale_y_continuous(breaks = seq(0,120,10))\n\n# --- BIG MAC price index ---\nbigMac <- read_csv(\"https://raw.githubusercontent.com/TheEconomist/big-mac-data/master/output-data/big-mac-raw-index.csv\")\nnorBigMac <- bigMac %>% \n  filter(name %in% c(\"Norway\")) %>% \n  mutate(Year = lubridate::year(date)) %>%\n  as_tsibble(index = \"date\")%>%\n  filter(Year <2022) %>%\n  left_join(CPI, by = \"Year\") \n\n# --- BIG MAC price index figure ---\nnorBigMac %>% \n  autoplot(local_price) +\n  labs(title= \"Big Mac price in Norway\", y = \"NOK\",\n       subtitle = \"Data source: The Economist\") +\n  geom_smooth(method = \"lm\", se=FALSE)\n\n#--- Inflation adjusted BIG MAC price index figure ---\nnorBigMac %>%\n  mutate(cpiAdjusted =local_price / CPI * 100)  %>%\n  as_tsibble(index = date) %>%\n  autoplot(cpiAdjusted)+\n  labs(title= \"Inflation adjusted Big Mac price in Norway\", y = \"NOK (2015)\",\n       subtitle = \"Data sources: The Economist (big mac index), Statistics Norway (CPI)\") +\n  geom_smooth(method = \"lm\", se=FALSE)\n\n\n\n\n\nIn many situations it can be necessary to do a mathematical transformation of a time series. There can be different reasons for doing so, but a main one is to make it stationary (or at least more stationary). For instance, if you see that the variation increases or decreases with the level of the series. The most common transformation (for positive time series) is probably using the logarithm. It is often effective and interpretable as changes in the log value correspond to relative changes in the original scale. We write the transformed series, w_t, as w_t =\\log y_t, where y_t is the original time series.\nThe textbook also mentions power transformations of the form w_t = y_t^p (squarte roots - p=\\frac12, cube roots - p=\\frac13, etc). These are not as common to use, but there are situations where these may be better than the logarithm.\nA family of transformations (including log- and a class of power transformations) is the Box-Cox transform. For any value of \\lambda\\in \\mathbb R, \\begin{equation*}\nw_t = \\begin{cases}\n\\log(y_t),&\\text{if }\\lambda = 0;\\\\\n(y_t^\\lambda -1)/\\lambda, &\\text{otherwise}.\n\\end{cases}\n\\end{equation*} As you can see, if \\lambda = 0 we have a simple natural logarithm transform. This version of the Box-Cox transform is also defined for negative values of y_t as long as \\lambda >0.\nThe book has a very nice shiny app for experimenting with different values of \\lambda on a time series of gas production in Australia. We have borrowed it below, but you find it also here. They write that: ” A good value of \\lambda is one which makes the size of the seasonal variation about the same across the whole series, as that makes the forecasting model simpler.” This pretty much sums up why one does mathematical transformations as a preprocessing step before fitting a model - it makes the model simpler.\n\n\nFor financial assets, such as stocks, it is often better to consider the returns rather than the price series. This is also a mathematical transformation and involves differencing. First order differencing means subtracting the previous observation from the present, i.e. w_t=y_t-y_{t-1}. Taking differences is an effective way of potentially making a non-stationary time series stationary. E.g. if a time series has a linear trend: Y_t = \\alpha t + Z_t, where \\alpha is a real constant and Z_t is a white noise, we get that W_t = Y_t-Y_{t-1} = \\alpha t + Z_t - \\alpha(t-1) - Z_{t-1} = Z_t-Z_{t-1} + \\alpha, effectively removing the trend in the transformed series. We will return to this when considering ARIMA models.\nThere are different definitions of returns, but the most common ones are the standard returns, r_t, and log-returns, \\textrm{lr}_t, defined respectively by \\begin{equation*}\n\\begin{split}\nr_t &= \\frac{y_t-y_{t-1}}{y_{t-1}},\\\\\n\\textrm{lr}_t &= \\log y_t-\\log y_{t-1} = \\log\\frac{y_t}{y_{t-1}}.\n\\end{split}\n\\end{equation*} A daily return series for a stock usually has expectation close to zero and little autocorrelation, which can be convenient in many situations. However, they are typically hetereoskedastic (non-constant variance) and the squared returns are often autocorrelated. We will come back to this, when discussing volatility forecasting towards the end of the course.\n\n\nCode\n\n\n# Package for downloading stock data (primarily from Yahoo! Finance)\nlibrary(quantmod)\n\n# -- Download the data: --\ngetSymbols(\"AAPL\")\n\n[1] \"AAPL\"\n\n# -- Extract the closing price and create a tsibble: --\nclose.price <- tibble(\n  close = as.numeric(AAPL$AAPL.Close),\n  date  = time(AAPL)\n) %>% as_tsibble(index = date) \n\n# -- Plot closing price: --\nclose.price %>% autoplot(close) +\n  labs(title = \"Apple Closing price\", \n       y     = \"US$\")\n\n\n\n# -- Adding transformations : -- \nclose.price <- close.price %>% \n  mutate(logclose  = log(close), # log-transform\n         logreturn = c(NA, diff(logclose)), # log returns\n         return    = c(NA, diff(close)/close[-nrow(close.price)]) # Returns\n         ) \n\n\n# -- Box-cox-transform --\nlambda <- close.price %>%\n  features(close, features = guerrero) %>%\n  pull(lambda_guerrero)\nclose.price <- close.price %>%\n  mutate(boxcox = box_cox(close,lambda))\nclose.price %>%\n  autoplot(boxcox) +\n  labs(y = \"\",\n       title = latex2exp::TeX(paste0(\n         \"Transformed apple closing price with $\\\\lambda$ = \",\n         round(lambda,3))))+\n  # Adding red curve with log-transform\n  geom_line(aes(y=logclose), col = 2) \n\n\n\n# -- Plotting the different transformations --\nclose.price %>%\n  pivot_longer(-date) %>% \n  autoplot(value) + \n  facet_wrap(~name, scales=\"free_y\", strip.position = \"left\")+\n  labs(title = \"Apple Closing Price transformations\") +\n  theme(strip.placement = \"outside\")\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\nset.seed(1344)\nlibrary(tidyverse)\nlibrary(fpp3)\ndat <- tibble(\n  date = seq(as.Date(\"2015-01-01\"), as.Date(\"2020-12-31\"), by = \"1 day\"),\n  t = 1:length(date),\n  Tt = 100+9*t/300 + .5*((t-1000)/200)^3+(.87*cos(2*pi*t/1200)+.42*sin(2*pi*t/600))*11,\n  St = (.94*cos(2*pi*t/365) -1.2*sin(2* pi*t/365))*13,\n  Rt = rnorm(length(date), sd = 15),\n  Yt = Tt + St +  Rt\n) \n\n# -- Plotting Y and its components --\ndat %>%\n  pivot_longer(cols = -c(date,t)) %>% \n  mutate(name = factor(name, levels = c(\"Yt\", \"Tt\", \"St\", \"Rt\"))) %>% # to order the panels\n  ggplot(aes(x=date, y = value, col = name)) + \n  geom_line() + facet_wrap(~name, ncol = 1, scales = \"free_y\", strip.position = \"left\") +\n  theme(strip.placement = \"outside\", axis.title = element_blank(), legend.position = \"none\")\n\n\n\n# -- Plotting seasonally adjusted Y --\nggplot(dat, aes(x = date, y = Yt-St)) + \n  geom_line() +\n  labs(title = \"Seasonally adjusted\", x = \"\")"
  },
  {
    "objectID": "3_decomposition.html",
    "href": "3_decomposition.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "In this chapter we will consider different adjusments and transformations one can do prior to a model task. Then we move on to techniques for decomposing a time series into a trend-cycle, season and remainder component.\n\n\n\n\n\n\n\nIn the example in the video above, we are not interested in a proxy for working days per month, and to avoid the effect of this we use the mean (average) production per working day within each month instead of total production per month. The code to generate the example can be found below:\n\n\nCode for generating examples:\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(fpp3)\n\n# ggplot theme:\ntheme_set(\n  theme_bw() + \n    theme(panel.grid.minor = element_blank(),\n          panel.grid.major = element_blank())\n)\n\n# Daily production: \ndat <- tibble(\n  date = seq(as.Date(\"2015-01-01\"), as.Date(\"2019-12-31\"), by = \"1 day\"),\n  price = pi\n) %>% \n  #Removing the weekends:\n  filter(wday(date, week_start = 1) %in% 1:5) %>%  \n  #Note: We do not remove public holidays, and the worker never takes a day off\n  mutate(YearMonth = yearmonth(date))\n\n# -- TOTAL PRODUCTION FIGURE --\ndat %>%\n  group_by(YearMonth) %>%\n  summarize(`Total production` = sum(price)) %>%\n  as_tsibble(index = \"YearMonth\") %>% \n  ggplot(aes(x = YearMonth, \n             y = `Total production`)) + \n  geom_point(color = \"skyblue\") + \n  geom_line(color = \"skyblue\") + \n  scale_y_continuous(breaks = seq(60, 100, 5),\n                     labels = paste0(\"$\",seq(60, 100, 5),\"k\"), \n                     limits = c(60,75))\n# -- MEAN PRODUCTION FIGURE --\ndat %>% group_by(YearMonth) %>%\n  summarize(`Mean production` = mean(price)) %>%\n  as_tsibble(index = \"YearMonth\") %>% \n  ggplot(aes(x = YearMonth, \n             y = `Mean production`)) + \n  geom_point(color = \"skyblue\") + \n  geom_line(color=\"skyblue\")+ \n  scale_y_continuous(breaks = seq(3, 4, .02), \n                     labels = paste0(\"$\",seq(3, 4, .02),\"k\"))\n\n\n\n\n\nAdjusting for population size is usually a good idea when studying a quantity that is affected by it. The most obvious example is to study GDP (Gross Domestic Product) per capita instead of GDP.\n\n\n\n\nCode for generating examples in the video\n# --- Setting up the data --\nscandinaviaUSA <- global_economy %>% \n  filter(Country %in% c(\"Norway\",\"Sweden\",\"Denmark\", \"United States\"))\nscandinaviaUSA %>% head()\n\n# --- GDP by country in $US --\nscandinaviaUSA %>%\n  autoplot(GDP)+\n  labs(title= \"GDP\", y = \"$US\")\n\n# --- Population by country  --\nscandinaviaUSA %>%\n  autoplot(Population)+\n  labs(title= \"Population\", y = \"Number of people\")\n\n# --- Population by country  (log-scale on y-axis) --\nscandinaviaUSA %>% \n  autoplot(Population)+\n  scale_y_log10()+\n  labs(title= \"Population (log-scale)\",y = \"Number of people\")\n\n# --- GDP per capita by country ---\nscandinaviaUSA %>%\n  autoplot(GDP/Population) +\n  labs(title= \"GDP per capita = GDP / Population\", y = \"$US\")\n\n\n\n\n\nAdjusting for inflation is a simple way of taking into account that 5$ in 1950 would get you much more than 5$ would today. This compensation is usually done by a consumer price index, which is standardized to a specific year (in the video below we show examples with 2010 and 2015 as reference years).\nLet Y_t denote the raw time series and Y_t^\\star the inflation adjusted. Let \\text{CPI}_t denote a relevant consumer price index defined to be 100 in the reference year t^\\star. Then Y_t^\\star = Y_t \\cdot \\frac{100}{\\text{CPI}_t}. More generally, we can choose the reference year t^\\star and write this as Y_t^\\star = Y_t \\cdot \\frac{\\text{CPI}_{t^\\star}}{\\text{CPI}_t}.\nThe inflation adjusted series is then measured in the unit “year t^\\star-money”.\n\n\n\n\nCode for generating examples:\n# --- Inflation adjusted GDP per capita by country ---\nscandinaviaUSA %>%\n  autoplot(GDP/Population *100 / CPI) +\n  labs(title= \"GDP per capita = GDP / Population\", y = \"$US\")\n\n# --- CPI FOR NORWAY (data from Statistics Norway)---\nCPI <- read.csv(\"data/CPI_norway.csv\", sep = \";\") %>% as_tibble() %>%\n  select(1:2) %>%\n  rename(Year = X,CPI = Y.avg2) %>% \n  mutate(Year = as.numeric(Year), CPI = as.numeric(CPI))%>%\n  filter(Year < 2022) %>%\n  as_tsibble(index = Year)\n\n# --- CPI figure ---\nCPI %>% \n  autoplot(CPI, color = \"blue\", lwd = 1.2) +\n  labs(title= \"Consumer Price Index\", y = \"NOK\",\n       subtitle = \"Data source: Statistics Norway\")+\n  geom_hline(yintercept = 100, lty = 2) + geom_vline(xintercept = 2015, lty = 2)+\n  scale_x_continuous(breaks = seq(1925,2025,10))+\n  scale_y_continuous(breaks = seq(0,120,10))\n\n# --- BIG MAC price index ---\nbigMac <- read_csv(\"https://raw.githubusercontent.com/TheEconomist/big-mac-data/master/output-data/big-mac-raw-index.csv\")\nnorBigMac <- bigMac %>% \n  filter(name %in% c(\"Norway\")) %>% \n  mutate(Year = lubridate::year(date)) %>%\n  as_tsibble(index = \"date\")%>%\n  filter(Year <2022) %>%\n  left_join(CPI, by = \"Year\") \n\n# --- BIG MAC price index figure ---\nnorBigMac %>% \n  autoplot(local_price) +\n  labs(title= \"Big Mac price in Norway\", y = \"NOK\",\n       subtitle = \"Data source: The Economist\") +\n  geom_smooth(method = \"lm\", se=FALSE)\n\n#--- Inflation adjusted BIG MAC price index figure ---\nnorBigMac %>%\n  mutate(cpiAdjusted =local_price / CPI * 100)  %>%\n  as_tsibble(index = date) %>%\n  autoplot(cpiAdjusted)+\n  labs(title= \"Inflation adjusted Big Mac price in Norway\", y = \"NOK (2015)\",\n       subtitle = \"Data sources: The Economist (big mac index), Statistics Norway (CPI)\") +\n  geom_smooth(method = \"lm\", se=FALSE)\n\n\n\n\n\nIn many situations it can be necessary to do a mathematical transformation of a time series. There can be different reasons for doing so, but a main one is to make it stationary (or at least more stationary). For instance, if you see that the variation increases or decreases with the level of the series. The most common transformation (for positive time series) is probably using the logarithm. It is often effective and interpretable as changes in the log value correspond to relative changes in the original scale. We write the transformed series, w_t, as w_t =\\log y_t, where y_t is the original time series.\nThe textbook also mentions power transformations of the form w_t = y_t^p (squarte roots - p=\\frac12, cube roots - p=\\frac13, etc). These are not as common to use, but there are situations where these may be better than the logarithm.\nA family of transformations (including log- and a class of power transformations) is the Box-Cox transform. For any value of \\lambda\\in \\mathbb R, \\begin{equation*}\nw_t = \\begin{cases}\n\\log(y_t),&\\text{if }\\lambda = 0;\\\\\n(y_t^\\lambda -1)/\\lambda, &\\text{otherwise}.\n\\end{cases}\n\\end{equation*} As you can see, if \\lambda = 0 we have a simple natural logarithm transform. This version of the Box-Cox transform is also defined for negative values of y_t as long as \\lambda >0.\nThe book has a very nice shiny app for experimenting with different values of \\lambda on a time series of gas production in Australia. We have borrowed it below, but you find it also here. They write that: ” A good value of \\lambda is one which makes the size of the seasonal variation about the same across the whole series, as that makes the forecasting model simpler.” This pretty much sums up why one does mathematical transformations as a preprocessing step before fitting a model - it makes the model simpler.\n\n\nFor financial assets, such as stocks, it is often better to consider the returns rather than the price series. This is also a mathematical transformation and involves differencing. First order differencing means subtracting the previous observation from the present, i.e. w_t=y_t-y_{t-1}. Taking differences is an effective way of potentially making a non-stationary time series stationary. E.g. if a time series has a linear trend: Y_t = \\alpha t + Z_t, where \\alpha is a real constant and Z_t is a white noise, we get that W_t = Y_t-Y_{t-1} = \\alpha t + Z_t - \\alpha(t-1) - Z_{t-1} = Z_t-Z_{t-1} + \\alpha, effectively removing the trend in the transformed series. We will return to this when considering ARIMA models.\nThere are different definitions of returns, but the most common ones are the standard returns, r_t, and log-returns, \\textrm{lr}_t, defined respectively by \\begin{equation*}\n\\begin{split}\nr_t &= \\frac{y_t-y_{t-1}}{y_{t-1}},\\\\\n\\textrm{lr}_t &= \\log y_t-\\log y_{t-1} = \\log\\frac{y_t}{y_{t-1}}.\n\\end{split}\n\\end{equation*} A daily return series for a stock usually has expectation close to zero and little autocorrelation, which can be convenient in many situations. However, they are typically hetereoskedastic (non-constant variance) and the squared returns are often autocorrelated. We will come back to this, when discussing volatility forecasting towards the end of the course.\n\n\nCode on mathematical transformations\n\n\n# Package for downloading stock data (primarily from Yahoo! Finance)\nlibrary(quantmod)\n\n# -- Download the data: --\ngetSymbols(\"AAPL\")\n\n[1] \"AAPL\"\n\n# -- Extract the closing price and create a tsibble: --\nclose.price <- tibble(\n  close = as.numeric(AAPL$AAPL.Close),\n  date  = time(AAPL)\n) %>% as_tsibble(index = date) \n\n# -- Plot closing price: --\nclose.price %>% autoplot(close) +\n  labs(title = \"Apple Closing price\", \n       y     = \"US$\")\n\n\n\n# -- Adding transformations : -- \nclose.price <- close.price %>% \n  mutate(logclose  = log(close), # log-transform\n         logreturn = c(NA, diff(logclose)), # log returns\n         return    = c(NA, diff(close)/close[-nrow(close.price)]) # Returns\n         ) \n\n\n# -- Box-cox-transform --\nlambda <- close.price %>%\n  features(close, features = guerrero) %>%\n  pull(lambda_guerrero)\nclose.price <- close.price %>%\n  mutate(boxcox = box_cox(close,lambda))\nclose.price %>%\n  autoplot(boxcox) +\n  labs(y = \"\",\n       title = latex2exp::TeX(paste0(\n         \"Transformed apple closing price with $\\\\lambda$ = \",\n         round(lambda,3))))+\n  # Adding red curve with log-transform\n  geom_line(aes(y=logclose), col = 2) \n\n\n\n# -- Plotting the different transformations --\nclose.price %>%\n  pivot_longer(-date) %>% \n  autoplot(value) + \n  facet_wrap(~name, scales=\"free_y\", strip.position = \"left\")+\n  labs(title = \"Apple Closing Price transformations\") +\n  theme(strip.placement = \"outside\")\n\n\n\n\n\n\n\n\n\n\n\n\nCode on mathematical transformations\n\n\nset.seed(1344)\nlibrary(tidyverse)\nlibrary(fpp3)\ndat <- tibble(\n  date = seq(as.Date(\"2015-01-01\"), as.Date(\"2020-12-31\"), by = \"1 day\"),\n  t = 1:length(date),\n  Tt = 100+9*t/300 + .5*((t-1000)/200)^3+(.87*cos(2*pi*t/1200)+.42*sin(2*pi*t/600))*11,\n  St = (.94*cos(2*pi*t/365) -1.2*sin(2* pi*t/365))*13,\n  Rt = rnorm(length(date), sd = 15),\n  Yt = Tt + St +  Rt\n) \n\n# -- Plotting Y and its components --\ndat %>%\n  pivot_longer(cols = -c(date,t)) %>% \n  mutate(name = factor(name, levels = c(\"Yt\", \"Tt\", \"St\", \"Rt\"))) %>% # to order the panels\n  ggplot(aes(x=date, y = value, col = name)) + \n  geom_line() + facet_wrap(~name, ncol = 1, scales = \"free_y\", strip.position = \"left\") +\n  theme(strip.placement = \"outside\", axis.title = element_blank(), legend.position = \"none\")\n\n\n\n# -- Plotting seasonally adjusted Y --\nggplot(dat, aes(x = date, y = Yt-St)) + \n  geom_line() +\n  labs(title = \"Seasonally adjusted\", x = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\nlibrary(fpp3)\nlibrary(tidyverse)\n\n# ggplot theme: \ntheme_set(theme_bw() + \n            theme(panel.grid.major = element_blank(),\n                  panel.grid.minor = element_blank()))\n\n# -- Read in data: --\ndat <- readxl::read_excel(\n  \"data/NorwayEmployment_15-74years_bySex.xlsx\") %>%\n  as_tibble() %>%\n  mutate(Quarter = str_replace(Quarter, \"K\",\"Q\"),\n         Quarter = yearquarter(Quarter)) \nnames(dat)[3] <- \"Employed\"\n\n# -- Aggregating from Employed by sex to total --\ndat <- dat %>% \n  group_by(Quarter) %>% \n  summarize(Employed = sum(Employed)) %>%\n  as_tsibble(index = Quarter) # Time series table\ndat %>% \n  autoplot(Employed, lwd = 1, colour = \"blue\")\n\n\n\ndat <- dat %>%\n  mutate(\n    `12-MA` = slider::slide_dbl(Employed, mean,\n                                .before = 5, .after = 6, .complete = TRUE),\n    `2x12-MA` = slider::slide_dbl(`12-MA`, mean,\n                                  .before = 1, .after = 0, .complete = TRUE)\n  )\ndat %>%\n  ggplot(aes(x=Quarter, y =Employed))+\n  geom_line(colour = \"gray\") +\n  geom_line(aes(y = `2x12-MA`), colour = \"#D55E00\") +\n  theme_bw()+\n  labs(y = \"Persons (thousands)\",\n       title = \"Total employment in US retail\")\n\nWarning: Removed 12 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\ndat %>% \n  model(\n    classical_decomposition(Employed, type = \"additive\")\n  ) %>%\n  components() %>%\n  autoplot()\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\ndat %>% \n  model(\n    classical = classical_decomposition(Employed, \n                                        type = \"multiplicative\"),\n    x11 = X_13ARIMA_SEATS(Employed ~ x11()),\n    seats = X_13ARIMA_SEATS(Employed ~ seats())\n  ) %>%\n  components() %>% \n  mutate(random = ifelse(.model == \"classical\", \n                         random, \n                         irregular)) %>%\n  autoplot(lwd = 1)\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\ndat %>%\n  model(\n    STL0 = STL(Employed),\n    STL1 = STL(Employed ~ trend(window = 5) + # default 7\n                 season(window = 19),         # default 11\n               robust = FALSE)\n  ) %>%\n  components() %>%\n  autoplot()\n\n\n\n\n\n\n\n\n\nUse the global_economy data, select a country (e.g. Austria). Plot GDP, GDP per capita and GDP per capita inflation adjusted, GDP inflation adjusted.\n\n\n\nSolution\n\n\nlibrary(fpp3)\nlibrary(tidyverse)\ndat <- global_economy %>% \n  filter(Country == \"Austria\")\ndat %>% autoplot(GDP)\n\n\n\ndat %>% autoplot(GDP/Population)\n\n\n\ndat %>% autoplot(GDP/Population * 100/CPI)\n\n\n\ndat %>% autoplot(GDP * 100/CPI)\n\n\n\n\n\n\nIn the global_economy data set, the CPI has a reference year of 2010. Do the necessary changes to inflation adjust GDP per capita with 1990 as reference year.\n\n\n\nSolution\n\n\nlibrary(fpp3)\nlibrary(tidyverse)\ndat <- global_economy %>% \n  filter(Country == \"Austria\")\n\n# -- Extracting the CPI in 1990: --\ncpi1990 <- dat %>% filter(Year ==1990) %>% pull(CPI)\n\n# -- Transforming such that CPI1990 is 100 in 1990: --\ndat <- dat %>% mutate(CPI1990 = CPI / cpi1990 * 100)\n\n# -- Plotting Inflation adjusted GDP per capita: --\ndat %>% autoplot(GDP/Population * 100/CPI1990) + \n  labs(y = \"Inflation adjusted GDP per capita (1990 US$)\") +\n  geom_line(aes(y= GDP/Population * 100/CPI), col = 2)\n\n\n\n# -- Comparing the two CPIs: --\ndat %>% \n  pivot_longer(cols = c(CPI,CPI1990)) %>%\n  ggplot(aes(x = Year, y = value, col = name)) + geom_line()+\n  geom_hline(yintercept = 100) +\n  scale_color_manual(values = c(\"red\",\"blue\"))+\n  geom_segment(x = 1990,xend = 1990, y = -Inf, yend =100, lty = 2, col = \"blue\")+\n  geom_segment(x = 2010,xend = 2010, y = -Inf, yend =100, lty = 2, col = \"red\")+\n  labs(title = \"Differences between CPI with reference year 1990 and 2010\",\n       y = \"Consumer Price Index\") +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\nUse the quantmod package to download data for another stock. Will a log transformation do a good job for this as well?\nUse the guerrero feature to select a \\lambda for the Box-Cox transformation on the data from the previous exercise."
  },
  {
    "objectID": "3_decomposition_intro.html",
    "href": "3_decomposition_intro.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Decomposition\nIn this chapter we will consider different adjusments and transformations one can do prior to a model task. Then we move on to techniques for decomposing a time series into a trend-cycle, season and remainder component."
  },
  {
    "objectID": "3_exercises.html",
    "href": "3_exercises.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Exercises\n\n\n\n\nUse the simulated data from here. Compare different methods with the truth from the simulation. Are there large differences?\n\n\n\nSolution\n\nSince the simulated data is daily, we cannot use the x11 and seats methods. We will therefore only compare the STL and classical decomposition.\nWe start by re-running the simulation and creating a tsibble.\n\nset.seed(1344)\nlibrary(fpp3)\ntheme_set(theme_bw())\ndat <- tibble(\n  date = seq(as.Date(\"2015-01-01\"), as.Date(\"2020-12-31\"), by = \"1 day\"),\n  t = 1:length(date),\n  Tt = 100+9*t/300 + .5*((t-1000)/200)^3+(.87*cos(2*pi*t/1200)+.42*sin(2*pi*t/600))*11,\n  St = (.94*cos(2*pi*t/365) -1.2*sin(2* pi*t/365))*13,\n  Rt = rnorm(length(date), sd = 15),\n  Yt = Tt + St +  Rt\n) \n\ndat <- as_tsibble(dat, index = date)\n\nThen we do the decomposition, classical and STL:\n\ndecomp <- dat %>% \n  model(\n    classic = classical_decomposition(Yt~season(period = \"1 year\"), type = \"additive\"),\n    stl = STL(Yt~season(period = \"1 year\"), robust = TRUE)\n) %>% components() %>%\n  # For some reason, the classic and stl uses differnt naming conventions:\n  mutate(random = ifelse(is.na(remainder), random, remainder),\n          seasonal = ifelse(is.na(`season_1 year`), seasonal, `season_1 year`))\n# Adjusting the names of dat to fit with the decomp\nnames(dat) <- c(\"date\", \"t\", \"trend\", \"seasonal\", \"random\", \"Yt\")\ndat$.model = \"simulation\"\n# Classical decomposition: \ndecomp %>% filter(.model == \"classic\") %>% autoplot()+ labs(title= \"Classical\")\n\nWarning: Removed 182 rows containing missing values (`geom_line()`).\n\n\n\n\n# STL: \ndecomp %>% filter(.model == \"stl\") %>% autoplot() + labs(title= \"STL\")\n\n\n\n# Simulation: \ndecomp %>% bind_rows(dat) %>% filter(.model == \"simulation\") %>% autoplot()\n\n\n\n# All\ndecomp %>% bind_rows(dat) %>% autoplot()\n\nWarning: Removed 182 rows containing missing values (`geom_line()`).\n\n\n\n\n\nWith the setting I used here, it does not seem that the decomposition methods manage to get as smooth estimate for the seasonal component as we see in the simulation, but for the trend part the fit is almost perfect.\n\n\nUse the global_economy data, select a country (e.g. Austria). Plot GDP, GDP per capita and GDP per capita inflation adjusted, GDP inflation adjusted.\n\n\n\nSolution\n\n\nlibrary(fpp3)\nlibrary(tidyverse)\ndat <- global_economy %>% \n  filter(Country == \"Austria\")\ndat %>% autoplot(GDP)\n\n\n\ndat %>% autoplot(GDP/Population)\n\n\n\ndat %>% autoplot(GDP/Population * 100/CPI)\n\n\n\ndat %>% autoplot(GDP * 100/CPI)\n\n\n\n\n\n\nIn the global_economy data set, the CPI has a reference year of 2010. Do the necessary changes to inflation adjust GDP per capita with 1990 as reference year.\n\n\n\nSolution\n\n\nlibrary(fpp3)\nlibrary(tidyverse)\ndat <- global_economy %>% \n  filter(Country == \"Austria\")\n\n# -- Extracting the CPI in 1990: --\ncpi1990 <- dat %>% filter(Year ==1990) %>% pull(CPI)\n\n# -- Transforming such that CPI1990 is 100 in 1990: --\ndat <- dat %>% mutate(CPI1990 = CPI / cpi1990 * 100)\n\n# -- Plotting Inflation adjusted GDP per capita: --\ndat %>% autoplot(GDP/Population * 100/CPI1990) + \n  labs(y = \"Inflation adjusted GDP per capita (1990 US$)\") +\n  geom_line(aes(y= GDP/Population * 100/CPI), col = 2)\n\n\n\n# -- Comparing the two CPIs: --\ndat %>% \n  pivot_longer(cols = c(CPI,CPI1990)) %>%\n  ggplot(aes(x = Year, y = value, col = name)) + geom_line()+\n  geom_hline(yintercept = 100) +\n  scale_color_manual(values = c(\"red\",\"blue\"))+\n  geom_segment(x = 1990,xend = 1990, y = -Inf, yend =100, lty = 2, col = \"blue\")+\n  geom_segment(x = 2010,xend = 2010, y = -Inf, yend =100, lty = 2, col = \"red\")+\n  labs(title = \"Differences between CPI with reference year 1990 and 2010\",\n       y = \"Consumer Price Index\") +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\nUse the quantmod package to download data for another stock. Will a log transformation do a good job for this as well?\n\n\n\nSolution\n\n\nlibrary(quantmod)\n# Equinor: \ngetSymbols(\"EQNR\")\n\n[1] \"EQNR\"\n\nclose.price <- tibble(\n  close = as.numeric(EQNR$EQNR.Close),\n  date  = time(EQNR)\n) %>% as_tsibble(index = date) \n\n# -- Plot closing price: --\nclose.price %>% autoplot(close) +\n  labs(title = \"Apple Closing price\", \n       y     = \"US$\")\n\n\n\nclose.price <- close.price %>% \n  mutate(logclose  = log(close), # log-transform\n         logreturn = c(NA, diff(logclose)), # log returns\n         return    = c(NA, diff(close)/close[-nrow(close.price)]) # Returns\n         ) \n\nclose.price %>% \n  pivot_longer(-date) %>%\n  ggplot(aes(x=date, y = value))+ geom_line()+ \n  facet_wrap(~name, scales = \"free_y\")\n\n\n\n\n\n\nUse the guerrero feature to select a \\lambda for the Box-Cox transformation on the data from the previous exercise.\n\n\n\nSolution\n\n\n# -- Box-cox-transform --\nlambda <- close.price %>%\n  features(close, features = guerrero) %>%\n  pull(lambda_guerrero)\nlambda\n\n[1] 0.5108475\n\nclose.price <- close.price %>%\n  mutate(boxcox = box_cox(close,lambda))\nclose.price %>%\n  autoplot(boxcox) +\n  labs(y = \"\",\n       title = latex2exp::TeX(paste0(\n         \"Transformed apple closing price with $\\\\lambda$ = \",\n         round(lambda,3))))+\n  # adding the blue close price\n  geom_line(aes(y=close), col = 4)"
  },
  {
    "objectID": "3_timeseriesdecomposition.html",
    "href": "3_timeseriesdecomposition.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Code\n\n\nlibrary(fpp3)\nlibrary(tidyverse)\n\n# ggplot theme: \ntheme_set(theme_bw() + \n            theme(panel.grid.major = element_blank(),\n                  panel.grid.minor = element_blank()))\n\n# -- Read in data: --\ndat <- readxl::read_excel(\n  \"data/NorwayEmployment_15-74years_bySex.xlsx\") %>%\n  as_tibble() %>%\n  mutate(Quarter = str_replace(Quarter, \"K\",\"Q\"),\n         Quarter = yearquarter(Quarter)) \nnames(dat)[3] <- \"Employed\"\n\n# -- Aggregating from Employed by sex to total --\ndat <- dat %>% \n  group_by(Quarter) %>% \n  summarize(Employed = sum(Employed)) %>%\n  as_tsibble(index = Quarter) # Time series table\ndat %>% \n  autoplot(Employed, colour = \"blue\")\n\n\n\ndat <- dat %>%\n  mutate(\n    `12-MA` = slider::slide_dbl(Employed, mean,\n                                .before = 5, .after = 6, .complete = TRUE),\n    `2x12-MA` = slider::slide_dbl(`12-MA`, mean,\n                                  .before = 1, .after = 0, .complete = TRUE)\n  )\ndat %>%\n  ggplot(aes(x=Quarter, y =Employed))+\n  geom_line(colour = \"gray\") +\n  geom_line(aes(y = `2x12-MA`), colour = \"#D55E00\") +\n  theme_bw()+\n  labs(y = \"Persons (thousands)\",\n       title = \"Total employment in US retail\")\n\nWarning: Removed 12 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\ndat %>% \n  model(\n    classical_decomposition(Employed, type = \"additive\")\n  ) %>%\n  components() %>%\n  autoplot()\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\ndat %>% \n  model(\n    classical = classical_decomposition(Employed, \n                                        type = \"multiplicative\"),\n    x11 = X_13ARIMA_SEATS(Employed ~ x11()),\n    seats = X_13ARIMA_SEATS(Employed ~ seats())\n  ) %>%\n  components() %>% \n  mutate(random = ifelse(.model == \"classical\", \n                         random, \n                         irregular)) %>%\n  autoplot()\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\ndat %>%\n  model(\n    STL0 = STL(Employed),\n    STL1 = STL(Employed ~ trend(window = 5) + # default 7\n                 season(window = 19),         # default 11\n               robust = FALSE)\n  ) %>%\n  components() %>%\n  autoplot()"
  },
  {
    "objectID": "4_exercises.html",
    "href": "4_exercises.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Exercises\n\n\n\n\nExercise 1 in Chapter 4.6.\nExercise 2 in Chapter 4.6.\nExercise 3 in Chapter 4.6."
  },
  {
    "objectID": "4_features.html",
    "href": "4_features.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Features\nHere we will go through how to calculate features in R and show some examples of features you can calculate for time series using the feasts package. You find more details in Chapter 4.1-4.3 of Hyndman and Athanasopoulos(2021). The code used in this lecture is an adaptation of what you find in these sections.\n\n\n\n\nCode used in video\n\n\n# --- Time series features ---\nlibrary(fpp3)\nlibrary(tidyverse)\ntheme_set(theme_bw() +\n            theme(panel.grid.minor = element_blank(),\n                  panel.grid.major = element_blank(),\n                  strip.background = element_rect(fill =\"white\",\n                                                  color = \"transparent\")))\n \n# -- Features: mean --\ntourism %>% \n  features(Trips, list(mean = mean)) %>%\n  arrange(mean)\n\n# A tibble: 304 × 4\n   Region          State              Purpose   mean\n   <chr>           <chr>              <chr>    <dbl>\n 1 Kangaroo Island South Australia    Other    0.340\n 2 MacDonnell      Northern Territory Other    0.449\n 3 Wilderness West Tasmania           Other    0.478\n 4 Barkly          Northern Territory Other    0.632\n 5 Clare Valley    South Australia    Other    0.898\n 6 Barossa         South Australia    Other    1.02 \n 7 Kakadu Arnhem   Northern Territory Other    1.04 \n 8 Lasseter        Northern Territory Other    1.14 \n 9 Wimmera         Victoria           Other    1.15 \n10 MacDonnell      Northern Territory Visiting 1.18 \n# … with 294 more rows\n\n# -- Features: mean, sd, 2.5 and 97.5 percentiles --\ntourism %>% \n  features(Trips, list(mean = mean,\n                       sd   = sd,\n                       p    = ~quantile(., probs = c(0.025, 0.975)))) %>%\n  arrange(desc(sd))\n\n# A tibble: 304 × 7\n   Region                 State             Purpose   mean    sd p_2.5…¹ p_97.…²\n   <chr>                  <chr>             <chr>    <dbl> <dbl>   <dbl>   <dbl>\n 1 South Coast            New South Wales   Holiday   495. 170.    285.     812.\n 2 North Coast NSW        New South Wales   Holiday   588. 117.    404.     815.\n 3 Sydney                 New South Wales   Business  602. 117.    402.     884.\n 4 Great Ocean Road       Victoria          Holiday   281. 116.    135.     545.\n 5 Melbourne              Victoria          Holiday   507. 103.    354.     736.\n 6 Peninsula              Victoria          Holiday   185.  96.7    70.8    458.\n 7 Australia's South West Western Australia Holiday   309.  95.3   179.     541.\n 8 Melbourne              Victoria          Visiting  619.  93.6   472.     807.\n 9 Brisbane               Queensland        Visiting  493.  90.6   344.     663.\n10 Sydney                 New South Wales   Visiting  747.  89.6   564.     916.\n# … with 294 more rows, and abbreviated variable names ¹​`p_2.5%`, ²​`p_97.5%`\n\n# -- dplyr equivalent: --\ntourism  %>% \n  as_tibble() %>% \n  group_by(Region,State,Purpose) %>%\n  summarize(mean = mean(Trips)) %>%\n  arrange(mean)\n\n# A tibble: 304 × 4\n# Groups:   Region, State [76]\n   Region          State              Purpose   mean\n   <chr>           <chr>              <chr>    <dbl>\n 1 Kangaroo Island South Australia    Other    0.340\n 2 MacDonnell      Northern Territory Other    0.449\n 3 Wilderness West Tasmania           Other    0.478\n 4 Barkly          Northern Territory Other    0.632\n 5 Clare Valley    South Australia    Other    0.898\n 6 Barossa         South Australia    Other    1.02 \n 7 Kakadu Arnhem   Northern Territory Other    1.04 \n 8 Lasseter        Northern Territory Other    1.14 \n 9 Wimmera         Victoria           Other    1.15 \n10 MacDonnell      Northern Territory Visiting 1.18 \n# … with 294 more rows\n\n# -- ACF features --\ntourism %>% \n  features(Trips, feat_acf) \n\n# A tibble: 304 × 10\n   Region   State Purpose     acf1 acf10 diff1…¹ diff1…² diff2…³ diff2…⁴ seaso…⁵\n   <chr>    <chr> <chr>      <dbl> <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 Adelaide Sout… Busine…  0.0333  0.131  -0.520   0.463  -0.676   0.741  0.201 \n 2 Adelaide Sout… Holiday  0.0456  0.372  -0.343   0.614  -0.487   0.558  0.351 \n 3 Adelaide Sout… Other    0.517   1.15   -0.409   0.383  -0.675   0.792  0.342 \n 4 Adelaide Sout… Visiti…  0.0684  0.294  -0.394   0.452  -0.518   0.447  0.345 \n 5 Adelaid… Sout… Busine…  0.0709  0.134  -0.580   0.415  -0.750   0.746 -0.0628\n 6 Adelaid… Sout… Holiday  0.131   0.313  -0.536   0.500  -0.716   0.906  0.208 \n 7 Adelaid… Sout… Other    0.261   0.330  -0.253   0.317  -0.457   0.392  0.0745\n 8 Adelaid… Sout… Visiti…  0.139   0.117  -0.472   0.239  -0.626   0.408  0.170 \n 9 Alice S… Nort… Busine…  0.217   0.367  -0.500   0.381  -0.658   0.587  0.315 \n10 Alice S… Nort… Holiday -0.00660 2.11   -0.153   2.11   -0.274   1.55   0.729 \n# … with 294 more rows, and abbreviated variable names ¹​diff1_acf1,\n#   ²​diff1_acf10, ³​diff2_acf1, ⁴​diff2_acf10, ⁵​season_acf1\n\n# -- STL features --\ntourism %>%\n  features(Trips, feat_stl) %>%\n  ggplot(aes(x = trend_strength, y=seasonal_strength_year,\n             color = Purpose)) + \n  geom_point() + \n  facet_wrap(vars(State))\n\n\n\n# -- Time series with strongest seasonal component: --\ntourism %>%\n  features(Trips, feat_stl) %>%\n  filter(\n    seasonal_strength_year == max(seasonal_strength_year)\n  ) %>%\n  left_join(tourism, by = c(\"Region\",\"State\",\"Purpose\")) %>%\n  ggplot(aes(x=Quarter, y = Trips)) + geom_line() + \n  facet_grid(vars(Region,State,Purpose))"
  },
  {
    "objectID": "4_timeseriesfeatures.html",
    "href": "4_timeseriesfeatures.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Time series features\nThis chapter is maybe mostly about learning how to use features in time series analysis. It has mostly a technical side that you should learn to master. Calculating features such a means, standard deviation or quantiles is maybe not new to you, but the ACF and STL features I would imagine are. Using features to detect outlyers among many time series is a nice application of the material you should learn in this chapter."
  },
  {
    "objectID": "4_tourismexample.html",
    "href": "4_tourismexample.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Exploring Australian tourism example\nYou find the example in the textbook here: Chapter 4.5. This is a somewhat complicated example to follow perhaps, and in the video below we go through the code line by line (almost) and explain what is going on. You may watch the video or simply study it in the book. The code is not added here, since it is the same as in the book (more or less)."
  },
  {
    "objectID": "5_forecasterstoolbox.html",
    "href": "5_forecasterstoolbox.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Forecasters toolbox"
  },
  {
    "objectID": "6_judgementalforecast.html",
    "href": "6_judgementalforecast.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "In this chapter, we go through chapter 6 of the textbook (Hyndman and Athanasopoulos, 2021). This material is mostly for you to be aware of and be able do discuss different aspects of the judgmental forecasting methods. We will through short videos describe some general aspects and specific methods. Particularly, the Delphi method and forecasting by analogy will be described in some detail, while other methods are mentioned. For more details, see the referred chapter 6 in the textbook."
  },
  {
    "objectID": "7_regressionmodels.html",
    "href": "7_regressionmodels.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Regression models"
  },
  {
    "objectID": "8_ARIMAmodels.html",
    "href": "8_ARIMAmodels.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "The following shiny app has been developed by Sondre Hølleland and being administred at https://sholleland.shinyapps.io/ban430_shinyapps. Due to restrictions relating to available computing hours on the free shinyapps account, the app may not work. You may then copy the code below to run the shiny app locally on your own computer. Remember that understanding the details of this code is not necessary.\n\n\nShiny app code:\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(fpp3)\nlibrary(ggpubr)\nsbp.width <- 3\n\ntheme_set(theme_bw() + theme(panel.grid.major = element_blank(),\n                             panel.grid.minor = element_blank()))\n# Define UI for application that draws a histogram\nui <- fluidPage(\n\n    # Application title\n    titlePanel(\"ARMA models\"),\n    tabsetPanel(\n      tabPanel(\"AR(1)\", \n               \n               sidebarLayout(\n                 sidebarPanel(width = sbp.width,\n                              uiOutput(\"artext\"),\n                   sliderInput(\"arphi\",\n                               \"phi:\",\n                               min = -.99,\n                               max = .99,\n                               value = .7,\n                               step = .01),\n                   sliderInput(\"arsigma\",\n                               \"sigma:\",\n                               min = 0,\n                               max = 10,\n                               value = 1,\n                               step = .2),\n                   numericInput(\"arseed\",\n                                \"Seed:\",\n                                value = 1234,\n                                min = 0, \n                                max = 9999,\n                                step = 1),\n                   numericInput(\"arn\",\n                                \"Sample size:\",\n                                value = 500,\n                                min = 100, \n                                max = 5000,\n                                step = 100)\n                 ),\n                 \n                 # Show a plot of the generated distribution\n                 mainPanel(\n                   plotOutput(\"arPlot\", height = \"600px\")\n                 )\n               )),\n      tabPanel(\"MA(1)\",  \n               \n               sidebarLayout(\n                 sidebarPanel(width = sbp.width,\n                              uiOutput(\"matext\"),\n                   sliderInput(\"matheta\",\n                               \"theta:\",\n                               min = -1,\n                               max = 1,\n                               value = .7,\n                               step = .01),\n                   sliderInput(\"masigma\",\n                               \"sigma:\",\n                               min = 0,\n                               max = 10,\n                               value = 1,\n                               step = .2),\n                   numericInput(\"maseed\",\n                                \"Seed:\",\n                                value = 1234,\n                                min = 0, \n                                max = 9999,\n                                step = 1),\n                   numericInput(\"man\",\n                                \"Sample size:\",\n                                value = 500,\n                                min = 100, \n                                max = 5000,\n                                step = 100)\n                 ),\n                 \n                 # Show a plot of the generated distribution\n                 mainPanel(\n                   plotOutput(\"maPlot\", height = \"600px\")\n                 )\n               )),\n      tabPanel(\"ARMA(1,1)\", \n              \n               sidebarLayout(\n                 sidebarPanel(width = sbp.width,\n                              uiOutput(\"armatext\"),\n                   sliderInput(\"armaphi\",\n                               \"phi:\",\n                               min = -.99,\n                               max = .99,\n                               value = .7,\n                               step = .01),\n                   sliderInput(\"armatheta\",\n                               \"theta:\",\n                               min = -1,\n                               max = 1,\n                               value = .7,\n                               step = .01),\n                   sliderInput(\"armasigma\",\n                               \"sigma:\",\n                               min = 0,\n                               max = 10,\n                               value = 1,\n                               step = .2),\n                   numericInput(\"armaseed\",\n                                \"Seed:\",\n                                value = 1234,\n                                min = 0, \n                                max = 9999,\n                                step = 1),\n                   numericInput(\"arman\",\n                                \"Sample size:\",\n                                value = 500,\n                                min = 100, \n                                max = 5000,\n                                step = 100)\n                 ),\n                 \n                 # Show a plot of the generated distribution\n                 mainPanel(\n                   plotOutput(\"armaPlot\", height = \"600px\")\n                 )\n               ))))\n    # Sidebar with a slider input for number of bins \n   \n\n\n# Define server logic required to draw a histogram\nserver <- function(input, output) {\n    output$artext <- renderUI({\n      withMathJax(\"Model: $$Y_t=\\\\phi \\\\,Y_{t-1}+Z_t,\\\\quad Z_t\\\\sim \\\\text{i.i.d N}(0,\\\\sigma^2)$$\")\n    })\n    output$matext <- renderUI({\n      withMathJax(\"Model: $$Y_t=\\\\theta\\\\, Z_{t-1}+Z_t,\\\\quad Z_t\\\\sim \\\\text{i.i.d N}(0,\\\\sigma^2)$$\")\n    })\n    output$armatext <- renderUI({\n      withMathJax(\"Model: $$Y_t=\\\\phi\\\\, Y_{t-1}+\\\\theta \\\\,Z_{t-1}+Z_t,\\\\quad Z_t\\\\sim \\\\text{i.i.d N}(0,\\\\sigma^2)$$\")\n    })\n    output$arPlot <- renderPlot({\n      set.seed(input$arseed)\n      burnin <- 200\n      x <- rnorm(input$arn+burnin, sd = input$arsigma)\n      for(i in 2:length(x))\n        x[i] <- input$arphi*x[i-1]+rnorm(1, sd = input$arsigma)\n      df <- tsibble(x = x[burnin+1:(length(x)-burnin)],\n                    t = 1:(length(x)-burnin),\n                    index = \"t\")\n      # Theoretical acf: \n      \n      theoretical.correlations <- \n        tibble(lag = 1:29,\n               acf = ARMAacf(ar = c(input$arphi), lag.max = 29, pacf =FALSE)[-1],\n               pacf = ARMAacf(ar = c(input$arphi), lag.max = 29, pacf =TRUE))\n      # generate bins based on input$bins from ui.R\n       ggarrange(\n         df %>% autoplot() + scale_y_continuous(\"AR(1) series\")+\n           scale_x_continuous(\"Time index\", expand = c(0,0)),\n         ggarrange(df %>% ACF() %>% autoplot()+\n                     scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                     ylab(\"Sample PACF\"),\n         df %>% PACF() %>% autoplot()+\n           scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n           ylab(\"Sample PACF\"), \n         theoretical.correlations %>% ggplot(aes(x=lag))+\n           geom_segment(aes(x=lag,xend=lag,y=0,yend=acf))+geom_hline(yintercept=0)+\n           scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n           scale_y_continuous(\"Theoretical ACF\"),\n         theoretical.correlations %>% ggplot(aes(x=lag))+\n           geom_segment(aes(x=lag,xend=lag,y=0,yend=pacf))+geom_hline(yintercept=0)+\n           scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n           scale_y_continuous(\"Theoretical PACF\"),\n         nrow = 2, ncol = 2),\n         ncol = 1, nrow = 2, heights = c(1,2))\n    })\n\n    output$maPlot <- renderPlot({\n      set.seed(input$maseed)\n      burnin <- 200\n      z <- rnorm(input$man+burnin, sd = input$masigma)\n      x <- numeric(input$man+burnin)\n      for(i in 2:length(x))\n        x[i] <- input$matheta*z[i-1]+z[i]\n      df <- tsibble(x = x[burnin+1:(length(x)-burnin)],\n                    t = 1:(length(x)-burnin),\n                    index = \"t\")\n      \n      theoretical.correlations <- \n        tibble(lag = 1:29,\n               acf = ARMAacf(ma = c(input$matheta), lag.max = 29, pacf =FALSE)[-1],\n               pacf = ARMAacf(ma = c(input$matheta), lag.max = 29, pacf =TRUE))\n      # generate bins based on input$bins from ui.R\n      ggarrange(\n        df %>% autoplot() + scale_y_continuous(\"MA(1) series\")+\n          scale_x_continuous(\"Time index\", expand = c(0,0)),\n        ggarrange(df %>% ACF() %>% autoplot()+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                                         ylab(\"Sample ACF\"),\n                  df %>% PACF() %>% autoplot()+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                                         ylab(\"Sample PACF\"), \n                  theoretical.correlations %>% ggplot(aes(x=lag))+\n                    geom_segment(aes(x=lag,xend=lag,y=0,yend=acf))+geom_hline(yintercept=0)+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                    scale_y_continuous(\"Theoretical ACF\"),\n                  theoretical.correlations %>% ggplot(aes(x=lag))+\n                    geom_segment(aes(x=lag,xend=lag,y=0,yend=pacf))+geom_hline(yintercept=0)+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                    scale_y_continuous(\"Theoretical PACF\"),\n                  nrow = 2, ncol = 2),\n        ncol = 1, nrow = 2, heights = c(1,2))\n    })\n    output$armaPlot <- renderPlot({\n      set.seed(input$armaseed)\n      burnin <- 200\n      #sdZ = sqrt(input$armasigma *(1-input$armaphi^2)/(1+2*input$armaphi*input$armatheta + input$armatheta^2))\n      z <- rnorm(input$arman+burnin, sd = input$armasigma)\n      x <- numeric(input$arman+burnin)\n      for(i in 2:length(x))\n        x[i] <- input$armaphi*x[i-1]+input$armatheta*z[i-1]+z[i]\n      df <- tsibble(x = x[burnin+1:(length(x)-burnin)],\n                    t = 1:(length(x)-burnin),\n                    index = \"t\")\n      \n      theoretical.correlations <- \n        tibble(lag = 1:29,\n               acf = ARMAacf(ar = c(input$armaphi), ma = c(input$armatheta), lag.max = 29, pacf =FALSE)[-1],\n               pacf = ARMAacf(ar = c(input$armaphi), ma = c(input$armatheta), lag.max = 29, pacf =TRUE))\n      # generate bins based on input$bins from ui.R\n      ggarrange(\n        df %>% autoplot() + scale_y_continuous(\"MA(1) series\")+\n          scale_x_continuous(\"Time index\", expand = c(0,0)),\n        ggarrange(df %>% ACF() %>% autoplot()+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                                         ylab(\"Sample ACF\"),\n                  df %>% PACF() %>% autoplot()+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                                         ylab(\"Sample PACF\"), \n                  theoretical.correlations %>% ggplot(aes(x=lag))+\n                    geom_segment(aes(x=lag,xend=lag,y=0,yend=acf))+geom_hline(yintercept=0)+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                    scale_y_continuous(\"Theoretical ACF\"),\n                  theoretical.correlations %>% ggplot(aes(x=lag))+\n                    geom_segment(aes(x=lag,xend=lag,y=0,yend=pacf))+geom_hline(yintercept=0)+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                    scale_y_continuous(\"Theoretical PACF\"),\n                  nrow = 2, ncol = 2),\n        ncol = 1, nrow = 2, heights = c(1,2))\n    })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "9_Volatitiliy_forecasting.html",
    "href": "9_Volatitiliy_forecasting.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Volatility forecasting"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This course and website has been developed by Sondre Hølleland, assistant professor at Norwegian School of Economics, Department of Business and Management Science. More about me will be added."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Welcome to the website for BAN430 Forecasting. We will use this website as a supplement to lectures. The website is an ongoing development, so not all subjects will have content yet. Below you will find a detailed (preliminary) lecture plan, link to the textbook and curriculum.\n\n\n\n\n\n\n \n  \n    Monday 14:15-16:00 \n    Thursday 12:15-14:00 \n  \n \n\n  \n    16.01.2023 \n    19.01.2023 \n  \n  \n    Introduction lecture \n    R recap \n  \n  \n    23.01.2023 \n    26.01.2023 \n  \n  \n    Time series graphics \n    Time series decomposition (summary) \n  \n  \n    30.01.2023 \n    02.02.2023 \n  \n  \n    Forecasters toolbox \n    Forecasters toolbox \n  \n  \n    06.02.2023 \n    09.02.2023 \n  \n  \n    Regression models \n    Regression models \n  \n  \n    13.02.2023 \n    16.02.2023 \n  \n  \n    Exponential smoothing \n    Exponential smoothing \n  \n  \n    20.02.2023 \n    23.02.2023 \n  \n  \n    No lecture: Selfstudy time series features (website) \n    No lecture: Selfstudy judgmental forecasts (website) \n  \n  \n    27.02.2023 \n    02.03.2023 \n  \n  \n    ARIMA \n    ARIMA \n  \n  \n    06.03.2023 \n    09.03.2023 \n  \n  \n    ARIMA \n    No lecture / selfstudy \n  \n  \n    13.03.2023 \n    16.03.2023 \n  \n  \n    ARIMA \n    Dynamic regresion models \n  \n  \n    20.03.2023 \n    23.03.2023 \n  \n  \n    Dynamic regression models \n    No lecture / selfstudy \n  \n  \n    27.03.2023 \n    30.03.2023 \n  \n  \n    Volatility forecasting \n    Volatility forecasting \n  \n  \n    03.04.2023 \n    06.04.2023 \n  \n  \n    Easter holiday? \n    Easter holiday \n  \n  \n    10.04.2023 \n    13.04.2023 \n  \n  \n    Easter holiday \n    Summary lecture \n  \n  \n    Study period prior to exam \n     \n  \n  \n    08.05.2023 \n     \n  \n  \n    8 hour home exam \n     \n  \n\n\n\n\n\n\n\n\n\n\n\nFPP Third edition\n\n\nWe will use the textbook Forecasting: Principles and Practice, 3rd edition, by Hyndman and Athanasopoulos, i.e. the online version which can be accessed at https://otexts.com/fpp3/.\n\n\n\nTextbook Hyndman and Athanasopoulos(2021) chapters 1-10 and 13. Additional notes by lecturer on volatility forecasting. All the material on this website."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "References"
  }
]