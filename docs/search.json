[
  {
    "objectID": "W4_electricityprices.html",
    "href": "W4_electricityprices.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Traditionally, Norwegian electricity consumers have been blessed with low electricity prices due to the vast access to hydro power from the many Norwegian waterfalls. This has been a competitive advantage for Norwegian industry. But some time ago this changed and we have seen a large increase in electricity prices. This has led the Norwegian government to implement a monetary support scheme for Norwegian private households. The scheme is based on the monthly average price and covers 90% above 0.70 NOK/kWh excluding taxes (mva). The problem with this scheme is that the monthly average price is not known before the month has ended, so at the time of consumption, the consumers of electricity do not know how much governmental support will be and there how expensive the power is. Since the scheme does not take into account the spot price there can be times with negative prices after taking into account the governmental support. Say the monthly average is 2.7 NOK, then the support will be 0.90\\cdot (2.7-0.7) = 1.80 NOK/kWh.\nIt can therefore be useful to have a forecast model to have an idea of what the governmental support will be. Towards the end of the month we will have a quite good idea of what the average price will be, but at the beginning of the month it will be quite uncertain.\nMedia houses such as VG and Aftenposten present estimates of the support by simple moving averages (so far in the month), but Martin Jullum, a statistician at the Norwegian Computing Center, has developed a more sophisticated forecasting model presented at dinstrompris.no (only in Norwegian). The approach he is using is based on dynamic regression model with ARIMA errors that simulates the rest of the month 10 000 times and calculates an average based on that. In this exercise, we will end up with a version of his model.\nAlthough we may have negative electricity prices (free electricity!) for a few hours in a day, this is highly unlikely to happen on the daily average price and has not happened in the data we will be using. You may therefore assume non-negative daily electricity price if you find that necessary.\n\n\n\nWe download the data from Martin Jullum’s github repository using the following code\n\nelprices &lt;- readr::read_csv(\n  \"https://raw.githubusercontent.com/martinju/stromstotte/master/data/database_nordpool_daily.csv\")\n\nNote that the data is updated daily. The data contains three columns:\n\narea: Nordpool price region (NO1, NO2, NO3, NO4 or NO5)\ndate: date in %Y-%m-%d format\nprice: Average daily price in NOK per kWh\n\nWestern Norway is pricing region NO5.\nThe goal of this workshop is to build a model that can forecast the monthly average electricity price for February 2023 using daily observations. From that, we can also give an estimate of the Governmental support scheme. We will use January 2023 as our test set (for comparing different model classes) and February 2023 is the month we will apply our forecast model on.\n\nLoad the data and select only pricing area NO5. Create a training set containing observations up to and including Dec 31st 2022 and a test set with observations in January 2023. Also create a set called feb23 containing February 2023. All these data sets should be tsibble objects.\n\n\nExplore the data using suitable graphics. Look for trends and season and comment on what you find.\nDo a decomposition using a suitable decomposition method with weekly seasonality. Compare different options for the decomposition method. Comment on the trend and seasonal component. Are they reasonable? Why? Or why not?\n\nWe know that electricity demand is correlated with temperature. On colder days we need more heating which increase demand. Increased demand should also increase the price of electricity.\n\nCould daily average temperature in Bergen (at Florida) be a potential predictor candidate for the electricity price? Below is a scatterplot with price on the y-axis and Bergen average temperature on the x-axis. Comment on the relationship. Would forecasting with temperature as predictor be an ex ante or ex post forecast? Elaborate.\n\n\n\n\n\n\n\n\n\nIn this exercise we focus on ETS and ARIMA models and use AICc as our model selection criteria within each model family. We first seek to find the best ETS model and do ARIMA afterwards.\n\n\n\nUse the automated ETS model selection in the fable package and print the report. Write up the model equations with estimated values.\nPlot the residuals ACF and histogram. Are there any problems with these residuals? Do a Ljung-Box test including lags up to 10.\n\n\n\n\n\nEstimate the optimal \\lambda parameter using the Guerero method. Based on the estimated \\lambda parameter, is the box-cox transformation far from a log-transform? Make a plot including the box-cox transformed and log transformed. Include also the log(1+price) transformation.\n\nWe could have compared the different transformations by evaluating their performance on a test set, but for simplicity we stick to the log(1+price) transform. What is the main consequence for the forecasting model based on this assumption?\n\nWe have decided on a suitable transformation. Is the transformed time series stationary? Is differencing necessary? Use the unitroot_nsdiffs and unitroot_ndiffs to select the number of differences. Plot the differenced series and visually confirm that the differenced and transformed series is stationary using gg_tsdisplay() with plot_type = “partial”.\nSuggest some model candidates based on the ACF and PACF plots in the previous exercise. Fit the model candidates along with three automatically selected models:\n\n\nStepwise = TRUE and approx = TRUE\nStepwise = FALSE and approx = TRUE\nStepwise = FALSE and approx = FALSE\n\nCompare the methods using AICc as criteria. Which model would you choose? Create a fit object with only this model and call it fit.\n\nCheck the model assumptions using the gg_tsresiduals() function on the fit object from the selected model. Perform a Ljung-Box test and conclude. Are the assumptions fulfilled? What are the required and “nice-to-have” assumptions? Would you assume normality for producing prediction intervals in this case?\nPrint the report for your selected model. Based on the report, set up the model equation using the backshift notation.\nBased on the equation in the previous exercise, find an expression for the first point forecast (\\widehat y_{T+1|T}).\nMartin Jullumn’s model on dinstrompris.no is a dynamic regression model with a categorical weekday dummy and ARIMA errors. Fit such a model a compare it with the ARIMA model of choice so far using an accuracy measure of choice (e.g. RMSE). Use a 4 weeks forecast horizon (same as length of February). Jullum do not use any transformations in his model so our model will be different. See code hint below for how this model can be implemented. Check the residual plots for the best model.\n\n\n\nCode hint:\nmodel(\n  jullum = ARIMA(log(1+price) ~ wday(date, label = TRUE), approx = FALSE, stepwise = FALSE)\n)\n\n\n\n\n\nWe will now set up a Monte Carlo simulation for forecasting the average monthly electricity price for February. We simulate daily February prices from the best model and caluclate the monthly average price. This gives us a simulated sample of monthly average price of February and we can estimate the forecast distribution of that variable.\n\nWe continue with the best model based on the test set performance above. We have found our forecasting model, now it is time to set it into action. Imagine that it is January 31^{\\text{st}} 2023 today, and we want to forecast the monthly average electricity price for February 2023. We will complicate this further in the next question, but start refitting the selected model using observations including Jan 31st 2023. Plot a forecast for daily prices in February. Does the forecast behave as you expect? Generate 500 potential February prices from the fitted model. Calculate the monthly average price for each of the simulations (should give 500 average February prices). Present the mean, standard deviation and 95% prediction interval for the monthly average price.\n\n\n\nCode hint:\nfit %&gt;% forecast(h = 28) %&gt;% autoplot()\n# Simulate: \nsims &lt;- fit %&gt;%  generate(h = 28, times = 500)\nsims %&gt;% \n  index_by(month = ~yearmonth(.)) %&gt;% \n  group_by(.rep) %&gt;%  \n  # Calculate monthly average price for the simulations\n  summarize(average = mean(.sim)) %&gt;% \n  ungroup() %&gt;% \n  # Monte Carlo estimates: \n  summarize(\n    mean = mean(average),\n    sd   = sd(average),\n    lwr  = quantile(average, prob = 0.025),\n    upr  = quantile(average, prob = 0.975)\n  )\n\n\n\nIn this final exercise, we pretend that the we are doing the same thing as dinstrompris.no. We start from Jan 31st of 2023 with the mean, standard deviation and 95% prediction interval from exercise 14 (i.e. the forecast of the average price for February). Then a day goes by, it is now Feb 1st and we know the daily average price of Feb 1st, so we update our model with the new information and produce a new forecast for the remaining 27 days of the month and produce a mean, sd, prediction interval for the average price of the full month, where 1 day is observed and the rest is simulated. Continue adding days and updating the forecast until all days of February are observed. Create a nice graphic similar to the one below. Explain the behavior of the forecast we see in the figure.\n\nPS: The last exercise is extra challenging from a programming point of view. If you don’t know how to attack this, you may just comment on the figure.\n The red dashed line is the actual average price of February and the date on the x-axis is the first day of the forecasting horizon (Feb 1st means data up to Jan 31st is used)."
  },
  {
    "objectID": "W4_electricityprices.html#ws4-electricity-prices",
    "href": "W4_electricityprices.html#ws4-electricity-prices",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Traditionally, Norwegian electricity consumers have been blessed with low electricity prices due to the vast access to hydro power from the many Norwegian waterfalls. This has been a competitive advantage for Norwegian industry. But some time ago this changed and we have seen a large increase in electricity prices. This has led the Norwegian government to implement a monetary support scheme for Norwegian private households. The scheme is based on the monthly average price and covers 90% above 0.70 NOK/kWh excluding taxes (mva). The problem with this scheme is that the monthly average price is not known before the month has ended, so at the time of consumption, the consumers of electricity do not know how much governmental support will be and there how expensive the power is. Since the scheme does not take into account the spot price there can be times with negative prices after taking into account the governmental support. Say the monthly average is 2.7 NOK, then the support will be 0.90\\cdot (2.7-0.7) = 1.80 NOK/kWh.\nIt can therefore be useful to have a forecast model to have an idea of what the governmental support will be. Towards the end of the month we will have a quite good idea of what the average price will be, but at the beginning of the month it will be quite uncertain.\nMedia houses such as VG and Aftenposten present estimates of the support by simple moving averages (so far in the month), but Martin Jullum, a statistician at the Norwegian Computing Center, has developed a more sophisticated forecasting model presented at dinstrompris.no (only in Norwegian). The approach he is using is based on dynamic regression model with ARIMA errors that simulates the rest of the month 10 000 times and calculates an average based on that. In this exercise, we will end up with a version of his model.\nAlthough we may have negative electricity prices (free electricity!) for a few hours in a day, this is highly unlikely to happen on the daily average price and has not happened in the data we will be using. You may therefore assume non-negative daily electricity price if you find that necessary.\n\n\n\nWe download the data from Martin Jullum’s github repository using the following code\n\nelprices &lt;- readr::read_csv(\n  \"https://raw.githubusercontent.com/martinju/stromstotte/master/data/database_nordpool_daily.csv\")\n\nNote that the data is updated daily. The data contains three columns:\n\narea: Nordpool price region (NO1, NO2, NO3, NO4 or NO5)\ndate: date in %Y-%m-%d format\nprice: Average daily price in NOK per kWh\n\nWestern Norway is pricing region NO5.\nThe goal of this workshop is to build a model that can forecast the monthly average electricity price for February 2023 using daily observations. From that, we can also give an estimate of the Governmental support scheme. We will use January 2023 as our test set (for comparing different model classes) and February 2023 is the month we will apply our forecast model on.\n\nLoad the data and select only pricing area NO5. Create a training set containing observations up to and including Dec 31st 2022 and a test set with observations in January 2023. Also create a set called feb23 containing February 2023. All these data sets should be tsibble objects.\n\n\nExplore the data using suitable graphics. Look for trends and season and comment on what you find.\nDo a decomposition using a suitable decomposition method with weekly seasonality. Compare different options for the decomposition method. Comment on the trend and seasonal component. Are they reasonable? Why? Or why not?\n\nWe know that electricity demand is correlated with temperature. On colder days we need more heating which increase demand. Increased demand should also increase the price of electricity.\n\nCould daily average temperature in Bergen (at Florida) be a potential predictor candidate for the electricity price? Below is a scatterplot with price on the y-axis and Bergen average temperature on the x-axis. Comment on the relationship. Would forecasting with temperature as predictor be an ex ante or ex post forecast? Elaborate.\n\n\n\n\n\n\n\n\n\nIn this exercise we focus on ETS and ARIMA models and use AICc as our model selection criteria within each model family. We first seek to find the best ETS model and do ARIMA afterwards.\n\n\n\nUse the automated ETS model selection in the fable package and print the report. Write up the model equations with estimated values.\nPlot the residuals ACF and histogram. Are there any problems with these residuals? Do a Ljung-Box test including lags up to 10.\n\n\n\n\n\nEstimate the optimal \\lambda parameter using the Guerero method. Based on the estimated \\lambda parameter, is the box-cox transformation far from a log-transform? Make a plot including the box-cox transformed and log transformed. Include also the log(1+price) transformation.\n\nWe could have compared the different transformations by evaluating their performance on a test set, but for simplicity we stick to the log(1+price) transform. What is the main consequence for the forecasting model based on this assumption?\n\nWe have decided on a suitable transformation. Is the transformed time series stationary? Is differencing necessary? Use the unitroot_nsdiffs and unitroot_ndiffs to select the number of differences. Plot the differenced series and visually confirm that the differenced and transformed series is stationary using gg_tsdisplay() with plot_type = “partial”.\nSuggest some model candidates based on the ACF and PACF plots in the previous exercise. Fit the model candidates along with three automatically selected models:\n\n\nStepwise = TRUE and approx = TRUE\nStepwise = FALSE and approx = TRUE\nStepwise = FALSE and approx = FALSE\n\nCompare the methods using AICc as criteria. Which model would you choose? Create a fit object with only this model and call it fit.\n\nCheck the model assumptions using the gg_tsresiduals() function on the fit object from the selected model. Perform a Ljung-Box test and conclude. Are the assumptions fulfilled? What are the required and “nice-to-have” assumptions? Would you assume normality for producing prediction intervals in this case?\nPrint the report for your selected model. Based on the report, set up the model equation using the backshift notation.\nBased on the equation in the previous exercise, find an expression for the first point forecast (\\widehat y_{T+1|T}).\nMartin Jullumn’s model on dinstrompris.no is a dynamic regression model with a categorical weekday dummy and ARIMA errors. Fit such a model a compare it with the ARIMA model of choice so far using an accuracy measure of choice (e.g. RMSE). Use a 4 weeks forecast horizon (same as length of February). Jullum do not use any transformations in his model so our model will be different. See code hint below for how this model can be implemented. Check the residual plots for the best model.\n\n\n\nCode hint:\nmodel(\n  jullum = ARIMA(log(1+price) ~ wday(date, label = TRUE), approx = FALSE, stepwise = FALSE)\n)\n\n\n\n\n\nWe will now set up a Monte Carlo simulation for forecasting the average monthly electricity price for February. We simulate daily February prices from the best model and caluclate the monthly average price. This gives us a simulated sample of monthly average price of February and we can estimate the forecast distribution of that variable.\n\nWe continue with the best model based on the test set performance above. We have found our forecasting model, now it is time to set it into action. Imagine that it is January 31^{\\text{st}} 2023 today, and we want to forecast the monthly average electricity price for February 2023. We will complicate this further in the next question, but start refitting the selected model using observations including Jan 31st 2023. Plot a forecast for daily prices in February. Does the forecast behave as you expect? Generate 500 potential February prices from the fitted model. Calculate the monthly average price for each of the simulations (should give 500 average February prices). Present the mean, standard deviation and 95% prediction interval for the monthly average price.\n\n\n\nCode hint:\nfit %&gt;% forecast(h = 28) %&gt;% autoplot()\n# Simulate: \nsims &lt;- fit %&gt;%  generate(h = 28, times = 500)\nsims %&gt;% \n  index_by(month = ~yearmonth(.)) %&gt;% \n  group_by(.rep) %&gt;%  \n  # Calculate monthly average price for the simulations\n  summarize(average = mean(.sim)) %&gt;% \n  ungroup() %&gt;% \n  # Monte Carlo estimates: \n  summarize(\n    mean = mean(average),\n    sd   = sd(average),\n    lwr  = quantile(average, prob = 0.025),\n    upr  = quantile(average, prob = 0.975)\n  )\n\n\n\nIn this final exercise, we pretend that the we are doing the same thing as dinstrompris.no. We start from Jan 31st of 2023 with the mean, standard deviation and 95% prediction interval from exercise 14 (i.e. the forecast of the average price for February). Then a day goes by, it is now Feb 1st and we know the daily average price of Feb 1st, so we update our model with the new information and produce a new forecast for the remaining 27 days of the month and produce a mean, sd, prediction interval for the average price of the full month, where 1 day is observed and the rest is simulated. Continue adding days and updating the forecast until all days of February are observed. Create a nice graphic similar to the one below. Explain the behavior of the forecast we see in the figure.\n\nPS: The last exercise is extra challenging from a programming point of view. If you don’t know how to attack this, you may just comment on the figure.\n The red dashed line is the actual average price of February and the date on the x-axis is the first day of the forecasting horizon (Feb 1st means data up to Jan 31st is used)."
  },
  {
    "objectID": "W2_benchmarksmethods.html",
    "href": "W2_benchmarksmethods.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "The point of this workshop is to learn the workflow of any forecasting task. It is based on the curriculum of the course up to and including “Forecasters toolbox”. At this point, we have learned the naive, seasonal naive, random walk with drift methods. We refer to them as simple, basic or benchmark methods. Any “more fancy” model should outperform these simple ones (otherwise; why bother?).\nSay we are predicting the temperature in Bergen tomorrow. The temperature tomorrow will most likely be very similar to today. Hence, one would expect a naive model to do very well. If we increase the forecast horizon, the temperature today will probably not be such a good forecaster. At some point, the temperature on the same day last year will probably be a better one (seasonal naive model)."
  },
  {
    "objectID": "W2_benchmarksmethods.html#ws2-the-benchmark-methods",
    "href": "W2_benchmarksmethods.html#ws2-the-benchmark-methods",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "The point of this workshop is to learn the workflow of any forecasting task. It is based on the curriculum of the course up to and including “Forecasters toolbox”. At this point, we have learned the naive, seasonal naive, random walk with drift methods. We refer to them as simple, basic or benchmark methods. Any “more fancy” model should outperform these simple ones (otherwise; why bother?).\nSay we are predicting the temperature in Bergen tomorrow. The temperature tomorrow will most likely be very similar to today. Hence, one would expect a naive model to do very well. If we increase the forecast horizon, the temperature today will probably not be such a good forecaster. At some point, the temperature on the same day last year will probably be a better one (seasonal naive model)."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This course and website has been developed by Sondre Hølleland, assistant professor at Norwegian School of Economics, Department of Business and Management Science. More about me will be added."
  },
  {
    "objectID": "9_garch.html",
    "href": "9_garch.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Warning: package 'ggplot2' was built under R version 4.3.2\n\n\nThe ARCH model introduced by Engle (1982) was later extended to a generalized ARCH (GARCH) by Bollerslev (1986). The generalization is kind of equivalent to going from an AR model to an ARMA model (d=0). Similar to a ARCH, we can define a GARCH(r,s) as: \\begin{equation*}\n\\begin{split}\ny_t &= \\sigma_t\\,\\varepsilon_t,\\\\\n\\sigma_t^2 &=\\omega +\\alpha_1y_{t-1}^2+\\cdots+\\alpha_ry_{t-r}^2 + \\beta_1\\sigma_{t-1}^2+\\cdots+\\beta_s\\sigma_{t-s}^2,\n\\end{split}\n\\end{equation*} or \\begin{equation*}\n\\begin{split}\ny_t &= \\sigma_t\\,\\varepsilon_t,\\\\\n\\sigma_t^2 &=\\omega +\\sum_{i=1}^r\\alpha_iy_{t-i}^2+\\sum_{j=1}^s \\beta_j\\sigma_{t-j}^2.\n\\end{split}\n\\end{equation*} Here \\omega&gt;0, \\alpha_i\\ge0 for i=1,\\ldots, r and \\beta_j\\ge 0 for j=1,\\ldots, s. We need however that \\alpha_r,\\beta_s&gt;0, otherwise the model order can be reduced to e.g. (r-1, s-1) if both are zero. Same as for the ARCH model, \\varepsilon_t\\sim \\text{iid WN}(0,1). Note that if \\beta_1=\\beta_2=\\cdots=\\beta_s=0, the model reduces to a ARCH(r) model, i.e. a GARCH(r,0)=ARCH(r) and ARCH is a special case of GARCH.\nAnother special case, which is also maybe the most common GARCH model is the GARCH(1,1): \\begin{equation*}\n\\begin{split}\ny_t &= \\sigma_t\\,\\varepsilon_t,\\\\\n\\sigma_t^2 &=\\omega +\\alpha_1y_{t-1}^2+ \\beta_1\\sigma_{t-1}^2.\n\\end{split}\n\\end{equation*} We focus on this special case when considering some characteristics of the model.\n\n\nExpectation is still the same as for the ARCH models: \\mathbb E\\,y_t = \\mathbb E\\,\\sigma_t\\mathbb E\\,\\varepsilon_t = 0. The variance calculation is a bit different now that we have the GARCH component. Assuming stationarity, we have that \\sigma^2 = \\mathrm{Var}(y_t) =\\mathbb E\\,y_t^2&lt;\\infty. Thus \\sigma^2= \\mathbb E\\sigma_t^2 \\mathbb E\\,\\varepsilon_t^2 =\\mathbb E\\sigma_t^2=\\omega+\\alpha_1\\mathbb E\\,y_{t-1}^2+\\beta_1 \\mathbb E\\,\\sigma_{t-1}^2 Since y_t is stationary and the variance does not depend on t, we get that \\mathbb E\\,y_{t-1}^2 =\\mathbb E\\,\\sigma_{t-1}^2 = \\sigma^2, and thus \\sigma^2 =\\omega+\\alpha_1 \\sigma^2+\\beta_1\\sigma^2\\quad \\Leftrightarrow \\quad \\sigma^2 =\\frac{\\omega}{1-\\alpha_1-\\beta_1}. Clearly, we need that \\alpha_1+\\beta_1&lt;1. This is in fact a necessary and sufficient condition for stationarity of a GARCH process.\n\n\n\nAs we have seen, for an ARCH(r) model there is an AR(r) representation. Similarly, where is an ARMA(r,s) representation for a GARCH(r,s) model. Using the same approach as for the ARCH case (with r=s=1), we have that y_t^2= \\sigma_t^2 +v_t,\\quad v_t = \\sigma_t^2(\\varepsilon_t^2-1). Note that this also means that \\sigma_t^2 = y_t^2 -v_t. Inserting the definition of \\sigma_t^2 according to a GARCH(1,1) we get that y_t^2 = \\omega+\\alpha_1y_{t-1}^2 +\\beta_1\\underbrace{\\sigma_{t-1}^2}_{y_{t-1}^2-v_{t-1}} +v_t = \\omega+(\\alpha_1+\\beta_1)y_{t-1}^2-\\beta_1v_{t-1}+ v_t. We can write this as (1-(\\alpha_1+\\beta_1)\\,B)\\,y_t^2 = \\omega+(1-\\beta_1\\,B)v_t, which we can recognize as an ARMA(1,1) with intercept c=\\omega, \\phi_1 = \\alpha_1+\\beta_1, \\theta_1 = -\\beta_1 and v_t is the white noise process (assuming \\mathbb E\\,y_t^4&lt;\\infty).\n\n\n\nLet us simulate an GARCH(1,1) model by basic R code, assuming Gaussian innovations.\n\nlibrary(fpp3)\nlibrary(tidyverse)\n# Setting seed for reproduciblity\nset.seed(12345)\nnT = 1000\nomg = 1.2\na1 = 0.4\nb1 = 0.5\n# Initiating y and sigma\ny &lt;- sig &lt;- rep(sqrt(omg/(1-a1-b1)),\n                nT)\n# Simulation: \nfor(t in 2:nT){\n  sig[t] &lt;- sqrt(omg+a1*y[t-1]^2+b1*sig[t-1]^2)\n  y[t]   &lt;- sig[t]*rnorm(1)\n}\n# tsibble object: \ngarch &lt;- tsibble(\n  t = 1:nT,\n  y = y,\n  sig = sig,\n  index = t\n) \n# Plotting: \ngarch %&gt;% pivot_longer(-t) %&gt;% \n  mutate(name = factor(name, levels = c(\"y\",\"sig\"))) %&gt;% \n  ggplot(aes(x=t,y=value)) + \n  geom_line()+ \n  facet_wrap(~name, scales = \"free_y\", nrow=2, \n             strip.position = \"left\")+\n  theme(axis.title =element_blank())\n\n\n\n\nWe see similarities with the ARCH model. Perhaps there is a tendency for the clusters to be a bit more persistent.\nLet’s look at the autocorrelation of the simulated time series and its squared values. We also do a qq-plot and a distribution plot.\n\ngarch %&gt;% ACF(y)   %&gt;% autoplot() + labs(y = \"ACF of y\")\ngarch %&gt;% ACF(y^2) %&gt;% autoplot() + labs(y = \"ACF of squared y\")\ngarch %&gt;% ggplot(aes(sample = y)) + geom_qq() +geom_qq_line()\ngarch %&gt;% ggplot(aes(x = y))      + stat_density(fill = \"blue\", alpha = .2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe tendency is similar to the ARCH model. There is little autocorrelation and heavy tails in the values, but profound autocorrelation for the squared values.\nIn this course we will assume normality of the residuals for simplicity, but you can also use other distributions as long as they are standardized to have zero expectation and unit variance. We will come back to this when looking at the implementation."
  },
  {
    "objectID": "9_garch.html#garch-models",
    "href": "9_garch.html#garch-models",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Warning: package 'ggplot2' was built under R version 4.3.2\n\n\nThe ARCH model introduced by Engle (1982) was later extended to a generalized ARCH (GARCH) by Bollerslev (1986). The generalization is kind of equivalent to going from an AR model to an ARMA model (d=0). Similar to a ARCH, we can define a GARCH(r,s) as: \\begin{equation*}\n\\begin{split}\ny_t &= \\sigma_t\\,\\varepsilon_t,\\\\\n\\sigma_t^2 &=\\omega +\\alpha_1y_{t-1}^2+\\cdots+\\alpha_ry_{t-r}^2 + \\beta_1\\sigma_{t-1}^2+\\cdots+\\beta_s\\sigma_{t-s}^2,\n\\end{split}\n\\end{equation*} or \\begin{equation*}\n\\begin{split}\ny_t &= \\sigma_t\\,\\varepsilon_t,\\\\\n\\sigma_t^2 &=\\omega +\\sum_{i=1}^r\\alpha_iy_{t-i}^2+\\sum_{j=1}^s \\beta_j\\sigma_{t-j}^2.\n\\end{split}\n\\end{equation*} Here \\omega&gt;0, \\alpha_i\\ge0 for i=1,\\ldots, r and \\beta_j\\ge 0 for j=1,\\ldots, s. We need however that \\alpha_r,\\beta_s&gt;0, otherwise the model order can be reduced to e.g. (r-1, s-1) if both are zero. Same as for the ARCH model, \\varepsilon_t\\sim \\text{iid WN}(0,1). Note that if \\beta_1=\\beta_2=\\cdots=\\beta_s=0, the model reduces to a ARCH(r) model, i.e. a GARCH(r,0)=ARCH(r) and ARCH is a special case of GARCH.\nAnother special case, which is also maybe the most common GARCH model is the GARCH(1,1): \\begin{equation*}\n\\begin{split}\ny_t &= \\sigma_t\\,\\varepsilon_t,\\\\\n\\sigma_t^2 &=\\omega +\\alpha_1y_{t-1}^2+ \\beta_1\\sigma_{t-1}^2.\n\\end{split}\n\\end{equation*} We focus on this special case when considering some characteristics of the model.\n\n\nExpectation is still the same as for the ARCH models: \\mathbb E\\,y_t = \\mathbb E\\,\\sigma_t\\mathbb E\\,\\varepsilon_t = 0. The variance calculation is a bit different now that we have the GARCH component. Assuming stationarity, we have that \\sigma^2 = \\mathrm{Var}(y_t) =\\mathbb E\\,y_t^2&lt;\\infty. Thus \\sigma^2= \\mathbb E\\sigma_t^2 \\mathbb E\\,\\varepsilon_t^2 =\\mathbb E\\sigma_t^2=\\omega+\\alpha_1\\mathbb E\\,y_{t-1}^2+\\beta_1 \\mathbb E\\,\\sigma_{t-1}^2 Since y_t is stationary and the variance does not depend on t, we get that \\mathbb E\\,y_{t-1}^2 =\\mathbb E\\,\\sigma_{t-1}^2 = \\sigma^2, and thus \\sigma^2 =\\omega+\\alpha_1 \\sigma^2+\\beta_1\\sigma^2\\quad \\Leftrightarrow \\quad \\sigma^2 =\\frac{\\omega}{1-\\alpha_1-\\beta_1}. Clearly, we need that \\alpha_1+\\beta_1&lt;1. This is in fact a necessary and sufficient condition for stationarity of a GARCH process.\n\n\n\nAs we have seen, for an ARCH(r) model there is an AR(r) representation. Similarly, where is an ARMA(r,s) representation for a GARCH(r,s) model. Using the same approach as for the ARCH case (with r=s=1), we have that y_t^2= \\sigma_t^2 +v_t,\\quad v_t = \\sigma_t^2(\\varepsilon_t^2-1). Note that this also means that \\sigma_t^2 = y_t^2 -v_t. Inserting the definition of \\sigma_t^2 according to a GARCH(1,1) we get that y_t^2 = \\omega+\\alpha_1y_{t-1}^2 +\\beta_1\\underbrace{\\sigma_{t-1}^2}_{y_{t-1}^2-v_{t-1}} +v_t = \\omega+(\\alpha_1+\\beta_1)y_{t-1}^2-\\beta_1v_{t-1}+ v_t. We can write this as (1-(\\alpha_1+\\beta_1)\\,B)\\,y_t^2 = \\omega+(1-\\beta_1\\,B)v_t, which we can recognize as an ARMA(1,1) with intercept c=\\omega, \\phi_1 = \\alpha_1+\\beta_1, \\theta_1 = -\\beta_1 and v_t is the white noise process (assuming \\mathbb E\\,y_t^4&lt;\\infty).\n\n\n\nLet us simulate an GARCH(1,1) model by basic R code, assuming Gaussian innovations.\n\nlibrary(fpp3)\nlibrary(tidyverse)\n# Setting seed for reproduciblity\nset.seed(12345)\nnT = 1000\nomg = 1.2\na1 = 0.4\nb1 = 0.5\n# Initiating y and sigma\ny &lt;- sig &lt;- rep(sqrt(omg/(1-a1-b1)),\n                nT)\n# Simulation: \nfor(t in 2:nT){\n  sig[t] &lt;- sqrt(omg+a1*y[t-1]^2+b1*sig[t-1]^2)\n  y[t]   &lt;- sig[t]*rnorm(1)\n}\n# tsibble object: \ngarch &lt;- tsibble(\n  t = 1:nT,\n  y = y,\n  sig = sig,\n  index = t\n) \n# Plotting: \ngarch %&gt;% pivot_longer(-t) %&gt;% \n  mutate(name = factor(name, levels = c(\"y\",\"sig\"))) %&gt;% \n  ggplot(aes(x=t,y=value)) + \n  geom_line()+ \n  facet_wrap(~name, scales = \"free_y\", nrow=2, \n             strip.position = \"left\")+\n  theme(axis.title =element_blank())\n\n\n\n\nWe see similarities with the ARCH model. Perhaps there is a tendency for the clusters to be a bit more persistent.\nLet’s look at the autocorrelation of the simulated time series and its squared values. We also do a qq-plot and a distribution plot.\n\ngarch %&gt;% ACF(y)   %&gt;% autoplot() + labs(y = \"ACF of y\")\ngarch %&gt;% ACF(y^2) %&gt;% autoplot() + labs(y = \"ACF of squared y\")\ngarch %&gt;% ggplot(aes(sample = y)) + geom_qq() +geom_qq_line()\ngarch %&gt;% ggplot(aes(x = y))      + stat_density(fill = \"blue\", alpha = .2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe tendency is similar to the ARCH model. There is little autocorrelation and heavy tails in the values, but profound autocorrelation for the squared values.\nIn this course we will assume normality of the residuals for simplicity, but you can also use other distributions as long as they are standardized to have zero expectation and unit variance. We will come back to this when looking at the implementation."
  },
  {
    "objectID": "9_garch.html#references",
    "href": "9_garch.html#references",
    "title": "BAN430 Forecasting",
    "section": "References",
    "text": "References\n\nBollerslev, T. (1986). Generalized autoregressive conditional heteroskedasticity. Journal of econometrics, 31(3), 307-327.\nEngle, R. F. (1982). Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation. Econometrica: Journal of the econometric society, 987-1007."
  },
  {
    "objectID": "9_forecasting_garch_R.html",
    "href": "9_forecasting_garch_R.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "The fable package that we are using for everything else does not cover volatility modeling. We could maybe trick it, by using the ARMA representation of a GARCH model and back-transforming the estimates to the GARCH parameters, but this is not very convenient. It is also useful for you as students to see something other than the tidy packages.\nWe use a package called rugarch for fitting and forecasting GARCH models. You find the vignette Introduction to the rugarch package here. Install and load the package.\n\ninstall.packages(\"rugarch\")\nlibrary(rugarch)\n\nWe continue with the Microsoft closing price returns as example. We cannot use the tsibble structure with the functions from this packages, so we will transform the Microsoft data to an xts object. This is also a time series object. We leave 200 observations for testing.\n\nMSFT &lt;- MSFT %&gt;% filter(!is.na(return)) # Removing NAs\nmsft &lt;- xts(MSFT$return,          # values of time series\n            order.by = MSFT$date) # dates of time series\n\ntrain &lt;- msft[1:(length(msft)-200)]\n\nWe start by specifying the model. In this case, we want to fit a GARCH(1,1) without mean. We use a Gaussian assumption on the distribution of \\varepsilon_t. We do not want to use a model for the mean, and therefore explicitly state that the arma-order is (0,0).\n\nmodel &lt;- ugarchspec(variance.model = list(model = \"sGARCH\", \n                                          garchOrder = c(1,1)),\n                    mean.model = list(armaOrder = c(0,0), include.mean = FALSE),\n                    distribution.model = \"norm\")\n\nOnce we have the specification ready, we can fit the model, using the ugarchfit function. This need a model spec and the data.\n\nfit &lt;- ugarchfit(spec = model, \n                 data = train)\n\nLet’s look at the model output:\n\nprint(fit)\n\n\n*---------------------------------*\n*          GARCH Model Fit        *\n*---------------------------------*\n\nConditional Variance Dynamics   \n-----------------------------------\nGARCH Model : sGARCH(1,1)\nMean Model  : ARFIMA(0,0,0)\nDistribution    : norm \n\nOptimal Parameters\n------------------------------------\n        Estimate  Std. Error  t value Pr(&gt;|t|)\nomega   0.000019    0.000004   4.4096    1e-05\nalpha1  0.161813    0.029780   5.4336    0e+00\nbeta1   0.783611    0.035951  21.7969    0e+00\n\nRobust Standard Errors:\n        Estimate  Std. Error  t value Pr(&gt;|t|)\nomega   0.000019    0.000010   1.9194 0.054934\nalpha1  0.161813    0.061175   2.6451 0.008167\nbeta1   0.783611    0.077296  10.1378 0.000000\n\nLogLikelihood : 5670.344 \n\nInformation Criteria\n------------------------------------\n                    \nAkaike       -5.4520\nBayes        -5.4439\nShibata      -5.4520\nHannan-Quinn -5.4490\n\nWeighted Ljung-Box Test on Standardized Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                      4.241 0.03946\nLag[2*(p+q)+(p+q)-1][2]     4.625 0.05126\nLag[4*(p+q)+(p+q)-1][5]     5.225 0.13611\nd.o.f=0\nH0 : No serial correlation\n\nWeighted Ljung-Box Test on Standardized Squared Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                     0.2090  0.6476\nLag[2*(p+q)+(p+q)-1][5]    0.7333  0.9164\nLag[4*(p+q)+(p+q)-1][9]    2.0299  0.9014\nd.o.f=2\n\nWeighted ARCH LM Tests\n------------------------------------\n            Statistic Shape Scale P-Value\nARCH Lag[3]    0.1233 0.500 2.000  0.7255\nARCH Lag[5]    0.4797 1.440 1.667  0.8896\nARCH Lag[7]    1.2375 2.315 1.543  0.8721\n\nNyblom stability test\n------------------------------------\nJoint Statistic:  1.3577\nIndividual Statistics:             \nomega  0.2311\nalpha1 0.1927\nbeta1  0.3022\n\nAsymptotic Critical Values (10% 5% 1%)\nJoint Statistic:         0.846 1.01 1.35\nIndividual Statistic:    0.35 0.47 0.75\n\nSign Bias Test\n------------------------------------\n                   t-value   prob sig\nSign Bias          0.09801 0.9219    \nNegative Sign Bias 1.09331 0.2744    \nPositive Sign Bias 0.54723 0.5843    \nJoint Effect       2.36637 0.4999    \n\n\nAdjusted Pearson Goodness-of-Fit Test:\n------------------------------------\n  group statistic p-value(g-1)\n1    20     136.4    8.800e-20\n2    30     145.3    1.981e-17\n3    40     155.3    8.321e-16\n4    50     173.1    9.328e-16\n\n\nElapsed time : 0.1291599 \n\n\nIt is very long with a lot of test results, but the main infromation is the parameter estimates. In this case, we estimate a model with \\omega = 0.000019, \\alpha_1 = 0.162506 and \\beta_1 = 0.783419. Note that \\alpha_1+\\beta_1 = 0.946 &lt;1, indicating that we have a stationary GARCH(1,1) model (given other model assumptions are fulfilled).\nLet’s plot the time series with the fitted \\sigma_t as a confidence interval.\n\nplot(fit, which = 1)\n\n\n\n\nThe plotting function has many plots, which can be specified using the which argument. You can also run plot(fit) and select from a list of 12 plots in the console, many of which can be used to assess the residuals. The list of plots are given below.\n\nMake a plot selection (or 0 to exit): \n\n 1:   Series with 2 Conditional SD Superimposed\n 2:   Series with 1% VaR Limits\n 3:   Conditional SD (vs |returns|)\n 4:   ACF of Observations\n 5:   ACF of Squared Observations\n 6:   ACF of Absolute Observations\n 7:   Cross Correlation\n 8:   Empirical Density of Standardized Residuals\n 9:   QQ-Plot of Standardized Residuals\n10:   ACF of Standardized Residuals\n11:   ACF of Squared Standardized Residuals\n12:   News-Impact Curve\n\nLet’s look the residual related plots:\n\npar(mfrow = c(2,2))\nplot(fit, which = 8)\nplot(fit, which = 9)\nplot(fit, which = 10)\nplot(fit, which = 11)\n\n\n\n\nWe can see some heavy tail behavor. Maybe we should try a t-distribution instead.\n\nmodel2 &lt;- ugarchspec(variance.model = list(model = \"sGARCH\", \n                                          garchOrder = c(1,1)),\n                    mean.model = list(armaOrder = c(0,0), include.mean = FALSE),\n                    distribution.model = \"std\")\nfit2 &lt;- ugarchfit(spec = model2, \n                 data = train)\npar(mfrow = c(2,2))\nplot(fit2, which = 8)\nplot(fit2, which = 9)\nplot(fit2, which = 10)\nplot(fit2, which = 11)\n\n\n\n\nThis looks much better. Let us use this as our model and forecast 10 steps ahead.\nIn this case, width of the prediction interval is slowly increasing. We can have a look at the long term forecast, increasing the n.ahead to 500.\nThe ugarchforecast function also has a rolling forecast option, which is more relevant in this context. The point is to do a short term forecast, but add information as time goes by. The argument n.roll controlls how many rolling forecast should be performed and needs to be run with the out.sample argument, which holds out observations for the forecast. We need to set out.sample \\ge n.roll.\n\nspec = getspec(fit2);\nsetfixed(spec) &lt;- as.list(coef(fit2));\nfc &lt;- ugarchforecast(spec, \n                     data = msft,\n                     n.ahead = 1, \n                     n.roll = 200,\n                     out.sample =200)\nplot(fc, which = 2)\n\n\n\n\nClearly, you get more dynamics by doing a rolling forecast, than forecasting 100 steps ahead. This is also the typical application. How will the return distribution of a stock look like tomorrow?\nWe can also calculate accuracy measures on forecast. This is more relevant for ARMA-GARCH models.\n\nfpm(fc) # forecast performance measures\n\n           MSE        MAE DAC\n1 0.0001998012 0.01070255   0\n\n\nWe can simulate from a fitted model using the ugarchsim function. Check out the help file."
  },
  {
    "objectID": "9_forecasting_garch_R.html#forecasting-volatility-in-r",
    "href": "9_forecasting_garch_R.html#forecasting-volatility-in-r",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "The fable package that we are using for everything else does not cover volatility modeling. We could maybe trick it, by using the ARMA representation of a GARCH model and back-transforming the estimates to the GARCH parameters, but this is not very convenient. It is also useful for you as students to see something other than the tidy packages.\nWe use a package called rugarch for fitting and forecasting GARCH models. You find the vignette Introduction to the rugarch package here. Install and load the package.\n\ninstall.packages(\"rugarch\")\nlibrary(rugarch)\n\nWe continue with the Microsoft closing price returns as example. We cannot use the tsibble structure with the functions from this packages, so we will transform the Microsoft data to an xts object. This is also a time series object. We leave 200 observations for testing.\n\nMSFT &lt;- MSFT %&gt;% filter(!is.na(return)) # Removing NAs\nmsft &lt;- xts(MSFT$return,          # values of time series\n            order.by = MSFT$date) # dates of time series\n\ntrain &lt;- msft[1:(length(msft)-200)]\n\nWe start by specifying the model. In this case, we want to fit a GARCH(1,1) without mean. We use a Gaussian assumption on the distribution of \\varepsilon_t. We do not want to use a model for the mean, and therefore explicitly state that the arma-order is (0,0).\n\nmodel &lt;- ugarchspec(variance.model = list(model = \"sGARCH\", \n                                          garchOrder = c(1,1)),\n                    mean.model = list(armaOrder = c(0,0), include.mean = FALSE),\n                    distribution.model = \"norm\")\n\nOnce we have the specification ready, we can fit the model, using the ugarchfit function. This need a model spec and the data.\n\nfit &lt;- ugarchfit(spec = model, \n                 data = train)\n\nLet’s look at the model output:\n\nprint(fit)\n\n\n*---------------------------------*\n*          GARCH Model Fit        *\n*---------------------------------*\n\nConditional Variance Dynamics   \n-----------------------------------\nGARCH Model : sGARCH(1,1)\nMean Model  : ARFIMA(0,0,0)\nDistribution    : norm \n\nOptimal Parameters\n------------------------------------\n        Estimate  Std. Error  t value Pr(&gt;|t|)\nomega   0.000019    0.000004   4.4096    1e-05\nalpha1  0.161813    0.029780   5.4336    0e+00\nbeta1   0.783611    0.035951  21.7969    0e+00\n\nRobust Standard Errors:\n        Estimate  Std. Error  t value Pr(&gt;|t|)\nomega   0.000019    0.000010   1.9194 0.054934\nalpha1  0.161813    0.061175   2.6451 0.008167\nbeta1   0.783611    0.077296  10.1378 0.000000\n\nLogLikelihood : 5670.344 \n\nInformation Criteria\n------------------------------------\n                    \nAkaike       -5.4520\nBayes        -5.4439\nShibata      -5.4520\nHannan-Quinn -5.4490\n\nWeighted Ljung-Box Test on Standardized Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                      4.241 0.03946\nLag[2*(p+q)+(p+q)-1][2]     4.625 0.05126\nLag[4*(p+q)+(p+q)-1][5]     5.225 0.13611\nd.o.f=0\nH0 : No serial correlation\n\nWeighted Ljung-Box Test on Standardized Squared Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                     0.2090  0.6476\nLag[2*(p+q)+(p+q)-1][5]    0.7333  0.9164\nLag[4*(p+q)+(p+q)-1][9]    2.0299  0.9014\nd.o.f=2\n\nWeighted ARCH LM Tests\n------------------------------------\n            Statistic Shape Scale P-Value\nARCH Lag[3]    0.1233 0.500 2.000  0.7255\nARCH Lag[5]    0.4797 1.440 1.667  0.8896\nARCH Lag[7]    1.2375 2.315 1.543  0.8721\n\nNyblom stability test\n------------------------------------\nJoint Statistic:  1.3577\nIndividual Statistics:             \nomega  0.2311\nalpha1 0.1927\nbeta1  0.3022\n\nAsymptotic Critical Values (10% 5% 1%)\nJoint Statistic:         0.846 1.01 1.35\nIndividual Statistic:    0.35 0.47 0.75\n\nSign Bias Test\n------------------------------------\n                   t-value   prob sig\nSign Bias          0.09801 0.9219    \nNegative Sign Bias 1.09331 0.2744    \nPositive Sign Bias 0.54723 0.5843    \nJoint Effect       2.36637 0.4999    \n\n\nAdjusted Pearson Goodness-of-Fit Test:\n------------------------------------\n  group statistic p-value(g-1)\n1    20     136.4    8.800e-20\n2    30     145.3    1.981e-17\n3    40     155.3    8.321e-16\n4    50     173.1    9.328e-16\n\n\nElapsed time : 0.1291599 \n\n\nIt is very long with a lot of test results, but the main infromation is the parameter estimates. In this case, we estimate a model with \\omega = 0.000019, \\alpha_1 = 0.162506 and \\beta_1 = 0.783419. Note that \\alpha_1+\\beta_1 = 0.946 &lt;1, indicating that we have a stationary GARCH(1,1) model (given other model assumptions are fulfilled).\nLet’s plot the time series with the fitted \\sigma_t as a confidence interval.\n\nplot(fit, which = 1)\n\n\n\n\nThe plotting function has many plots, which can be specified using the which argument. You can also run plot(fit) and select from a list of 12 plots in the console, many of which can be used to assess the residuals. The list of plots are given below.\n\nMake a plot selection (or 0 to exit): \n\n 1:   Series with 2 Conditional SD Superimposed\n 2:   Series with 1% VaR Limits\n 3:   Conditional SD (vs |returns|)\n 4:   ACF of Observations\n 5:   ACF of Squared Observations\n 6:   ACF of Absolute Observations\n 7:   Cross Correlation\n 8:   Empirical Density of Standardized Residuals\n 9:   QQ-Plot of Standardized Residuals\n10:   ACF of Standardized Residuals\n11:   ACF of Squared Standardized Residuals\n12:   News-Impact Curve\n\nLet’s look the residual related plots:\n\npar(mfrow = c(2,2))\nplot(fit, which = 8)\nplot(fit, which = 9)\nplot(fit, which = 10)\nplot(fit, which = 11)\n\n\n\n\nWe can see some heavy tail behavor. Maybe we should try a t-distribution instead.\n\nmodel2 &lt;- ugarchspec(variance.model = list(model = \"sGARCH\", \n                                          garchOrder = c(1,1)),\n                    mean.model = list(armaOrder = c(0,0), include.mean = FALSE),\n                    distribution.model = \"std\")\nfit2 &lt;- ugarchfit(spec = model2, \n                 data = train)\npar(mfrow = c(2,2))\nplot(fit2, which = 8)\nplot(fit2, which = 9)\nplot(fit2, which = 10)\nplot(fit2, which = 11)\n\n\n\n\nThis looks much better. Let us use this as our model and forecast 10 steps ahead.\nIn this case, width of the prediction interval is slowly increasing. We can have a look at the long term forecast, increasing the n.ahead to 500.\nThe ugarchforecast function also has a rolling forecast option, which is more relevant in this context. The point is to do a short term forecast, but add information as time goes by. The argument n.roll controlls how many rolling forecast should be performed and needs to be run with the out.sample argument, which holds out observations for the forecast. We need to set out.sample \\ge n.roll.\n\nspec = getspec(fit2);\nsetfixed(spec) &lt;- as.list(coef(fit2));\nfc &lt;- ugarchforecast(spec, \n                     data = msft,\n                     n.ahead = 1, \n                     n.roll = 200,\n                     out.sample =200)\nplot(fc, which = 2)\n\n\n\n\nClearly, you get more dynamics by doing a rolling forecast, than forecasting 100 steps ahead. This is also the typical application. How will the return distribution of a stock look like tomorrow?\nWe can also calculate accuracy measures on forecast. This is more relevant for ARMA-GARCH models.\n\nfpm(fc) # forecast performance measures\n\n           MSE        MAE DAC\n1 0.0001998012 0.01070255   0\n\n\nWe can simulate from a fitted model using the ugarchsim function. Check out the help file."
  },
  {
    "objectID": "9_forecasting_garch_R.html#arma-garch",
    "href": "9_forecasting_garch_R.html#arma-garch",
    "title": "BAN430 Forecasting",
    "section": "ARMA-GARCH",
    "text": "ARMA-GARCH\nWe can also fit a ARMA-GARCH model to the data.\n\nmodel3 &lt;- ugarchspec(variance.model = list(model = \"sGARCH\", \n                                          garchOrder = c(1,1)),\n                    mean.model = list(armaOrder = c(1,1), include.mean = TRUE),\n                    distribution.model = \"std\")\nfit3 &lt;- ugarchfit(spec = model3, \n                 data = train)\n\nYou should of course, check model assumption by considering the residuals. Let’s just check out the forecast.\n\nspec = getspec(fit3);\nsetfixed(spec) &lt;- as.list(coef(fit3));\nfc3 &lt;- ugarchforecast(spec, \n                     data = msft,\n                     n.ahead = 1, \n                     n.roll = 200,\n                     out.sample =200)\nplot(fc3, which = 2)\n\n\n\nrbind(\"GARCH\"=fpm(fc),\n      \"ARMA-GARCH\"=fpm(fc3))\n\n                    MSE        MAE   DAC\nGARCH      0.0001998012 0.01070255 0.000\nARMA-GARCH 0.0001959884 0.01053788 0.605\n\n\nIt seems the pure GARCH model is better, except for the directional accuracy. Accurately predicting the return of tomorrows is difficult. But there may be other contexts where an ARMA-GARCH model is more suited."
  },
  {
    "objectID": "9_forecasting_garch_R.html#exercise",
    "href": "9_forecasting_garch_R.html#exercise",
    "title": "BAN430 Forecasting",
    "section": "Exercise",
    "text": "Exercise\n\nFit a ARCH(r), with r=1,2,3 model to Microsoft data and compare AIC with the GARCH(1,1). Would this any of these models be an improvement in terms of AIC?\nChoose another stock and fit an ARCH(1) and a GARCH(1,1) model to that. Consider using a t-distribution instead of normal for the residuals."
  },
  {
    "objectID": "9_forecasting_garch_R.html#references",
    "href": "9_forecasting_garch_R.html#references",
    "title": "BAN430 Forecasting",
    "section": "References",
    "text": "References\n\nEngle, R. F. (1982). Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation. Econometrica: Journal of the econometric society, 987-1007.\nMcNeil, A. J., Frey, R., & Embrechts, P. (2005). Quantitative risk management: concepts, techniques and tools-revised edition. Princeton university press."
  },
  {
    "objectID": "8_ARIMAmodels.html",
    "href": "8_ARIMAmodels.html",
    "title": "ARIMA models",
    "section": "",
    "text": "The following shiny app has been developed by Sondre Hølleland and being administred at https://sholleland.shinyapps.io/ban430_shinyapps. Due to restrictions relating to available computing hours on the free shinyapps account, the app may not work. You may then copy the code below to run the shiny app locally on your own computer. Remember that understanding the details of this code is not necessary.\n\n\nShiny app code:\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(fpp3)\nlibrary(ggpubr)\nsbp.width &lt;- 3\n\ntheme_set(theme_bw() + theme(panel.grid.major = element_blank(),\n                             panel.grid.minor = element_blank()))\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n\n    # Application title\n    titlePanel(\"ARMA models\"),\n    tabsetPanel(\n      tabPanel(\"AR(1)\", \n               \n               sidebarLayout(\n                 sidebarPanel(width = sbp.width,\n                              uiOutput(\"artext\"),\n                   sliderInput(\"arphi\",\n                               \"phi:\",\n                               min = -.99,\n                               max = .99,\n                               value = .7,\n                               step = .01),\n                   sliderInput(\"arsigma\",\n                               \"sigma:\",\n                               min = 0,\n                               max = 10,\n                               value = 1,\n                               step = .2),\n                   numericInput(\"arseed\",\n                                \"Seed:\",\n                                value = 1234,\n                                min = 0, \n                                max = 9999,\n                                step = 1),\n                   numericInput(\"arn\",\n                                \"Sample size:\",\n                                value = 500,\n                                min = 100, \n                                max = 5000,\n                                step = 100)\n                 ),\n                 \n                 # Show a plot of the generated distribution\n                 mainPanel(\n                   plotOutput(\"arPlot\", height = \"600px\")\n                 )\n               )),\n      tabPanel(\"MA(1)\",  \n               \n               sidebarLayout(\n                 sidebarPanel(width = sbp.width,\n                              uiOutput(\"matext\"),\n                   sliderInput(\"matheta\",\n                               \"theta:\",\n                               min = -1,\n                               max = 1,\n                               value = .7,\n                               step = .01),\n                   sliderInput(\"masigma\",\n                               \"sigma:\",\n                               min = 0,\n                               max = 10,\n                               value = 1,\n                               step = .2),\n                   numericInput(\"maseed\",\n                                \"Seed:\",\n                                value = 1234,\n                                min = 0, \n                                max = 9999,\n                                step = 1),\n                   numericInput(\"man\",\n                                \"Sample size:\",\n                                value = 500,\n                                min = 100, \n                                max = 5000,\n                                step = 100)\n                 ),\n                 \n                 # Show a plot of the generated distribution\n                 mainPanel(\n                   plotOutput(\"maPlot\", height = \"600px\")\n                 )\n               )),\n      tabPanel(\"ARMA(1,1)\", \n              \n               sidebarLayout(\n                 sidebarPanel(width = sbp.width,\n                              uiOutput(\"armatext\"),\n                   sliderInput(\"armaphi\",\n                               \"phi:\",\n                               min = -.99,\n                               max = .99,\n                               value = .7,\n                               step = .01),\n                   sliderInput(\"armatheta\",\n                               \"theta:\",\n                               min = -1,\n                               max = 1,\n                               value = .7,\n                               step = .01),\n                   sliderInput(\"armasigma\",\n                               \"sigma:\",\n                               min = 0,\n                               max = 10,\n                               value = 1,\n                               step = .2),\n                   numericInput(\"armaseed\",\n                                \"Seed:\",\n                                value = 1234,\n                                min = 0, \n                                max = 9999,\n                                step = 1),\n                   numericInput(\"arman\",\n                                \"Sample size:\",\n                                value = 500,\n                                min = 100, \n                                max = 5000,\n                                step = 100)\n                 ),\n                 \n                 # Show a plot of the generated distribution\n                 mainPanel(\n                   plotOutput(\"armaPlot\", height = \"600px\")\n                 )\n               ))))\n    # Sidebar with a slider input for number of bins \n   \n\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n    output$artext &lt;- renderUI({\n      withMathJax(\"Model: $$Y_t=\\\\phi \\\\,Y_{t-1}+Z_t,\\\\quad Z_t\\\\sim \\\\text{i.i.d N}(0,\\\\sigma^2)$$\")\n    })\n    output$matext &lt;- renderUI({\n      withMathJax(\"Model: $$Y_t=\\\\theta\\\\, Z_{t-1}+Z_t,\\\\quad Z_t\\\\sim \\\\text{i.i.d N}(0,\\\\sigma^2)$$\")\n    })\n    output$armatext &lt;- renderUI({\n      withMathJax(\"Model: $$Y_t=\\\\phi\\\\, Y_{t-1}+\\\\theta \\\\,Z_{t-1}+Z_t,\\\\quad Z_t\\\\sim \\\\text{i.i.d N}(0,\\\\sigma^2)$$\")\n    })\n    output$arPlot &lt;- renderPlot({\n      set.seed(input$arseed)\n      burnin &lt;- 200\n      x &lt;- rnorm(input$arn+burnin, sd = input$arsigma)\n      for(i in 2:length(x))\n        x[i] &lt;- input$arphi*x[i-1]+rnorm(1, sd = input$arsigma)\n      df &lt;- tsibble(x = x[burnin+1:(length(x)-burnin)],\n                    t = 1:(length(x)-burnin),\n                    index = \"t\")\n      # Theoretical acf: \n      \n      theoretical.correlations &lt;- \n        tibble(lag = 1:29,\n               acf = ARMAacf(ar = c(input$arphi), lag.max = 29, pacf =FALSE)[-1],\n               pacf = ARMAacf(ar = c(input$arphi), lag.max = 29, pacf =TRUE))\n      # generate bins based on input$bins from ui.R\n       ggarrange(\n         df %&gt;% autoplot() + scale_y_continuous(\"AR(1) series\")+\n           scale_x_continuous(\"Time index\", expand = c(0,0)),\n         ggarrange(df %&gt;% ACF() %&gt;% autoplot()+\n                     scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                     ylab(\"Sample PACF\"),\n         df %&gt;% PACF() %&gt;% autoplot()+\n           scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n           ylab(\"Sample PACF\"), \n         theoretical.correlations %&gt;% ggplot(aes(x=lag))+\n           geom_segment(aes(x=lag,xend=lag,y=0,yend=acf))+geom_hline(yintercept=0)+\n           scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n           scale_y_continuous(\"Theoretical ACF\"),\n         theoretical.correlations %&gt;% ggplot(aes(x=lag))+\n           geom_segment(aes(x=lag,xend=lag,y=0,yend=pacf))+geom_hline(yintercept=0)+\n           scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n           scale_y_continuous(\"Theoretical PACF\"),\n         nrow = 2, ncol = 2),\n         ncol = 1, nrow = 2, heights = c(1,2))\n    })\n\n    output$maPlot &lt;- renderPlot({\n      set.seed(input$maseed)\n      burnin &lt;- 200\n      z &lt;- rnorm(input$man+burnin, sd = input$masigma)\n      x &lt;- numeric(input$man+burnin)\n      for(i in 2:length(x))\n        x[i] &lt;- input$matheta*z[i-1]+z[i]\n      df &lt;- tsibble(x = x[burnin+1:(length(x)-burnin)],\n                    t = 1:(length(x)-burnin),\n                    index = \"t\")\n      \n      theoretical.correlations &lt;- \n        tibble(lag = 1:29,\n               acf = ARMAacf(ma = c(input$matheta), lag.max = 29, pacf =FALSE)[-1],\n               pacf = ARMAacf(ma = c(input$matheta), lag.max = 29, pacf =TRUE))\n      # generate bins based on input$bins from ui.R\n      ggarrange(\n        df %&gt;% autoplot() + scale_y_continuous(\"MA(1) series\")+\n          scale_x_continuous(\"Time index\", expand = c(0,0)),\n        ggarrange(df %&gt;% ACF() %&gt;% autoplot()+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                                         ylab(\"Sample ACF\"),\n                  df %&gt;% PACF() %&gt;% autoplot()+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                                         ylab(\"Sample PACF\"), \n                  theoretical.correlations %&gt;% ggplot(aes(x=lag))+\n                    geom_segment(aes(x=lag,xend=lag,y=0,yend=acf))+geom_hline(yintercept=0)+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                    scale_y_continuous(\"Theoretical ACF\"),\n                  theoretical.correlations %&gt;% ggplot(aes(x=lag))+\n                    geom_segment(aes(x=lag,xend=lag,y=0,yend=pacf))+geom_hline(yintercept=0)+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                    scale_y_continuous(\"Theoretical PACF\"),\n                  nrow = 2, ncol = 2),\n        ncol = 1, nrow = 2, heights = c(1,2))\n    })\n    output$armaPlot &lt;- renderPlot({\n      set.seed(input$armaseed)\n      burnin &lt;- 200\n      #sdZ = sqrt(input$armasigma *(1-input$armaphi^2)/(1+2*input$armaphi*input$armatheta + input$armatheta^2))\n      z &lt;- rnorm(input$arman+burnin, sd = input$armasigma)\n      x &lt;- numeric(input$arman+burnin)\n      for(i in 2:length(x))\n        x[i] &lt;- input$armaphi*x[i-1]+input$armatheta*z[i-1]+z[i]\n      df &lt;- tsibble(x = x[burnin+1:(length(x)-burnin)],\n                    t = 1:(length(x)-burnin),\n                    index = \"t\")\n      \n      theoretical.correlations &lt;- \n        tibble(lag = 1:29,\n               acf = ARMAacf(ar = c(input$armaphi), ma = c(input$armatheta), lag.max = 29, pacf =FALSE)[-1],\n               pacf = ARMAacf(ar = c(input$armaphi), ma = c(input$armatheta), lag.max = 29, pacf =TRUE))\n      # generate bins based on input$bins from ui.R\n      ggarrange(\n        df %&gt;% autoplot() + scale_y_continuous(\"MA(1) series\")+\n          scale_x_continuous(\"Time index\", expand = c(0,0)),\n        ggarrange(df %&gt;% ACF() %&gt;% autoplot()+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                                         ylab(\"Sample ACF\"),\n                  df %&gt;% PACF() %&gt;% autoplot()+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                                         ylab(\"Sample PACF\"), \n                  theoretical.correlations %&gt;% ggplot(aes(x=lag))+\n                    geom_segment(aes(x=lag,xend=lag,y=0,yend=acf))+geom_hline(yintercept=0)+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                    scale_y_continuous(\"Theoretical ACF\"),\n                  theoretical.correlations %&gt;% ggplot(aes(x=lag))+\n                    geom_segment(aes(x=lag,xend=lag,y=0,yend=pacf))+geom_hline(yintercept=0)+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                    scale_y_continuous(\"Theoretical PACF\"),\n                  nrow = 2, ncol = 2),\n        ncol = 1, nrow = 2, heights = c(1,2))\n    })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "8_ARIMAmodels.html#shiny-app-for-playing-with-acf-and-arma-models",
    "href": "8_ARIMAmodels.html#shiny-app-for-playing-with-acf-and-arma-models",
    "title": "ARIMA models",
    "section": "",
    "text": "The following shiny app has been developed by Sondre Hølleland and being administred at https://sholleland.shinyapps.io/ban430_shinyapps. Due to restrictions relating to available computing hours on the free shinyapps account, the app may not work. You may then copy the code below to run the shiny app locally on your own computer. Remember that understanding the details of this code is not necessary.\n\n\nShiny app code:\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(fpp3)\nlibrary(ggpubr)\nsbp.width &lt;- 3\n\ntheme_set(theme_bw() + theme(panel.grid.major = element_blank(),\n                             panel.grid.minor = element_blank()))\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n\n    # Application title\n    titlePanel(\"ARMA models\"),\n    tabsetPanel(\n      tabPanel(\"AR(1)\", \n               \n               sidebarLayout(\n                 sidebarPanel(width = sbp.width,\n                              uiOutput(\"artext\"),\n                   sliderInput(\"arphi\",\n                               \"phi:\",\n                               min = -.99,\n                               max = .99,\n                               value = .7,\n                               step = .01),\n                   sliderInput(\"arsigma\",\n                               \"sigma:\",\n                               min = 0,\n                               max = 10,\n                               value = 1,\n                               step = .2),\n                   numericInput(\"arseed\",\n                                \"Seed:\",\n                                value = 1234,\n                                min = 0, \n                                max = 9999,\n                                step = 1),\n                   numericInput(\"arn\",\n                                \"Sample size:\",\n                                value = 500,\n                                min = 100, \n                                max = 5000,\n                                step = 100)\n                 ),\n                 \n                 # Show a plot of the generated distribution\n                 mainPanel(\n                   plotOutput(\"arPlot\", height = \"600px\")\n                 )\n               )),\n      tabPanel(\"MA(1)\",  \n               \n               sidebarLayout(\n                 sidebarPanel(width = sbp.width,\n                              uiOutput(\"matext\"),\n                   sliderInput(\"matheta\",\n                               \"theta:\",\n                               min = -1,\n                               max = 1,\n                               value = .7,\n                               step = .01),\n                   sliderInput(\"masigma\",\n                               \"sigma:\",\n                               min = 0,\n                               max = 10,\n                               value = 1,\n                               step = .2),\n                   numericInput(\"maseed\",\n                                \"Seed:\",\n                                value = 1234,\n                                min = 0, \n                                max = 9999,\n                                step = 1),\n                   numericInput(\"man\",\n                                \"Sample size:\",\n                                value = 500,\n                                min = 100, \n                                max = 5000,\n                                step = 100)\n                 ),\n                 \n                 # Show a plot of the generated distribution\n                 mainPanel(\n                   plotOutput(\"maPlot\", height = \"600px\")\n                 )\n               )),\n      tabPanel(\"ARMA(1,1)\", \n              \n               sidebarLayout(\n                 sidebarPanel(width = sbp.width,\n                              uiOutput(\"armatext\"),\n                   sliderInput(\"armaphi\",\n                               \"phi:\",\n                               min = -.99,\n                               max = .99,\n                               value = .7,\n                               step = .01),\n                   sliderInput(\"armatheta\",\n                               \"theta:\",\n                               min = -1,\n                               max = 1,\n                               value = .7,\n                               step = .01),\n                   sliderInput(\"armasigma\",\n                               \"sigma:\",\n                               min = 0,\n                               max = 10,\n                               value = 1,\n                               step = .2),\n                   numericInput(\"armaseed\",\n                                \"Seed:\",\n                                value = 1234,\n                                min = 0, \n                                max = 9999,\n                                step = 1),\n                   numericInput(\"arman\",\n                                \"Sample size:\",\n                                value = 500,\n                                min = 100, \n                                max = 5000,\n                                step = 100)\n                 ),\n                 \n                 # Show a plot of the generated distribution\n                 mainPanel(\n                   plotOutput(\"armaPlot\", height = \"600px\")\n                 )\n               ))))\n    # Sidebar with a slider input for number of bins \n   \n\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n    output$artext &lt;- renderUI({\n      withMathJax(\"Model: $$Y_t=\\\\phi \\\\,Y_{t-1}+Z_t,\\\\quad Z_t\\\\sim \\\\text{i.i.d N}(0,\\\\sigma^2)$$\")\n    })\n    output$matext &lt;- renderUI({\n      withMathJax(\"Model: $$Y_t=\\\\theta\\\\, Z_{t-1}+Z_t,\\\\quad Z_t\\\\sim \\\\text{i.i.d N}(0,\\\\sigma^2)$$\")\n    })\n    output$armatext &lt;- renderUI({\n      withMathJax(\"Model: $$Y_t=\\\\phi\\\\, Y_{t-1}+\\\\theta \\\\,Z_{t-1}+Z_t,\\\\quad Z_t\\\\sim \\\\text{i.i.d N}(0,\\\\sigma^2)$$\")\n    })\n    output$arPlot &lt;- renderPlot({\n      set.seed(input$arseed)\n      burnin &lt;- 200\n      x &lt;- rnorm(input$arn+burnin, sd = input$arsigma)\n      for(i in 2:length(x))\n        x[i] &lt;- input$arphi*x[i-1]+rnorm(1, sd = input$arsigma)\n      df &lt;- tsibble(x = x[burnin+1:(length(x)-burnin)],\n                    t = 1:(length(x)-burnin),\n                    index = \"t\")\n      # Theoretical acf: \n      \n      theoretical.correlations &lt;- \n        tibble(lag = 1:29,\n               acf = ARMAacf(ar = c(input$arphi), lag.max = 29, pacf =FALSE)[-1],\n               pacf = ARMAacf(ar = c(input$arphi), lag.max = 29, pacf =TRUE))\n      # generate bins based on input$bins from ui.R\n       ggarrange(\n         df %&gt;% autoplot() + scale_y_continuous(\"AR(1) series\")+\n           scale_x_continuous(\"Time index\", expand = c(0,0)),\n         ggarrange(df %&gt;% ACF() %&gt;% autoplot()+\n                     scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                     ylab(\"Sample PACF\"),\n         df %&gt;% PACF() %&gt;% autoplot()+\n           scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n           ylab(\"Sample PACF\"), \n         theoretical.correlations %&gt;% ggplot(aes(x=lag))+\n           geom_segment(aes(x=lag,xend=lag,y=0,yend=acf))+geom_hline(yintercept=0)+\n           scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n           scale_y_continuous(\"Theoretical ACF\"),\n         theoretical.correlations %&gt;% ggplot(aes(x=lag))+\n           geom_segment(aes(x=lag,xend=lag,y=0,yend=pacf))+geom_hline(yintercept=0)+\n           scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n           scale_y_continuous(\"Theoretical PACF\"),\n         nrow = 2, ncol = 2),\n         ncol = 1, nrow = 2, heights = c(1,2))\n    })\n\n    output$maPlot &lt;- renderPlot({\n      set.seed(input$maseed)\n      burnin &lt;- 200\n      z &lt;- rnorm(input$man+burnin, sd = input$masigma)\n      x &lt;- numeric(input$man+burnin)\n      for(i in 2:length(x))\n        x[i] &lt;- input$matheta*z[i-1]+z[i]\n      df &lt;- tsibble(x = x[burnin+1:(length(x)-burnin)],\n                    t = 1:(length(x)-burnin),\n                    index = \"t\")\n      \n      theoretical.correlations &lt;- \n        tibble(lag = 1:29,\n               acf = ARMAacf(ma = c(input$matheta), lag.max = 29, pacf =FALSE)[-1],\n               pacf = ARMAacf(ma = c(input$matheta), lag.max = 29, pacf =TRUE))\n      # generate bins based on input$bins from ui.R\n      ggarrange(\n        df %&gt;% autoplot() + scale_y_continuous(\"MA(1) series\")+\n          scale_x_continuous(\"Time index\", expand = c(0,0)),\n        ggarrange(df %&gt;% ACF() %&gt;% autoplot()+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                                         ylab(\"Sample ACF\"),\n                  df %&gt;% PACF() %&gt;% autoplot()+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                                         ylab(\"Sample PACF\"), \n                  theoretical.correlations %&gt;% ggplot(aes(x=lag))+\n                    geom_segment(aes(x=lag,xend=lag,y=0,yend=acf))+geom_hline(yintercept=0)+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                    scale_y_continuous(\"Theoretical ACF\"),\n                  theoretical.correlations %&gt;% ggplot(aes(x=lag))+\n                    geom_segment(aes(x=lag,xend=lag,y=0,yend=pacf))+geom_hline(yintercept=0)+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                    scale_y_continuous(\"Theoretical PACF\"),\n                  nrow = 2, ncol = 2),\n        ncol = 1, nrow = 2, heights = c(1,2))\n    })\n    output$armaPlot &lt;- renderPlot({\n      set.seed(input$armaseed)\n      burnin &lt;- 200\n      #sdZ = sqrt(input$armasigma *(1-input$armaphi^2)/(1+2*input$armaphi*input$armatheta + input$armatheta^2))\n      z &lt;- rnorm(input$arman+burnin, sd = input$armasigma)\n      x &lt;- numeric(input$arman+burnin)\n      for(i in 2:length(x))\n        x[i] &lt;- input$armaphi*x[i-1]+input$armatheta*z[i-1]+z[i]\n      df &lt;- tsibble(x = x[burnin+1:(length(x)-burnin)],\n                    t = 1:(length(x)-burnin),\n                    index = \"t\")\n      \n      theoretical.correlations &lt;- \n        tibble(lag = 1:29,\n               acf = ARMAacf(ar = c(input$armaphi), ma = c(input$armatheta), lag.max = 29, pacf =FALSE)[-1],\n               pacf = ARMAacf(ar = c(input$armaphi), ma = c(input$armatheta), lag.max = 29, pacf =TRUE))\n      # generate bins based on input$bins from ui.R\n      ggarrange(\n        df %&gt;% autoplot() + scale_y_continuous(\"MA(1) series\")+\n          scale_x_continuous(\"Time index\", expand = c(0,0)),\n        ggarrange(df %&gt;% ACF() %&gt;% autoplot()+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                                         ylab(\"Sample ACF\"),\n                  df %&gt;% PACF() %&gt;% autoplot()+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                                         ylab(\"Sample PACF\"), \n                  theoretical.correlations %&gt;% ggplot(aes(x=lag))+\n                    geom_segment(aes(x=lag,xend=lag,y=0,yend=acf))+geom_hline(yintercept=0)+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                    scale_y_continuous(\"Theoretical ACF\"),\n                  theoretical.correlations %&gt;% ggplot(aes(x=lag))+\n                    geom_segment(aes(x=lag,xend=lag,y=0,yend=pacf))+geom_hline(yintercept=0)+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                    scale_y_continuous(\"Theoretical PACF\"),\n                  nrow = 2, ncol = 2),\n        ncol = 1, nrow = 2, heights = c(1,2))\n    })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "6_judgementalforecast.html",
    "href": "6_judgementalforecast.html",
    "title": "Judgemental forecast",
    "section": "",
    "text": "In this chapter, we go through chapter 6 of the textbook (Hyndman and Athanasopoulos, 2021). This material is mostly for you to be aware of and be able do discuss different aspects of the judgmental forecasting methods. We will through short videos describe some general aspects and specific methods. Particularly, the Delphi method and forecasting by analogy will be described in some detail, while other methods are mentioned. For more details, see the referred chapter 6 in the textbook."
  },
  {
    "objectID": "4_tourismexample.html",
    "href": "4_tourismexample.html",
    "title": "Exploring Australian tourism example",
    "section": "",
    "text": "Exploring Australian tourism example\nYou find the example in the textbook here: Chapter 4.5. This is a somewhat complicated example to follow perhaps, and in the video below we go through the code line by line (almost) and explain what is going on. You may watch the video or simply study it in the book. The code is not added here, since it is the same as in the book (more or less)."
  },
  {
    "objectID": "4_features.html",
    "href": "4_features.html",
    "title": "Features",
    "section": "",
    "text": "Features\nHere we will go through how to calculate features in R and show some examples of features you can calculate for time series using the feasts package. You find more details in Chapter 4.1-4.3 of Hyndman and Athanasopoulos(2021). The code used in this lecture is an adaptation of what you find in these sections.\n\n\n\n\nCode used in video\n\n\n# --- Time series features ---\nlibrary(fpp3)\nlibrary(tidyverse)\ntheme_set(theme_bw() +\n            theme(panel.grid.minor = element_blank(),\n                  panel.grid.major = element_blank(),\n                  strip.background = element_rect(fill =\"white\",\n                                                  color = \"transparent\")))\n \n# -- Features: mean --\ntourism %&gt;% \n  features(Trips, list(mean = mean)) %&gt;%\n  arrange(mean)\n\n# A tibble: 304 × 4\n   Region          State              Purpose   mean\n   &lt;chr&gt;           &lt;chr&gt;              &lt;chr&gt;    &lt;dbl&gt;\n 1 Kangaroo Island South Australia    Other    0.340\n 2 MacDonnell      Northern Territory Other    0.449\n 3 Wilderness West Tasmania           Other    0.478\n 4 Barkly          Northern Territory Other    0.632\n 5 Clare Valley    South Australia    Other    0.898\n 6 Barossa         South Australia    Other    1.02 \n 7 Kakadu Arnhem   Northern Territory Other    1.04 \n 8 Lasseter        Northern Territory Other    1.14 \n 9 Wimmera         Victoria           Other    1.15 \n10 MacDonnell      Northern Territory Visiting 1.18 \n# ℹ 294 more rows\n\n# -- Features: mean, sd, 2.5 and 97.5 percentiles --\ntourism %&gt;% \n  features(Trips, list(mean = mean,\n                       sd   = sd,\n                       p    = ~quantile(., probs = c(0.025, 0.975)))) %&gt;%\n  arrange(desc(sd))\n\n# A tibble: 304 × 7\n   Region                 State           Purpose  mean    sd `p_2.5%` `p_97.5%`\n   &lt;chr&gt;                  &lt;chr&gt;           &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 South Coast            New South Wales Holiday  495. 170.     285.       812.\n 2 North Coast NSW        New South Wales Holiday  588. 117.     404.       815.\n 3 Sydney                 New South Wales Busine…  602. 117.     402.       884.\n 4 Great Ocean Road       Victoria        Holiday  281. 116.     135.       545.\n 5 Melbourne              Victoria        Holiday  507. 103.     354.       736.\n 6 Peninsula              Victoria        Holiday  185.  96.7     70.8      458.\n 7 Australia's South West Western Austra… Holiday  309.  95.3    179.       541.\n 8 Melbourne              Victoria        Visiti…  619.  93.6    472.       807.\n 9 Brisbane               Queensland      Visiti…  493.  90.6    344.       663.\n10 Sydney                 New South Wales Visiti…  747.  89.6    564.       916.\n# ℹ 294 more rows\n\n# -- dplyr equivalent: --\ntourism  %&gt;% \n  as_tibble() %&gt;% \n  group_by(Region,State,Purpose) %&gt;%\n  summarize(mean = mean(Trips)) %&gt;%\n  arrange(mean)\n\n# A tibble: 304 × 4\n# Groups:   Region, State [76]\n   Region          State              Purpose   mean\n   &lt;chr&gt;           &lt;chr&gt;              &lt;chr&gt;    &lt;dbl&gt;\n 1 Kangaroo Island South Australia    Other    0.340\n 2 MacDonnell      Northern Territory Other    0.449\n 3 Wilderness West Tasmania           Other    0.478\n 4 Barkly          Northern Territory Other    0.632\n 5 Clare Valley    South Australia    Other    0.898\n 6 Barossa         South Australia    Other    1.02 \n 7 Kakadu Arnhem   Northern Territory Other    1.04 \n 8 Lasseter        Northern Territory Other    1.14 \n 9 Wimmera         Victoria           Other    1.15 \n10 MacDonnell      Northern Territory Visiting 1.18 \n# ℹ 294 more rows\n\n# -- ACF features --\ntourism %&gt;% \n  features(Trips, feat_acf) \n\n# A tibble: 304 × 10\n   Region         State Purpose     acf1 acf10 diff1_acf1 diff1_acf10 diff2_acf1\n   &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n 1 Adelaide       Sout… Busine…  0.0333  0.131     -0.520       0.463     -0.676\n 2 Adelaide       Sout… Holiday  0.0456  0.372     -0.343       0.614     -0.487\n 3 Adelaide       Sout… Other    0.517   1.15      -0.409       0.383     -0.675\n 4 Adelaide       Sout… Visiti…  0.0684  0.294     -0.394       0.452     -0.518\n 5 Adelaide Hills Sout… Busine…  0.0709  0.134     -0.580       0.415     -0.750\n 6 Adelaide Hills Sout… Holiday  0.131   0.313     -0.536       0.500     -0.716\n 7 Adelaide Hills Sout… Other    0.261   0.330     -0.253       0.317     -0.457\n 8 Adelaide Hills Sout… Visiti…  0.139   0.117     -0.472       0.239     -0.626\n 9 Alice Springs  Nort… Busine…  0.217   0.367     -0.500       0.381     -0.658\n10 Alice Springs  Nort… Holiday -0.00660 2.11      -0.153       2.11      -0.274\n# ℹ 294 more rows\n# ℹ 2 more variables: diff2_acf10 &lt;dbl&gt;, season_acf1 &lt;dbl&gt;\n\n# -- STL features --\ntourism %&gt;%\n  features(Trips, feat_stl) %&gt;%\n  ggplot(aes(x = trend_strength, y=seasonal_strength_year,\n             color = Purpose)) + \n  geom_point() + \n  facet_wrap(vars(State))\n\n\n\n# -- Time series with strongest seasonal component: --\ntourism %&gt;%\n  features(Trips, feat_stl) %&gt;%\n  filter(\n    seasonal_strength_year == max(seasonal_strength_year)\n  ) %&gt;%\n  left_join(tourism, by = c(\"Region\",\"State\",\"Purpose\")) %&gt;%\n  ggplot(aes(x=Quarter, y = Trips)) + geom_line() + \n  facet_grid(vars(Region,State,Purpose))"
  },
  {
    "objectID": "3_timeseriesdecomposition.html",
    "href": "3_timeseriesdecomposition.html",
    "title": "Moving averages and Decomposition",
    "section": "",
    "text": "Code\n\n\nlibrary(fpp3)\nlibrary(tidyverse)\n\n# ggplot theme: \ntheme_set(theme_bw() + \n            theme(panel.grid.major = element_blank(),\n                  panel.grid.minor = element_blank()))\n\n# -- Read in data: --\ndat &lt;- readxl::read_excel(\n  \"data/NorwayEmployment_15-74years_bySex.xlsx\") %&gt;%\n  as_tibble() %&gt;%\n  mutate(Quarter = str_replace(Quarter, \"K\",\"Q\"),\n         Quarter = yearquarter(Quarter)) \nnames(dat)[3] &lt;- \"Employed\"\n\n# -- Aggregating from Employed by sex to total --\ndat &lt;- dat %&gt;% \n  group_by(Quarter) %&gt;% \n  summarize(Employed = sum(Employed)) %&gt;%\n  as_tsibble(index = Quarter) # Time series table\ndat %&gt;% \n  autoplot(Employed, colour = \"blue\")\n\n\n\ndat &lt;- dat %&gt;%\n  mutate(\n    `12-MA` = slider::slide_dbl(Employed, mean,\n                                .before = 5, .after = 6, .complete = TRUE),\n    `2x12-MA` = slider::slide_dbl(`12-MA`, mean,\n                                  .before = 1, .after = 0, .complete = TRUE)\n  )\ndat %&gt;%\n  ggplot(aes(x=Quarter, y =Employed))+\n  geom_line(colour = \"gray\") +\n  geom_line(aes(y = `2x12-MA`), colour = \"#D55E00\") +\n  theme_bw()+\n  labs(y = \"Persons (thousands)\",\n       title = \"Total employment in US retail\")\n\nWarning: Removed 12 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\ndat %&gt;% \n  model(\n    classical_decomposition(Employed, type = \"additive\")\n  ) %&gt;%\n  components() %&gt;%\n  autoplot()\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\ndat %&gt;% \n  model(\n    classical = classical_decomposition(Employed, \n                                        type = \"multiplicative\"),\n    x11 = X_13ARIMA_SEATS(Employed ~ x11()),\n    seats = X_13ARIMA_SEATS(Employed ~ seats())\n  ) %&gt;%\n  components() %&gt;% \n  mutate(random = ifelse(.model == \"classical\", \n                         random, \n                         irregular)) %&gt;%\n  autoplot()\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\ndat %&gt;%\n  model(\n    STL0 = STL(Employed),\n    STL1 = STL(Employed ~ trend(window = 5) + # default 7\n                 season(window = 19),         # default 11\n               robust = FALSE)\n  ) %&gt;%\n  components() %&gt;%\n  autoplot()"
  },
  {
    "objectID": "3_timeseriesdecomposition.html#moving-averages",
    "href": "3_timeseriesdecomposition.html#moving-averages",
    "title": "Moving averages and Decomposition",
    "section": "",
    "text": "Code\n\n\nlibrary(fpp3)\nlibrary(tidyverse)\n\n# ggplot theme: \ntheme_set(theme_bw() + \n            theme(panel.grid.major = element_blank(),\n                  panel.grid.minor = element_blank()))\n\n# -- Read in data: --\ndat &lt;- readxl::read_excel(\n  \"data/NorwayEmployment_15-74years_bySex.xlsx\") %&gt;%\n  as_tibble() %&gt;%\n  mutate(Quarter = str_replace(Quarter, \"K\",\"Q\"),\n         Quarter = yearquarter(Quarter)) \nnames(dat)[3] &lt;- \"Employed\"\n\n# -- Aggregating from Employed by sex to total --\ndat &lt;- dat %&gt;% \n  group_by(Quarter) %&gt;% \n  summarize(Employed = sum(Employed)) %&gt;%\n  as_tsibble(index = Quarter) # Time series table\ndat %&gt;% \n  autoplot(Employed, colour = \"blue\")\n\n\n\ndat &lt;- dat %&gt;%\n  mutate(\n    `12-MA` = slider::slide_dbl(Employed, mean,\n                                .before = 5, .after = 6, .complete = TRUE),\n    `2x12-MA` = slider::slide_dbl(`12-MA`, mean,\n                                  .before = 1, .after = 0, .complete = TRUE)\n  )\ndat %&gt;%\n  ggplot(aes(x=Quarter, y =Employed))+\n  geom_line(colour = \"gray\") +\n  geom_line(aes(y = `2x12-MA`), colour = \"#D55E00\") +\n  theme_bw()+\n  labs(y = \"Persons (thousands)\",\n       title = \"Total employment in US retail\")\n\nWarning: Removed 12 rows containing missing values (`geom_line()`)."
  },
  {
    "objectID": "3_timeseriesdecomposition.html#classical-decomposition",
    "href": "3_timeseriesdecomposition.html#classical-decomposition",
    "title": "Moving averages and Decomposition",
    "section": "",
    "text": "Code\n\n\ndat %&gt;% \n  model(\n    classical_decomposition(Employed, type = \"additive\")\n  ) %&gt;%\n  components() %&gt;%\n  autoplot()\n\nWarning: Removed 2 rows containing missing values (`geom_line()`)."
  },
  {
    "objectID": "3_timeseriesdecomposition.html#statistics-agencies-x11-and-seats",
    "href": "3_timeseriesdecomposition.html#statistics-agencies-x11-and-seats",
    "title": "Moving averages and Decomposition",
    "section": "",
    "text": "Code\n\n\ndat %&gt;% \n  model(\n    classical = classical_decomposition(Employed, \n                                        type = \"multiplicative\"),\n    x11 = X_13ARIMA_SEATS(Employed ~ x11()),\n    seats = X_13ARIMA_SEATS(Employed ~ seats())\n  ) %&gt;%\n  components() %&gt;% \n  mutate(random = ifelse(.model == \"classical\", \n                         random, \n                         irregular)) %&gt;%\n  autoplot()\n\nWarning: Removed 2 rows containing missing values (`geom_line()`)."
  },
  {
    "objectID": "3_timeseriesdecomposition.html#stl-seasonal-and-trend-decomposition-using-loess",
    "href": "3_timeseriesdecomposition.html#stl-seasonal-and-trend-decomposition-using-loess",
    "title": "Moving averages and Decomposition",
    "section": "",
    "text": "Code\n\n\ndat %&gt;%\n  model(\n    STL0 = STL(Employed),\n    STL1 = STL(Employed ~ trend(window = 5) + # default 7\n                 season(window = 19),         # default 11\n               robust = FALSE)\n  ) %&gt;%\n  components() %&gt;%\n  autoplot()"
  },
  {
    "objectID": "3_decomposition_intro.html",
    "href": "3_decomposition_intro.html",
    "title": "Decomposition",
    "section": "",
    "text": "Decomposition\nIn this chapter we will consider different adjusments and transformations one can do prior to a model task. Then we move on to techniques for decomposing a time series into a trend-cycle, season and remainder component."
  },
  {
    "objectID": "3_adjustments.html",
    "href": "3_adjustments.html",
    "title": "Transformations and adjustments",
    "section": "",
    "text": "In the example in the video above, we are not interested in a proxy for working days per month, and to avoid the effect of this we use the mean (average) production per working day within each month instead of total production per month. The code to generate the example can be found below:\n\n\nCode:\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(fpp3)\n\n# ggplot theme:\ntheme_set(\n  theme_bw() + \n    theme(panel.grid.minor = element_blank(),\n          panel.grid.major = element_blank())\n)\n\n# Daily production: \ndat &lt;- tibble(\n  date = seq(as.Date(\"2015-01-01\"), as.Date(\"2019-12-31\"), by = \"1 day\"),\n  price = pi\n) %&gt;% \n  #Removing the weekends:\n  filter(wday(date, week_start = 1) %in% 1:5) %&gt;%  \n  #Note: We do not remove public holidays, and the worker never takes a day off\n  mutate(YearMonth = yearmonth(date))\n\n# -- TOTAL PRODUCTION FIGURE --\ndat %&gt;%\n  group_by(YearMonth) %&gt;%\n  summarize(`Total production` = sum(price)) %&gt;%\n  as_tsibble(index = \"YearMonth\") %&gt;% \n  ggplot(aes(x = YearMonth, \n             y = `Total production`)) + \n  geom_point(color = \"skyblue\") + \n  geom_line(color = \"skyblue\") + \n  scale_y_continuous(breaks = seq(60, 100, 5),\n                     labels = paste0(\"$\",seq(60, 100, 5),\"k\"), \n                     limits = c(60,75))\n# -- MEAN PRODUCTION FIGURE --\ndat %&gt;% group_by(YearMonth) %&gt;%\n  summarize(`Mean production` = mean(price)) %&gt;%\n  as_tsibble(index = \"YearMonth\") %&gt;% \n  ggplot(aes(x = YearMonth, \n             y = `Mean production`)) + \n  geom_point(color = \"skyblue\") + \n  geom_line(color=\"skyblue\")+ \n  scale_y_continuous(breaks = seq(3, 4, .02), \n                     labels = paste0(\"$\",seq(3, 4, .02),\"k\"))\n\n\n\n\n\nAdjusting for population size is usually a good idea when studying a quantity that is affected by it. The most obvious example is to study GDP (Gross Domestic Product) per capita instead of GDP.\n\n\n\n\nCode:\n# --- Setting up the data --\nscandinaviaUSA &lt;- global_economy %&gt;% \n  filter(Country %in% c(\"Norway\",\"Sweden\",\"Denmark\", \"United States\"))\nscandinaviaUSA %&gt;% head()\n\n# --- GDP by country in $US --\nscandinaviaUSA %&gt;%\n  autoplot(GDP)+\n  labs(title= \"GDP\", y = \"$US\")\n\n# --- Population by country  --\nscandinaviaUSA %&gt;%\n  autoplot(Population)+\n  labs(title= \"Population\", y = \"Number of people\")\n\n# --- Population by country  (log-scale on y-axis) --\nscandinaviaUSA %&gt;% \n  autoplot(Population)+\n  scale_y_log10()+\n  labs(title= \"Population (log-scale)\",y = \"Number of people\")\n\n# --- GDP per capita by country ---\nscandinaviaUSA %&gt;%\n  autoplot(GDP/Population) +\n  labs(title= \"GDP per capita = GDP / Population\", y = \"$US\")\n\n\n\n\n\nAdjusting for inflation is a simple way of taking into account that 5$ in 1950 would get you much more than 5$ would today. This compensation is usually done by a consumer price index, which is standardized to a specific year (in the video below we show examples with 2010 and 2015 as reference years).\nLet Y_t denote the raw time series and Y_t^\\star the inflation adjusted. Let \\text{CPI}_t denote a relevant consumer price index defined to be 100 in the reference year t^\\star. Then Y_t^\\star = Y_t \\cdot \\frac{100}{\\text{CPI}_t}. More generally, we can choose the reference year t^\\star and write this as Y_t^\\star = Y_t \\cdot \\frac{\\text{CPI}_{t^\\star}}{\\text{CPI}_t}.\nThe inflation adjusted series is then measured in the unit “year t^\\star-money”.\n\n\n\n\nCode:\n# --- Inflation adjusted GDP per capita by country ---\nscandinaviaUSA %&gt;%\n  autoplot(GDP/Population *100 / CPI) +\n  labs(title= \"GDP per capita = GDP / Population\", y = \"$US\")\n\n# --- CPI FOR NORWAY (data from Statistics Norway)---\nCPI &lt;- read.csv(\"data/CPI_norway.csv\", sep = \";\") %&gt;% as_tibble() %&gt;%\n  select(1:2) %&gt;%\n  rename(Year = X,CPI = Y.avg2) %&gt;% \n  mutate(Year = as.numeric(Year), CPI = as.numeric(CPI))%&gt;%\n  filter(Year &lt; 2022) %&gt;%\n  as_tsibble(index = Year)\n\n# --- CPI figure ---\nCPI %&gt;% \n  autoplot(CPI, color = \"blue\", lwd = 1.2) +\n  labs(title= \"Consumer Price Index\", y = \"NOK\",\n       subtitle = \"Data source: Statistics Norway\")+\n  geom_hline(yintercept = 100, lty = 2) + geom_vline(xintercept = 2015, lty = 2)+\n  scale_x_continuous(breaks = seq(1925,2025,10))+\n  scale_y_continuous(breaks = seq(0,120,10))\n\n# --- BIG MAC price index ---\nbigMac &lt;- read_csv(\"https://raw.githubusercontent.com/TheEconomist/big-mac-data/master/output-data/big-mac-raw-index.csv\")\nnorBigMac &lt;- bigMac %&gt;% \n  filter(name %in% c(\"Norway\")) %&gt;% \n  mutate(Year = lubridate::year(date)) %&gt;%\n  as_tsibble(index = \"date\")%&gt;%\n  filter(Year &lt;2022) %&gt;%\n  left_join(CPI, by = \"Year\") \n\n# --- BIG MAC price index figure ---\nnorBigMac %&gt;% \n  autoplot(local_price) +\n  labs(title= \"Big Mac price in Norway\", y = \"NOK\",\n       subtitle = \"Data source: The Economist\") +\n  geom_smooth(method = \"lm\", se=FALSE)\n\n#--- Inflation adjusted BIG MAC price index figure ---\nnorBigMac %&gt;%\n  mutate(cpiAdjusted =local_price / CPI * 100)  %&gt;%\n  as_tsibble(index = date) %&gt;%\n  autoplot(cpiAdjusted)+\n  labs(title= \"Inflation adjusted Big Mac price in Norway\", y = \"NOK (2015)\",\n       subtitle = \"Data sources: The Economist (big mac index), Statistics Norway (CPI)\") +\n  geom_smooth(method = \"lm\", se=FALSE)\n\n\n\n\n\nIn many situations it can be necessary to do a mathematical transformation of a time series. There can be different reasons for doing so, but a main one is to make it stationary (or at least more stationary). For instance, if you see that the variation increases or decreases with the level of the series. The most common transformation (for positive time series) is probably using the logarithm. It is often effective and interpretable as changes in the log value correspond to relative changes in the original scale. We write the transformed series, w_t, as w_t =\\log y_t, where y_t is the original time series.\nThe textbook also mentions power transformations of the form w_t = y_t^p (squarte roots - p=\\frac12, cube roots - p=\\frac13, etc). These are not as common to use, but there are situations where these may be better than the logarithm.\nA family of transformations (including log- and a class of power transformations) is the Box-Cox transform. For any value of \\lambda\\in \\mathbb R, \\begin{equation*}\nw_t = \\begin{cases}\n\\log(y_t),&\\text{if }\\lambda = 0;\\\\\n(y_t^\\lambda -1)/\\lambda, &\\text{otherwise}.\n\\end{cases}\n\\end{equation*} As you can see, if \\lambda = 0 we have a simple natural logarithm transform. This version of the Box-Cox transform is also defined for negative values of y_t as long as \\lambda &gt;0.\nThe book has a very nice shiny app for experimenting with different values of \\lambda on a time series of gas production in Australia. We have borrowed it below, but you find it also here. They write that: ” A good value of \\lambda is one which makes the size of the seasonal variation about the same across the whole series, as that makes the forecasting model simpler.” This pretty much sums up why one does mathematical transformations as a preprocessing step before fitting a model - it makes the model simpler.\n\n\nFor financial assets, such as stocks, it is often better to consider the returns rather than the price series. This is also a mathematical transformation and involves differencing. First order differencing means subtracting the previous observation from the present, i.e. w_t=y_t-y_{t-1}. Taking differences is an effective way of potentially making a non-stationary time series stationary. E.g. if a time series has a linear trend: Y_t = \\alpha t + Z_t, where \\alpha is a real constant and Z_t is a white noise, we get that W_t = Y_t-Y_{t-1} = \\alpha t + Z_t - \\alpha(t-1) - Z_{t-1} = Z_t-Z_{t-1} + \\alpha, effectively removing the trend in the transformed series. We will return to this when considering ARIMA models.\nThere are different definitions of returns, but the most common ones are the standard returns, r_t, and log-returns, \\textrm{lr}_t, defined respectively by \\begin{equation*}\n\\begin{split}\nr_t &= \\frac{y_t-y_{t-1}}{y_{t-1}},\\\\\n\\textrm{lr}_t &= \\log y_t-\\log y_{t-1} = \\log\\frac{y_t}{y_{t-1}}.\n\\end{split}\n\\end{equation*} A daily return series for a stock usually has expectation close to zero and little autocorrelation, which can be convenient in many situations. However, they are typically hetereoskedastic (non-constant variance) and the squared returns are often autocorrelated. We will come back to this, when discussing volatility forecasting towards the end of the course.\n\n\nCode\n\n\n# Package for downloading stock data (primarily from Yahoo! Finance)\nlibrary(quantmod)\n\n# -- Download the data: --\ngetSymbols(\"AAPL\")\n\n[1] \"AAPL\"\n\n# -- Extract the closing price and create a tsibble: --\nclose.price &lt;- tibble(\n  close = as.numeric(AAPL$AAPL.Close),\n  date  = time(AAPL)\n) %&gt;% as_tsibble(index = date) \n\n# -- Plot closing price: --\nclose.price %&gt;% autoplot(close) +\n  labs(title = \"Apple Closing price\", \n       y     = \"US$\")\n\n\n\n# -- Adding transformations : -- \nclose.price &lt;- close.price %&gt;% \n  mutate(logclose  = log(close), # log-transform\n         logreturn = c(NA, diff(logclose)), # log returns\n         return    = c(NA, diff(close)/close[-nrow(close.price)]) # Returns\n         ) \n\n\n# -- Box-cox-transform --\nlambda &lt;- close.price %&gt;%\n  features(close, features = guerrero) %&gt;%\n  pull(lambda_guerrero)\nclose.price &lt;- close.price %&gt;%\n  mutate(boxcox = box_cox(close,lambda))\nclose.price %&gt;%\n  autoplot(boxcox) +\n  labs(y = \"\",\n       title = latex2exp::TeX(paste0(\n         \"Transformed apple closing price with $\\\\lambda$ = \",\n         round(lambda,3))))+\n  # Adding red curve with log-transform\n  geom_line(aes(y=logclose), col = 2) \n\n\n\n# -- Plotting the different transformations --\nclose.price %&gt;%\n  pivot_longer(-date) %&gt;% \n  autoplot(value) + \n  facet_wrap(~name, scales=\"free_y\", strip.position = \"left\")+\n  labs(title = \"Apple Closing Price transformations\") +\n  theme(strip.placement = \"outside\")\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\nset.seed(1344)\nlibrary(tidyverse)\nlibrary(fpp3)\ndat &lt;- tibble(\n  date = seq(as.Date(\"2015-01-01\"), as.Date(\"2020-12-31\"), by = \"1 day\"),\n  t = 1:length(date),\n  Tt = 100+9*t/300 + .5*((t-1000)/200)^3+(.87*cos(2*pi*t/1200)+.42*sin(2*pi*t/600))*11,\n  St = (.94*cos(2*pi*t/365) -1.2*sin(2* pi*t/365))*13,\n  Rt = rnorm(length(date), sd = 15),\n  Yt = Tt + St +  Rt\n) \n\n# -- Plotting Y and its components --\ndat %&gt;%\n  pivot_longer(cols = -c(date,t)) %&gt;% \n  mutate(name = factor(name, levels = c(\"Yt\", \"Tt\", \"St\", \"Rt\"))) %&gt;% # to order the panels\n  ggplot(aes(x=date, y = value, col = name)) + \n  geom_line() + facet_wrap(~name, ncol = 1, scales = \"free_y\", strip.position = \"left\") +\n  theme(strip.placement = \"outside\", axis.title = element_blank(), legend.position = \"none\")\n\n\n\n# -- Plotting seasonally adjusted Y --\nggplot(dat, aes(x = date, y = Yt-St)) + \n  geom_line() +\n  labs(title = \"Seasonally adjusted\", x = \"\")"
  },
  {
    "objectID": "3_adjustments.html#calender-adjustments",
    "href": "3_adjustments.html#calender-adjustments",
    "title": "Transformations and adjustments",
    "section": "",
    "text": "In the example in the video above, we are not interested in a proxy for working days per month, and to avoid the effect of this we use the mean (average) production per working day within each month instead of total production per month. The code to generate the example can be found below:\n\n\nCode:\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(fpp3)\n\n# ggplot theme:\ntheme_set(\n  theme_bw() + \n    theme(panel.grid.minor = element_blank(),\n          panel.grid.major = element_blank())\n)\n\n# Daily production: \ndat &lt;- tibble(\n  date = seq(as.Date(\"2015-01-01\"), as.Date(\"2019-12-31\"), by = \"1 day\"),\n  price = pi\n) %&gt;% \n  #Removing the weekends:\n  filter(wday(date, week_start = 1) %in% 1:5) %&gt;%  \n  #Note: We do not remove public holidays, and the worker never takes a day off\n  mutate(YearMonth = yearmonth(date))\n\n# -- TOTAL PRODUCTION FIGURE --\ndat %&gt;%\n  group_by(YearMonth) %&gt;%\n  summarize(`Total production` = sum(price)) %&gt;%\n  as_tsibble(index = \"YearMonth\") %&gt;% \n  ggplot(aes(x = YearMonth, \n             y = `Total production`)) + \n  geom_point(color = \"skyblue\") + \n  geom_line(color = \"skyblue\") + \n  scale_y_continuous(breaks = seq(60, 100, 5),\n                     labels = paste0(\"$\",seq(60, 100, 5),\"k\"), \n                     limits = c(60,75))\n# -- MEAN PRODUCTION FIGURE --\ndat %&gt;% group_by(YearMonth) %&gt;%\n  summarize(`Mean production` = mean(price)) %&gt;%\n  as_tsibble(index = \"YearMonth\") %&gt;% \n  ggplot(aes(x = YearMonth, \n             y = `Mean production`)) + \n  geom_point(color = \"skyblue\") + \n  geom_line(color=\"skyblue\")+ \n  scale_y_continuous(breaks = seq(3, 4, .02), \n                     labels = paste0(\"$\",seq(3, 4, .02),\"k\"))"
  },
  {
    "objectID": "3_adjustments.html#population-adjustment",
    "href": "3_adjustments.html#population-adjustment",
    "title": "Transformations and adjustments",
    "section": "",
    "text": "Adjusting for population size is usually a good idea when studying a quantity that is affected by it. The most obvious example is to study GDP (Gross Domestic Product) per capita instead of GDP.\n\n\n\n\nCode:\n# --- Setting up the data --\nscandinaviaUSA &lt;- global_economy %&gt;% \n  filter(Country %in% c(\"Norway\",\"Sweden\",\"Denmark\", \"United States\"))\nscandinaviaUSA %&gt;% head()\n\n# --- GDP by country in $US --\nscandinaviaUSA %&gt;%\n  autoplot(GDP)+\n  labs(title= \"GDP\", y = \"$US\")\n\n# --- Population by country  --\nscandinaviaUSA %&gt;%\n  autoplot(Population)+\n  labs(title= \"Population\", y = \"Number of people\")\n\n# --- Population by country  (log-scale on y-axis) --\nscandinaviaUSA %&gt;% \n  autoplot(Population)+\n  scale_y_log10()+\n  labs(title= \"Population (log-scale)\",y = \"Number of people\")\n\n# --- GDP per capita by country ---\nscandinaviaUSA %&gt;%\n  autoplot(GDP/Population) +\n  labs(title= \"GDP per capita = GDP / Population\", y = \"$US\")"
  },
  {
    "objectID": "3_adjustments.html#inflation-adjustment",
    "href": "3_adjustments.html#inflation-adjustment",
    "title": "Transformations and adjustments",
    "section": "",
    "text": "Adjusting for inflation is a simple way of taking into account that 5$ in 1950 would get you much more than 5$ would today. This compensation is usually done by a consumer price index, which is standardized to a specific year (in the video below we show examples with 2010 and 2015 as reference years).\nLet Y_t denote the raw time series and Y_t^\\star the inflation adjusted. Let \\text{CPI}_t denote a relevant consumer price index defined to be 100 in the reference year t^\\star. Then Y_t^\\star = Y_t \\cdot \\frac{100}{\\text{CPI}_t}. More generally, we can choose the reference year t^\\star and write this as Y_t^\\star = Y_t \\cdot \\frac{\\text{CPI}_{t^\\star}}{\\text{CPI}_t}.\nThe inflation adjusted series is then measured in the unit “year t^\\star-money”.\n\n\n\n\nCode:\n# --- Inflation adjusted GDP per capita by country ---\nscandinaviaUSA %&gt;%\n  autoplot(GDP/Population *100 / CPI) +\n  labs(title= \"GDP per capita = GDP / Population\", y = \"$US\")\n\n# --- CPI FOR NORWAY (data from Statistics Norway)---\nCPI &lt;- read.csv(\"data/CPI_norway.csv\", sep = \";\") %&gt;% as_tibble() %&gt;%\n  select(1:2) %&gt;%\n  rename(Year = X,CPI = Y.avg2) %&gt;% \n  mutate(Year = as.numeric(Year), CPI = as.numeric(CPI))%&gt;%\n  filter(Year &lt; 2022) %&gt;%\n  as_tsibble(index = Year)\n\n# --- CPI figure ---\nCPI %&gt;% \n  autoplot(CPI, color = \"blue\", lwd = 1.2) +\n  labs(title= \"Consumer Price Index\", y = \"NOK\",\n       subtitle = \"Data source: Statistics Norway\")+\n  geom_hline(yintercept = 100, lty = 2) + geom_vline(xintercept = 2015, lty = 2)+\n  scale_x_continuous(breaks = seq(1925,2025,10))+\n  scale_y_continuous(breaks = seq(0,120,10))\n\n# --- BIG MAC price index ---\nbigMac &lt;- read_csv(\"https://raw.githubusercontent.com/TheEconomist/big-mac-data/master/output-data/big-mac-raw-index.csv\")\nnorBigMac &lt;- bigMac %&gt;% \n  filter(name %in% c(\"Norway\")) %&gt;% \n  mutate(Year = lubridate::year(date)) %&gt;%\n  as_tsibble(index = \"date\")%&gt;%\n  filter(Year &lt;2022) %&gt;%\n  left_join(CPI, by = \"Year\") \n\n# --- BIG MAC price index figure ---\nnorBigMac %&gt;% \n  autoplot(local_price) +\n  labs(title= \"Big Mac price in Norway\", y = \"NOK\",\n       subtitle = \"Data source: The Economist\") +\n  geom_smooth(method = \"lm\", se=FALSE)\n\n#--- Inflation adjusted BIG MAC price index figure ---\nnorBigMac %&gt;%\n  mutate(cpiAdjusted =local_price / CPI * 100)  %&gt;%\n  as_tsibble(index = date) %&gt;%\n  autoplot(cpiAdjusted)+\n  labs(title= \"Inflation adjusted Big Mac price in Norway\", y = \"NOK (2015)\",\n       subtitle = \"Data sources: The Economist (big mac index), Statistics Norway (CPI)\") +\n  geom_smooth(method = \"lm\", se=FALSE)"
  },
  {
    "objectID": "3_adjustments.html#mathematical-transformations",
    "href": "3_adjustments.html#mathematical-transformations",
    "title": "Transformations and adjustments",
    "section": "",
    "text": "In many situations it can be necessary to do a mathematical transformation of a time series. There can be different reasons for doing so, but a main one is to make it stationary (or at least more stationary). For instance, if you see that the variation increases or decreases with the level of the series. The most common transformation (for positive time series) is probably using the logarithm. It is often effective and interpretable as changes in the log value correspond to relative changes in the original scale. We write the transformed series, w_t, as w_t =\\log y_t, where y_t is the original time series.\nThe textbook also mentions power transformations of the form w_t = y_t^p (squarte roots - p=\\frac12, cube roots - p=\\frac13, etc). These are not as common to use, but there are situations where these may be better than the logarithm.\nA family of transformations (including log- and a class of power transformations) is the Box-Cox transform. For any value of \\lambda\\in \\mathbb R, \\begin{equation*}\nw_t = \\begin{cases}\n\\log(y_t),&\\text{if }\\lambda = 0;\\\\\n(y_t^\\lambda -1)/\\lambda, &\\text{otherwise}.\n\\end{cases}\n\\end{equation*} As you can see, if \\lambda = 0 we have a simple natural logarithm transform. This version of the Box-Cox transform is also defined for negative values of y_t as long as \\lambda &gt;0.\nThe book has a very nice shiny app for experimenting with different values of \\lambda on a time series of gas production in Australia. We have borrowed it below, but you find it also here. They write that: ” A good value of \\lambda is one which makes the size of the seasonal variation about the same across the whole series, as that makes the forecasting model simpler.” This pretty much sums up why one does mathematical transformations as a preprocessing step before fitting a model - it makes the model simpler.\n\n\nFor financial assets, such as stocks, it is often better to consider the returns rather than the price series. This is also a mathematical transformation and involves differencing. First order differencing means subtracting the previous observation from the present, i.e. w_t=y_t-y_{t-1}. Taking differences is an effective way of potentially making a non-stationary time series stationary. E.g. if a time series has a linear trend: Y_t = \\alpha t + Z_t, where \\alpha is a real constant and Z_t is a white noise, we get that W_t = Y_t-Y_{t-1} = \\alpha t + Z_t - \\alpha(t-1) - Z_{t-1} = Z_t-Z_{t-1} + \\alpha, effectively removing the trend in the transformed series. We will return to this when considering ARIMA models.\nThere are different definitions of returns, but the most common ones are the standard returns, r_t, and log-returns, \\textrm{lr}_t, defined respectively by \\begin{equation*}\n\\begin{split}\nr_t &= \\frac{y_t-y_{t-1}}{y_{t-1}},\\\\\n\\textrm{lr}_t &= \\log y_t-\\log y_{t-1} = \\log\\frac{y_t}{y_{t-1}}.\n\\end{split}\n\\end{equation*} A daily return series for a stock usually has expectation close to zero and little autocorrelation, which can be convenient in many situations. However, they are typically hetereoskedastic (non-constant variance) and the squared returns are often autocorrelated. We will come back to this, when discussing volatility forecasting towards the end of the course.\n\n\nCode\n\n\n# Package for downloading stock data (primarily from Yahoo! Finance)\nlibrary(quantmod)\n\n# -- Download the data: --\ngetSymbols(\"AAPL\")\n\n[1] \"AAPL\"\n\n# -- Extract the closing price and create a tsibble: --\nclose.price &lt;- tibble(\n  close = as.numeric(AAPL$AAPL.Close),\n  date  = time(AAPL)\n) %&gt;% as_tsibble(index = date) \n\n# -- Plot closing price: --\nclose.price %&gt;% autoplot(close) +\n  labs(title = \"Apple Closing price\", \n       y     = \"US$\")\n\n\n\n# -- Adding transformations : -- \nclose.price &lt;- close.price %&gt;% \n  mutate(logclose  = log(close), # log-transform\n         logreturn = c(NA, diff(logclose)), # log returns\n         return    = c(NA, diff(close)/close[-nrow(close.price)]) # Returns\n         ) \n\n\n# -- Box-cox-transform --\nlambda &lt;- close.price %&gt;%\n  features(close, features = guerrero) %&gt;%\n  pull(lambda_guerrero)\nclose.price &lt;- close.price %&gt;%\n  mutate(boxcox = box_cox(close,lambda))\nclose.price %&gt;%\n  autoplot(boxcox) +\n  labs(y = \"\",\n       title = latex2exp::TeX(paste0(\n         \"Transformed apple closing price with $\\\\lambda$ = \",\n         round(lambda,3))))+\n  # Adding red curve with log-transform\n  geom_line(aes(y=logclose), col = 2) \n\n\n\n# -- Plotting the different transformations --\nclose.price %&gt;%\n  pivot_longer(-date) %&gt;% \n  autoplot(value) + \n  facet_wrap(~name, scales=\"free_y\", strip.position = \"left\")+\n  labs(title = \"Apple Closing Price transformations\") +\n  theme(strip.placement = \"outside\")"
  },
  {
    "objectID": "3_adjustments.html#sec-componentsandseasonaladjustment",
    "href": "3_adjustments.html#sec-componentsandseasonaladjustment",
    "title": "Transformations and adjustments",
    "section": "",
    "text": "Code\n\n\nset.seed(1344)\nlibrary(tidyverse)\nlibrary(fpp3)\ndat &lt;- tibble(\n  date = seq(as.Date(\"2015-01-01\"), as.Date(\"2020-12-31\"), by = \"1 day\"),\n  t = 1:length(date),\n  Tt = 100+9*t/300 + .5*((t-1000)/200)^3+(.87*cos(2*pi*t/1200)+.42*sin(2*pi*t/600))*11,\n  St = (.94*cos(2*pi*t/365) -1.2*sin(2* pi*t/365))*13,\n  Rt = rnorm(length(date), sd = 15),\n  Yt = Tt + St +  Rt\n) \n\n# -- Plotting Y and its components --\ndat %&gt;%\n  pivot_longer(cols = -c(date,t)) %&gt;% \n  mutate(name = factor(name, levels = c(\"Yt\", \"Tt\", \"St\", \"Rt\"))) %&gt;% # to order the panels\n  ggplot(aes(x=date, y = value, col = name)) + \n  geom_line() + facet_wrap(~name, ncol = 1, scales = \"free_y\", strip.position = \"left\") +\n  theme(strip.placement = \"outside\", axis.title = element_blank(), legend.position = \"none\")\n\n\n\n# -- Plotting seasonally adjusted Y --\nggplot(dat, aes(x = date, y = Yt-St)) + \n  geom_line() +\n  labs(title = \"Seasonally adjusted\", x = \"\")"
  },
  {
    "objectID": "1_rrecap.html",
    "href": "1_rrecap.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "If R and Rstudio are completely new tools for you, this section will probably not be detailed enough to get you started, but fear not. There are lots of good and useful online material for learning basic R. One possibility is to work through the first section (chapter 1-8) of the book R for data science by Wickham and Grolemund available online. There is also a free Coursera course on R programming, recommended by the textbook authors. The course BAN420 is recommended for taking this course. If you have not completed that course, you will find the material as videos on the BAN420 website. I recommend going through this material if you do not know it already. What follows below is a (very) short version of parts of Tuesday-Wednesday.\nWe will be using the tidyverse approach to doing data manipulation. This is in line with what you learn in courses like BAN400 R programming for Data Science or BAN420 Introduction to R and also with what the authors of the textbook does (Hyndman and Athanasopoulos,2021).\nOn canvas, you will find an excel file called US10YTRR.xlsx in the R recap folder. If you have experience with R, try to solve the following exercise without looking at the suggested solution.\n\n\nYou are given an .xlsx file (MS excel format) of daily prices of an US 10 year Treasury bond. The excel file contains several sheets with the\n\nClosing ask price (“Ask”)\nClosing bid price (“Bid”)\nClosing mid price (“Mid”)\n\nEach contains two columns: date and price. In the figure below we have taken a screen shot of the Mid sheet.\n\n\n\nUS 10-year Treasury bonds index collected from the Refinitiv Eikon data base.\n\n\nDo the following:\n\nSet working directory.\nLoad the data for the Mid price.\nFormat the date column in Date format.\nAdd a column for which year the observation is from.\nFilter away observations prior to 2010 and after 2021.\nRemove columns except date and price.\nSummarize data to monthly mean prices (hint: use tsibble::yearmonth function - this might be new to you!).\nMake a plot with months on x-axis and monthly means on y-axis.\nSave the figure to file.\n\nIf you get stuck, check out the (“hidden”) suggested solutions below.\n\n\nShow suggested solutions\n\nFirst, you need to set your working directory to the folder where you have saved the downloaded file and where you want to save the final figure. This can be done using the user interface in RStudio (“Session” -&gt; “Set working directory” -&gt; …) or using the command\n\nsetwd(\"../path/to/the/folder/\")\n\nYou are interested in reading in the closing mid price. To read in this data, you may use the following code.\n\nlibrary(fpp3)   # loading textbook package\nlibrary(tidyverse)\nlibrary(readxl) # loading package for reading excel files\ndat &lt;- read_excel(\"data/US10YTRR.xlsx\", sheet = \"Mid\")  \nhead(dat) # printing out the first 6 rows\n\n# A tibble: 6 × 2\n  date                price\n  &lt;dttm&gt;              &lt;dbl&gt;\n1 2022-08-30 00:00:00  96.9\n2 2022-08-29 00:00:00  96.9\n3 2022-08-26 00:00:00  97.6\n4 2022-08-25 00:00:00  97.6\n5 2022-08-24 00:00:00  96.9\n6 2022-08-23 00:00:00  97.4\n\n\nThe sheet argument specifies which sheet in the excel file we want to read. The read_excel function is also quite smart so it recognizes that the date column is a date and automatically format it accordingly. It is however perhaps not so useful to also include the time of the day (all is 00:00:00), so let us remove this part.\n\ndat %&gt;% \n  mutate(date = as.Date(date))\n\n# A tibble: 8,804 × 2\n   date       price\n   &lt;date&gt;     &lt;dbl&gt;\n 1 2022-08-30  96.9\n 2 2022-08-29  96.9\n 3 2022-08-26  97.6\n 4 2022-08-25  97.6\n 5 2022-08-24  96.9\n 6 2022-08-23  97.4\n 7 2022-08-22  97.7\n 8 2022-08-19  98.1\n 9 2022-08-18  98.8\n10 2022-08-17  98.7\n# ℹ 8,794 more rows\n\n\nHere I have used the mutate function. This is a function we use to either mutate (change) an existing column or create a new one. In this case we mutated the date column transforming it to a “Date” object/format. We could also be interested in adding a column for which year the observation is from.\n\ndat %&gt;% \n  mutate(date = as.Date(date),\n         year = year(date))\n\n# A tibble: 8,804 × 3\n   date       price  year\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 2022-08-30  96.9  2022\n 2 2022-08-29  96.9  2022\n 3 2022-08-26  97.6  2022\n 4 2022-08-25  97.6  2022\n 5 2022-08-24  96.9  2022\n 6 2022-08-23  97.4  2022\n 7 2022-08-22  97.7  2022\n 8 2022-08-19  98.1  2022\n 9 2022-08-18  98.8  2022\n10 2022-08-17  98.7  2022\n# ℹ 8,794 more rows\n\n\nHere we have used the year function from the lubridate package, which is loaded with the fpp3 package. The operator %&gt;% is used to add operations to the data manipulation pipeline in the given order. We start with the data object (a tibble) and add a mutate operation to that where we first transform the date column and add a year column. Now that we are pleased with our pipeline, let us save this to the dat object.\n\ndat &lt;- dat %&gt;% \n  mutate(date = as.Date(date),\n         year = year(date))\ndat %&gt;% glimpse()\n\nRows: 8,804\nColumns: 3\n$ date  &lt;date&gt; 2022-08-30, 2022-08-29, 2022-08-26, 2022-08-25, 2022-08-24, 202…\n$ price &lt;dbl&gt; 96.92969, 96.92188, 97.62500, 97.60156, 96.94531, 97.38281, 97.6…\n$ year  &lt;dbl&gt; 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022…\n\n\nThe glimpse function summarizes the tibble/data frame.\nfilter and select\nNow, the data ranges from/to\n\nrange(dat$date)\n\n[1] \"1987-08-03\" \"2022-08-30\"\n\n\nbut you only want to use data from 2010 onwards. To do this, we use the filter function. This function is useful for selecting rows that fulfill some condition, in this case year &gt;= 2010. Let us make a pipeline for this\n\ndat %&gt;% \n  filter(year &gt;= 2010)\n\n# A tibble: 3,178 × 3\n   date       price  year\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 2022-08-30  96.9  2022\n 2 2022-08-29  96.9  2022\n 3 2022-08-26  97.6  2022\n 4 2022-08-25  97.6  2022\n 5 2022-08-24  96.9  2022\n 6 2022-08-23  97.4  2022\n 7 2022-08-22  97.7  2022\n 8 2022-08-19  98.1  2022\n 9 2022-08-18  98.8  2022\n10 2022-08-17  98.7  2022\n# ℹ 3,168 more rows\n\n\nSince 2022 is not a complete year (in the data), you also do not want observations after 2021. Then you can add this as an extra condition.\n\ndat %&gt;% \n  filter(year &gt;= 2010, year &lt;=2021)\n\n# A tibble: 3,012 × 3\n   date       price  year\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 2021-12-31  98.8  2021\n 2 2021-12-30  98.8  2021\n 3 2021-12-29  98.4  2021\n 4 2021-12-28  99.0  2021\n 5 2021-12-27  99.1  2021\n 6 2021-12-23  98.9  2021\n 7 2021-12-22  99.3  2021\n 8 2021-12-21  99.2  2021\n 9 2021-12-20  99.5  2021\n10 2021-12-17  99.7  2021\n# ℹ 3,002 more rows\n\n\nAlternatively, you can use the between function\n\ndat %&gt;% \n  filter(between(year, 2010, 2021))\n\nwhich will produce the same result. Another useful function is called select. While filter is used on the rows of your data, select is for columns. Say we don’t need the year column after having filtered out the years we don’t want. We can then either select the columns we want to keep\n\ndat %&gt;% \n  filter(between(year, 2010, 2021)) %&gt;%\n  select(date, price)\n\nor remove the columns we do not want\n\ndat %&gt;% \n  filter(between(year, 2010, 2021)) %&gt;%\n  select(-year)\n\n# A tibble: 3,012 × 2\n   date       price\n   &lt;date&gt;     &lt;dbl&gt;\n 1 2021-12-31  98.8\n 2 2021-12-30  98.8\n 3 2021-12-29  98.4\n 4 2021-12-28  99.0\n 5 2021-12-27  99.1\n 6 2021-12-23  98.9\n 7 2021-12-22  99.3\n 8 2021-12-21  99.2\n 9 2021-12-20  99.5\n10 2021-12-17  99.7\n# ℹ 3,002 more rows\n\n\ngroup_by and summarize\nWe are interested in calculating the monthly mean price. In the tidyverse pipeline this means we want to group our observations according to month and year and summarize by month and year the mean of the observations.\n\n(monthlyMeans &lt;- dat %&gt;% \n  filter(between(year,2010,2021)) %&gt;%\n  mutate(monthyear = tsibble::yearmonth(date)) %&gt;%\n  group_by(monthyear) %&gt;%\n  summarize(meanPrice = mean(price)))\n\n# A tibble: 144 × 2\n   monthyear meanPrice\n       &lt;mth&gt;     &lt;dbl&gt;\n 1  2010 Jan      97.3\n 2  2010 Feb      98.7\n 3  2010 Mar      99.2\n 4  2010 Apr      98.4\n 5  2010 May     101. \n 6  2010 Jun     103. \n 7  2010 Jul     104. \n 8  2010 Aug     102. \n 9  2010 Sep      99.9\n10  2010 Oct     101. \n# ℹ 134 more rows\n\n\nThis pipeline could be read as first we take out observations prior to 1988 and after 2021, then we group the observations according to year and summarize the mean price by year. Note that this operation will delete any columns that are not in the group_by or being calculated in the summarize.\nggplot\nPlotting a data frame is convenient to do using the ggplot2 package. This will (when used appropriately) produce beautiful figures. Let us plot the time series at hand. The ggplot2 follows the same logic with a pipeline, but instead of the %&gt;% operator, we add elements to the figure using +. We need to specify the data object and the name of the x and y columns to be plotted. Everything in the figure that is to vary based on values in the data frame needs to be wrapped in a aes (aesthetic) function (here the x and y arguments). By adding the geom_line() we insert a line.\n\nggplot(data = monthlyMeans, \n       aes(x=monthyear, y = meanPrice)) +\n  geom_line()\n\n\n\n\nWe could instead add geom_point()\n\n ggplot(data = monthlyMeans, \n       aes(x=monthyear, y = meanPrice)) +\n   geom_point()\n\n\n\n\nor do both\n\n ggplot(data = monthlyMeans, \n       aes(x=monthyear, y = meanPrice)) +\n   geom_line() +\n   geom_point()\n\n\n\n\nWe can change the colors and decrease the size of the points:\n\n ggplot(data =monthlyMeans, \n       aes(x=monthyear, y = meanPrice)) +\n   geom_line(color = \"blue\") +\n   geom_point(color = \"green\", size = .2)\n\n\n\n\nOr maybe we do not want to use the default theme: –&gt;\n\n ggplot(data = monthlyMeans, \n       aes(x=monthyear, y = meanPrice)) +\n   geom_line(color = \"blue\") +\n   geom_point(color = \"green\", size = .2) +\n   theme_bw()\n\n\n\n\nWe can also include the plotting in our data manipulation pipeline. For instance, lets summarize the data by year and plot the resulting yearly time series.\n\ndat %&gt;%\n  filter(between(year, 1988, 2021)) %&gt;%\n  group_by(year) %&gt;%\n  summarize(meanPrice = mean(price)) %&gt;%\n  # adding plotting to pipeline:\n  ggplot(aes(x=year, y = meanPrice)) +\n  geom_line(color = \"blue\") +\n  geom_point(color = \"green\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe cannot illustrate all aspects here, but you will learn new elements by studying examples throughout the course. This recap is mostly for remembering the basics of data manipulation in R and simple plotting. As you will see in the continuation, the coding is not much more complex then what you have seen here and the fpp3 package uses the same type of logic and syntax as the tidyverse. There will however be some new functions specific for time series analysis that you will need to learn."
  },
  {
    "objectID": "1_rrecap.html#recap-r",
    "href": "1_rrecap.html#recap-r",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "If R and Rstudio are completely new tools for you, this section will probably not be detailed enough to get you started, but fear not. There are lots of good and useful online material for learning basic R. One possibility is to work through the first section (chapter 1-8) of the book R for data science by Wickham and Grolemund available online. There is also a free Coursera course on R programming, recommended by the textbook authors. The course BAN420 is recommended for taking this course. If you have not completed that course, you will find the material as videos on the BAN420 website. I recommend going through this material if you do not know it already. What follows below is a (very) short version of parts of Tuesday-Wednesday.\nWe will be using the tidyverse approach to doing data manipulation. This is in line with what you learn in courses like BAN400 R programming for Data Science or BAN420 Introduction to R and also with what the authors of the textbook does (Hyndman and Athanasopoulos,2021).\nOn canvas, you will find an excel file called US10YTRR.xlsx in the R recap folder. If you have experience with R, try to solve the following exercise without looking at the suggested solution.\n\n\nYou are given an .xlsx file (MS excel format) of daily prices of an US 10 year Treasury bond. The excel file contains several sheets with the\n\nClosing ask price (“Ask”)\nClosing bid price (“Bid”)\nClosing mid price (“Mid”)\n\nEach contains two columns: date and price. In the figure below we have taken a screen shot of the Mid sheet.\n\n\n\nUS 10-year Treasury bonds index collected from the Refinitiv Eikon data base.\n\n\nDo the following:\n\nSet working directory.\nLoad the data for the Mid price.\nFormat the date column in Date format.\nAdd a column for which year the observation is from.\nFilter away observations prior to 2010 and after 2021.\nRemove columns except date and price.\nSummarize data to monthly mean prices (hint: use tsibble::yearmonth function - this might be new to you!).\nMake a plot with months on x-axis and monthly means on y-axis.\nSave the figure to file.\n\nIf you get stuck, check out the (“hidden”) suggested solutions below.\n\n\nShow suggested solutions\n\nFirst, you need to set your working directory to the folder where you have saved the downloaded file and where you want to save the final figure. This can be done using the user interface in RStudio (“Session” -&gt; “Set working directory” -&gt; …) or using the command\n\nsetwd(\"../path/to/the/folder/\")\n\nYou are interested in reading in the closing mid price. To read in this data, you may use the following code.\n\nlibrary(fpp3)   # loading textbook package\nlibrary(tidyverse)\nlibrary(readxl) # loading package for reading excel files\ndat &lt;- read_excel(\"data/US10YTRR.xlsx\", sheet = \"Mid\")  \nhead(dat) # printing out the first 6 rows\n\n# A tibble: 6 × 2\n  date                price\n  &lt;dttm&gt;              &lt;dbl&gt;\n1 2022-08-30 00:00:00  96.9\n2 2022-08-29 00:00:00  96.9\n3 2022-08-26 00:00:00  97.6\n4 2022-08-25 00:00:00  97.6\n5 2022-08-24 00:00:00  96.9\n6 2022-08-23 00:00:00  97.4\n\n\nThe sheet argument specifies which sheet in the excel file we want to read. The read_excel function is also quite smart so it recognizes that the date column is a date and automatically format it accordingly. It is however perhaps not so useful to also include the time of the day (all is 00:00:00), so let us remove this part.\n\ndat %&gt;% \n  mutate(date = as.Date(date))\n\n# A tibble: 8,804 × 2\n   date       price\n   &lt;date&gt;     &lt;dbl&gt;\n 1 2022-08-30  96.9\n 2 2022-08-29  96.9\n 3 2022-08-26  97.6\n 4 2022-08-25  97.6\n 5 2022-08-24  96.9\n 6 2022-08-23  97.4\n 7 2022-08-22  97.7\n 8 2022-08-19  98.1\n 9 2022-08-18  98.8\n10 2022-08-17  98.7\n# ℹ 8,794 more rows\n\n\nHere I have used the mutate function. This is a function we use to either mutate (change) an existing column or create a new one. In this case we mutated the date column transforming it to a “Date” object/format. We could also be interested in adding a column for which year the observation is from.\n\ndat %&gt;% \n  mutate(date = as.Date(date),\n         year = year(date))\n\n# A tibble: 8,804 × 3\n   date       price  year\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 2022-08-30  96.9  2022\n 2 2022-08-29  96.9  2022\n 3 2022-08-26  97.6  2022\n 4 2022-08-25  97.6  2022\n 5 2022-08-24  96.9  2022\n 6 2022-08-23  97.4  2022\n 7 2022-08-22  97.7  2022\n 8 2022-08-19  98.1  2022\n 9 2022-08-18  98.8  2022\n10 2022-08-17  98.7  2022\n# ℹ 8,794 more rows\n\n\nHere we have used the year function from the lubridate package, which is loaded with the fpp3 package. The operator %&gt;% is used to add operations to the data manipulation pipeline in the given order. We start with the data object (a tibble) and add a mutate operation to that where we first transform the date column and add a year column. Now that we are pleased with our pipeline, let us save this to the dat object.\n\ndat &lt;- dat %&gt;% \n  mutate(date = as.Date(date),\n         year = year(date))\ndat %&gt;% glimpse()\n\nRows: 8,804\nColumns: 3\n$ date  &lt;date&gt; 2022-08-30, 2022-08-29, 2022-08-26, 2022-08-25, 2022-08-24, 202…\n$ price &lt;dbl&gt; 96.92969, 96.92188, 97.62500, 97.60156, 96.94531, 97.38281, 97.6…\n$ year  &lt;dbl&gt; 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022…\n\n\nThe glimpse function summarizes the tibble/data frame.\nfilter and select\nNow, the data ranges from/to\n\nrange(dat$date)\n\n[1] \"1987-08-03\" \"2022-08-30\"\n\n\nbut you only want to use data from 2010 onwards. To do this, we use the filter function. This function is useful for selecting rows that fulfill some condition, in this case year &gt;= 2010. Let us make a pipeline for this\n\ndat %&gt;% \n  filter(year &gt;= 2010)\n\n# A tibble: 3,178 × 3\n   date       price  year\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 2022-08-30  96.9  2022\n 2 2022-08-29  96.9  2022\n 3 2022-08-26  97.6  2022\n 4 2022-08-25  97.6  2022\n 5 2022-08-24  96.9  2022\n 6 2022-08-23  97.4  2022\n 7 2022-08-22  97.7  2022\n 8 2022-08-19  98.1  2022\n 9 2022-08-18  98.8  2022\n10 2022-08-17  98.7  2022\n# ℹ 3,168 more rows\n\n\nSince 2022 is not a complete year (in the data), you also do not want observations after 2021. Then you can add this as an extra condition.\n\ndat %&gt;% \n  filter(year &gt;= 2010, year &lt;=2021)\n\n# A tibble: 3,012 × 3\n   date       price  year\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 2021-12-31  98.8  2021\n 2 2021-12-30  98.8  2021\n 3 2021-12-29  98.4  2021\n 4 2021-12-28  99.0  2021\n 5 2021-12-27  99.1  2021\n 6 2021-12-23  98.9  2021\n 7 2021-12-22  99.3  2021\n 8 2021-12-21  99.2  2021\n 9 2021-12-20  99.5  2021\n10 2021-12-17  99.7  2021\n# ℹ 3,002 more rows\n\n\nAlternatively, you can use the between function\n\ndat %&gt;% \n  filter(between(year, 2010, 2021))\n\nwhich will produce the same result. Another useful function is called select. While filter is used on the rows of your data, select is for columns. Say we don’t need the year column after having filtered out the years we don’t want. We can then either select the columns we want to keep\n\ndat %&gt;% \n  filter(between(year, 2010, 2021)) %&gt;%\n  select(date, price)\n\nor remove the columns we do not want\n\ndat %&gt;% \n  filter(between(year, 2010, 2021)) %&gt;%\n  select(-year)\n\n# A tibble: 3,012 × 2\n   date       price\n   &lt;date&gt;     &lt;dbl&gt;\n 1 2021-12-31  98.8\n 2 2021-12-30  98.8\n 3 2021-12-29  98.4\n 4 2021-12-28  99.0\n 5 2021-12-27  99.1\n 6 2021-12-23  98.9\n 7 2021-12-22  99.3\n 8 2021-12-21  99.2\n 9 2021-12-20  99.5\n10 2021-12-17  99.7\n# ℹ 3,002 more rows\n\n\ngroup_by and summarize\nWe are interested in calculating the monthly mean price. In the tidyverse pipeline this means we want to group our observations according to month and year and summarize by month and year the mean of the observations.\n\n(monthlyMeans &lt;- dat %&gt;% \n  filter(between(year,2010,2021)) %&gt;%\n  mutate(monthyear = tsibble::yearmonth(date)) %&gt;%\n  group_by(monthyear) %&gt;%\n  summarize(meanPrice = mean(price)))\n\n# A tibble: 144 × 2\n   monthyear meanPrice\n       &lt;mth&gt;     &lt;dbl&gt;\n 1  2010 Jan      97.3\n 2  2010 Feb      98.7\n 3  2010 Mar      99.2\n 4  2010 Apr      98.4\n 5  2010 May     101. \n 6  2010 Jun     103. \n 7  2010 Jul     104. \n 8  2010 Aug     102. \n 9  2010 Sep      99.9\n10  2010 Oct     101. \n# ℹ 134 more rows\n\n\nThis pipeline could be read as first we take out observations prior to 1988 and after 2021, then we group the observations according to year and summarize the mean price by year. Note that this operation will delete any columns that are not in the group_by or being calculated in the summarize.\nggplot\nPlotting a data frame is convenient to do using the ggplot2 package. This will (when used appropriately) produce beautiful figures. Let us plot the time series at hand. The ggplot2 follows the same logic with a pipeline, but instead of the %&gt;% operator, we add elements to the figure using +. We need to specify the data object and the name of the x and y columns to be plotted. Everything in the figure that is to vary based on values in the data frame needs to be wrapped in a aes (aesthetic) function (here the x and y arguments). By adding the geom_line() we insert a line.\n\nggplot(data = monthlyMeans, \n       aes(x=monthyear, y = meanPrice)) +\n  geom_line()\n\n\n\n\nWe could instead add geom_point()\n\n ggplot(data = monthlyMeans, \n       aes(x=monthyear, y = meanPrice)) +\n   geom_point()\n\n\n\n\nor do both\n\n ggplot(data = monthlyMeans, \n       aes(x=monthyear, y = meanPrice)) +\n   geom_line() +\n   geom_point()\n\n\n\n\nWe can change the colors and decrease the size of the points:\n\n ggplot(data =monthlyMeans, \n       aes(x=monthyear, y = meanPrice)) +\n   geom_line(color = \"blue\") +\n   geom_point(color = \"green\", size = .2)\n\n\n\n\nOr maybe we do not want to use the default theme: –&gt;\n\n ggplot(data = monthlyMeans, \n       aes(x=monthyear, y = meanPrice)) +\n   geom_line(color = \"blue\") +\n   geom_point(color = \"green\", size = .2) +\n   theme_bw()\n\n\n\n\nWe can also include the plotting in our data manipulation pipeline. For instance, lets summarize the data by year and plot the resulting yearly time series.\n\ndat %&gt;%\n  filter(between(year, 1988, 2021)) %&gt;%\n  group_by(year) %&gt;%\n  summarize(meanPrice = mean(price)) %&gt;%\n  # adding plotting to pipeline:\n  ggplot(aes(x=year, y = meanPrice)) +\n  geom_line(color = \"blue\") +\n  geom_point(color = \"green\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe cannot illustrate all aspects here, but you will learn new elements by studying examples throughout the course. This recap is mostly for remembering the basics of data manipulation in R and simple plotting. As you will see in the continuation, the coding is not much more complex then what you have seen here and the fpp3 package uses the same type of logic and syntax as the tidyverse. There will however be some new functions specific for time series analysis that you will need to learn."
  },
  {
    "objectID": "1_installingrandrstudio.html",
    "href": "1_installingrandrstudio.html",
    "title": "R and Rstudio",
    "section": "",
    "text": "R and Rstudio\nIn this course, we will be using R and Rstudio to e.g. visualize time series, estimate model parameters, forecast, etc. It is therefore essential to have some basic knowledge of how to write an R script and how to read in data and do some simple data manipulation for preparing the data for different time series analysis. Hopefully, most of you have some experience with R and Rstudio before. If you have, this will be a short recap, if not this will be a very short introduction covering the most basic operations.\n\nInstalling R and Rstudio\n\nInstall R:\n\nGo to: cran.uib.no\nPress download R for Linux/MacOS/Windows\nPress base\nDownload R-4.x.x for Linux/MacOS/Windows\nRun the installation using default options\n\nInstall Rstudio\n\nGo to: rstudio.com\nSelect Rstudio desktop\nPress Download Rstudio desktop\nSelect the Rstudio desktop with open source licence, which is free\n\nSelect the version for your operating system\nRun the installation using default settings\n\nOpen Rstudio and check that it works (it should start without any error messages).\nInstall the R-package of the book “fpp3”.\n\nIn Rstudio, select Tools -&gt; Install packages -&gt; write “fpp3” and make sure install dependencies is marked. Press Install. You can also run the following code in the console\n\n\n\ninstall.packages(\"fpp3\", dependencies = TRUE)\n\n\n\nOther useful packages to install are\n\ninstall.packages(\"tidyverse\", dependencies = TRUE)\ninstall.packages(\"readxl\")\n\n(will add other packages as we go)"
  },
  {
    "objectID": "12_data_appendix.html",
    "href": "12_data_appendix.html",
    "title": "Data",
    "section": "",
    "text": "Data\nYou can find a overview of the data that is included in the textbook package fpp3 by running the following code:\n\ndata(package = \"fpp3\")\n\n\n\n# A tibble: 13 × 2\n   Item              Title                                                 \n   &lt;chr&gt;             &lt;chr&gt;                                                 \n 1 aus_accommodation Australian accommodation data                         \n 2 aus_airpassengers Air Transport Passengers Australia                    \n 3 aus_arrivals      International Arrivals to Australia                   \n 4 bank_calls        Call volume for a large North American commercial bank\n 5 boston_marathon   Boston marathon winning times since 1897              \n 6 canadian_gas      Monthly Canadian gas production                       \n 7 guinea_rice       Rice production (Guinea)                              \n 8 insurance         Insurance quotations and advertising expenditure      \n 9 prices            Price series for various commodities                  \n10 souvenirs         Sales for a souvenir shop                             \n11 us_change         Percentage changes in economic variables in the USA.  \n12 us_employment     US monthly employment data                            \n13 us_gasoline       US finished motor gasoline product supplied.          \n\n\nThese are avaiable when the fpp3 package is loaded, i.e.\n\nlibrary(fpp3)\nbank_calls\n\n# A tsibble: 27,716 x 2 [5m] &lt;UTC&gt;\n   DateTime            Calls\n   &lt;dttm&gt;              &lt;dbl&gt;\n 1 2003-03-03 07:00:00   111\n 2 2003-03-03 07:05:00   113\n 3 2003-03-03 07:10:00    76\n 4 2003-03-03 07:15:00    82\n 5 2003-03-03 07:20:00    91\n 6 2003-03-03 07:25:00    87\n 7 2003-03-03 07:30:00    75\n 8 2003-03-03 07:35:00    89\n 9 2003-03-03 07:40:00    99\n10 2003-03-03 07:45:00   125\n# ℹ 27,706 more rows\n\n\nTo load a specific data set explicitly in your R environment:\n\ndata(\"bank_calls\")\n\nOther examples used in the videos and content on this website is available for download at github.com/holleland/BAN430/tree/master/data. You should also be able to load them directly into R using the raw link:\n\n# CPI Norway\nread.csv(\n  \"https://raw.githubusercontent.com/holleland/BAN430/master/data/CPI_norway.csv\", sep = \";\") %&gt;% head()\n\n     X Y.avg2   Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov\n1 2022      . 117.8 119.1 119.8 121.2 121.5 122.6 124.2 123.9 125.6     .     .\n2 2021  116.1 114.1 114.9 114.6 115.0 114.9 115.3 116.3 116.3 117.5 117.2 118.1\n3 2020  112.2 111.3 111.2 111.2 111.7 111.9 112.1 112.9 112.5 112.9 113.2 112.4\n4 2019  110.8 109.3 110.2 110.4 110.8 110.5 110.6 111.4 110.6 111.1 111.3 111.6\n5 2018  108.4 106.0 107.0 107.3 107.7 107.8 108.5 109.3 108.9 109.5 109.3 109.8\n6 2017  105.5 104.3 104.7 105.0 105.2 105.4 105.8 106.1 105.3 105.9 106.0 106.1\n    Dec\n1     .\n2 118.9\n3 112.9\n4 111.3\n5 109.8\n6 106.1\n\n\nFor excel files it is a bit less convenient, because you will need to download the file. But you can let R do that for you (if you insist).\n\nloadExcel_url &lt;- function(url) {\n    temp_file &lt;- tempfile(fileext = \".xlsx\")\n    download.file(url = url, destfile = temp_file, mode = \"wb\", quiet = TRUE)\n    readxl::read_excel(temp_file)\n}\nloadExcel_url(\"https://github.com/holleland/BAN430/blob/master/data/NorwayEmployment_15-74years_bySex.xlsx?raw=true\")\n\n# A tibble: 214 × 3\n   Sex   Quarter `Employed persons (1 000 persons)`\n   &lt;chr&gt; &lt;chr&gt;                                &lt;dbl&gt;\n 1 Male  1996K1                                1133\n 2 Male  1996K2                                1152\n 3 Male  1996K3                                1171\n 4 Male  1996K4                                1161\n 5 Male  1997K1                                1164\n 6 Male  1997K2                                1189\n 7 Male  1997K3                                1200\n 8 Male  1997K4                                1189\n 9 Male  1998K1                                1195\n10 Male  1998K2                                1214\n# ℹ 204 more rows\n\n\nThe code above will save the file temporary in your computers temporary folder and load it into R from there. You could also adjust the code so that it stores the file in your working directory by adjusting the function.\n\ntemp_file &lt;- paste0(getwd(), \"/NorwayEmployment.xlsx\")\n\nBut the easiest will maybe be to just download the files manually from github and save them in a data folder of your own."
  },
  {
    "objectID": "10_Practicle_forecasting_issues.html",
    "href": "10_Practicle_forecasting_issues.html",
    "title": "Practicle forecasting issues",
    "section": "",
    "text": "Practicle forecasting issues\n\n1+2\n\n[1] 3"
  },
  {
    "objectID": "1_1_readingData.html",
    "href": "1_1_readingData.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "This is a book created from markdown and executable code."
  },
  {
    "objectID": "1_1_readingData.html#reading-data",
    "href": "1_1_readingData.html#reading-data",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "This is a book created from markdown and executable code."
  },
  {
    "objectID": "1_1_readingData.html#exercises",
    "href": "1_1_readingData.html#exercises",
    "title": "BAN430 Forecasting",
    "section": "Exercises",
    "text": "Exercises\nIs 1+1 the same as 2?\n\n\nSolution\n\n\n1+1 == 2"
  },
  {
    "objectID": "1_randrstudio.html",
    "href": "1_randrstudio.html",
    "title": "R and Rstudio",
    "section": "",
    "text": "In this course, we will be using R and Rstudio to e.g. visualize time series, estimate model parameters, forecast, etc. It is therefore essential to have some basic knowledge of how to write an R script and how to read in data and do some simple data manipulation for preparing the data for different time series analysis. Hopefully, most of you have some experience with R and Rstudio before. If you have, this will be a short recap, if not this will be a very short introduction covering the most basic operations.\n\n\n\nInstall R:\n\nGo to: cran.uib.no\nPress download R for Linux/MacOS/Windows\nPress base\nDownload R-4.x.x for Linux/MacOS/Windows\nRun the installation using default options\n\nInstall Rstudio\n\nGo to: rstudio.com\nSelect Rstudio desktop\nPress Download Rstudio desktop\nSelect the Rstudio desktop with open source licence, which is free\n\nSelect the version for your operating system\nRun the installation using default settings\n\nOpen Rstudio and check that it works (it should start without any error messages).\nInstall the R-package of the book “fpp3”.\n\nIn Rstudio, select Tools -&gt; Install packages -&gt; write “fpp3” and make sure install dependencies is marked. Press Install. You can also run the following code in the console\n\n\n\ninstall.packages(\"fpp3\", dependencies = TRUE)\n\n\n\nOther useful packages to install are\n\ninstall.packages(\"tidyverse\", dependencies = TRUE)\ninstall.packages(\"readxl\")\n\n(will add other packages as we go)\n\n\n\nIf R and Rstudio are completely new tools for you, this section will probably not be detailed enough to get you started, but fear not. There are lots of good and useful online material for learning basic R. One possibility is to work through the first section of (chapter 1-8) of the book R for data science by @wickham2016r available online. There is also a free Coursera course on R programming, recommended by the textbook authors.\nWe will mostly be using the tidyverse approach to doing data manipulation. This is in line with what you learn in courses like BAN400 R programming for Data Science or BAN420 Introduction to R and also with what the authors of the textbook does (@hyndman2018).\nSay you are given an .xlsx file (MS excel format) of daily prices of an US 10 year Treasury bond. The excel file contains several sheets with the\n\nClosing ask price (“Ask”)\nClosing bid price (“Bid”)\nClosing mid price (“Mid”)\n\nEach contains two columns: date and price. In the figure below we have taken a screen shot of the Mid sheet.\n\n\n\nUS 10-year Treasury bonds index collected from the Refinitiv Eikon data base.\n\n\nYou are interesting in reading in the closing mid price. To read in this data, you may use the following code.\n\nlibrary(fpp3)   # loading textbook package\nlibrary(tidyverse)\nlibrary(readxl) # loading package for reading excel files\ndat &lt;- read_excel(\"data/US10YTRR.xlsx\", sheet = \"Mid\")  \nhead(dat) # printing out the first 6 rows\n\n# A tibble: 6 × 2\n  date                price\n  &lt;dttm&gt;              &lt;dbl&gt;\n1 2022-08-30 00:00:00  96.9\n2 2022-08-29 00:00:00  96.9\n3 2022-08-26 00:00:00  97.6\n4 2022-08-25 00:00:00  97.6\n5 2022-08-24 00:00:00  96.9\n6 2022-08-23 00:00:00  97.4\n\n\nThe sheet argument specifies which sheet in the excel file we want to read. The read_excel function is also quite smart so it recognizes that the date column is a date and automatically format it accordingly. It is however perhaps not so useful to also include the time of the day (all is 00:00:00), so let us remove this part.\n\ndat %&gt;% \n  mutate(date = as.Date(date))\n\n# A tibble: 8,804 × 2\n   date       price\n   &lt;date&gt;     &lt;dbl&gt;\n 1 2022-08-30  96.9\n 2 2022-08-29  96.9\n 3 2022-08-26  97.6\n 4 2022-08-25  97.6\n 5 2022-08-24  96.9\n 6 2022-08-23  97.4\n 7 2022-08-22  97.7\n 8 2022-08-19  98.1\n 9 2022-08-18  98.8\n10 2022-08-17  98.7\n# ℹ 8,794 more rows\n\n\nHere I have used the mutate function. This is a function we use to either mutate an existing column or create a new one. In this case we mutated the date column transforming it to a “Date” object. We could also be intersted in adding a column for which year the observation is from.\n\ndat %&gt;% \n  mutate(date = as.Date(date),\n         year = year(date))\n\n# A tibble: 8,804 × 3\n   date       price  year\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 2022-08-30  96.9  2022\n 2 2022-08-29  96.9  2022\n 3 2022-08-26  97.6  2022\n 4 2022-08-25  97.6  2022\n 5 2022-08-24  96.9  2022\n 6 2022-08-23  97.4  2022\n 7 2022-08-22  97.7  2022\n 8 2022-08-19  98.1  2022\n 9 2022-08-18  98.8  2022\n10 2022-08-17  98.7  2022\n# ℹ 8,794 more rows\n\n\nHere we have used the year function from the lubridate package, which is loaded with the fpp3 package. The operator %&gt;% is used to add operations to the data manipulation pipeline in the given order. We start with the data object (a tibble) and add a mutate operation to that where we first transform the date column and add a year column. Now that we are pleased with our pipeline, let us save this to the dat object.\n\ndat &lt;- dat %&gt;% \n  mutate(date = as.Date(date),\n         year = year(date))\ndat %&gt;% glimpse()\n\nRows: 8,804\nColumns: 3\n$ date  &lt;date&gt; 2022-08-30, 2022-08-29, 2022-08-26, 2022-08-25, 2022-08-24, 202…\n$ price &lt;dbl&gt; 96.92969, 96.92188, 97.62500, 97.60156, 96.94531, 97.38281, 97.6…\n$ year  &lt;dbl&gt; 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022…\n\n\nThe glimpse function summarizes the tibble/data frame.\n\n\nNow, the data ranges from/to\n\nrange(dat$date)\n\n[1] \"1987-08-03\" \"2022-08-30\"\n\n\nbut say you only want to use data from 2010 onwards. To do this, we use the filter function. This function is useful for selecting rows that fulfil some condition, in this case year &gt;= 2010. Let us make a pipeline for this\n\ndat %&gt;% \n  filter(year &gt;= 2010)\n\n# A tibble: 3,178 × 3\n   date       price  year\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 2022-08-30  96.9  2022\n 2 2022-08-29  96.9  2022\n 3 2022-08-26  97.6  2022\n 4 2022-08-25  97.6  2022\n 5 2022-08-24  96.9  2022\n 6 2022-08-23  97.4  2022\n 7 2022-08-22  97.7  2022\n 8 2022-08-19  98.1  2022\n 9 2022-08-18  98.8  2022\n10 2022-08-17  98.7  2022\n# ℹ 3,168 more rows\n\n\nSince 2022 is not a complete year (in the data), you also don’t want observations after 2021. Then you can add this as an extra condition.\n\ndat %&gt;% \n  filter(year &gt;= 2010, year &lt;=2021)\n\n# A tibble: 3,012 × 3\n   date       price  year\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 2021-12-31  98.8  2021\n 2 2021-12-30  98.8  2021\n 3 2021-12-29  98.4  2021\n 4 2021-12-28  99.0  2021\n 5 2021-12-27  99.1  2021\n 6 2021-12-23  98.9  2021\n 7 2021-12-22  99.3  2021\n 8 2021-12-21  99.2  2021\n 9 2021-12-20  99.5  2021\n10 2021-12-17  99.7  2021\n# ℹ 3,002 more rows\n\n\nAlternatively, you can use the between function\n\ndat %&gt;% \n  filter(between(year, 2010, 2021))\n\nwhich will produce the same result. Another useful function is called select. While filter is used on the rows of your data, select is for columns. Say we don’t need the year column after having filtered out the years we don’t want. We can then either select the columns we want to keep\n\ndat %&gt;% \n  filter(between(year, 2010, 2021)) %&gt;%\n  select(date, price)\n\nor remove the columns we do not want\n\ndat %&gt;% \n  filter(between(year, 2010, 2021)) %&gt;%\n  select(-year)\n\n# A tibble: 3,012 × 2\n   date       price\n   &lt;date&gt;     &lt;dbl&gt;\n 1 2021-12-31  98.8\n 2 2021-12-30  98.8\n 3 2021-12-29  98.4\n 4 2021-12-28  99.0\n 5 2021-12-27  99.1\n 6 2021-12-23  98.9\n 7 2021-12-22  99.3\n 8 2021-12-21  99.2\n 9 2021-12-20  99.5\n10 2021-12-17  99.7\n# ℹ 3,002 more rows\n\n\n\n\n\nSay we are interested in calculating the yearly mean price. In the tidyverse pipeline this means we want to group our observations according to year and summarize by year the mean of the observations. We will filter to avoid having the first and last years that are incomplete.\n\ndat %&gt;% \n  filter(between(year, 1988, 2021)) %&gt;%\n  group_by(year) %&gt;%\n  summarize(meanPrice = mean(price))\n\n# A tibble: 34 × 2\n    year meanPrice\n   &lt;dbl&gt;     &lt;dbl&gt;\n 1  1988      99.7\n 2  1989     101. \n 3  1990     100. \n 4  1991     100. \n 5  1992     100. \n 6  1993     102. \n 7  1994      98.3\n 8  1995     102. \n 9  1996      99.8\n10  1997      99.9\n# ℹ 24 more rows\n\n\nThis pipeline could be read as first we take out observations prior to 1988 and after 2021, then we group the observations according to year and summarize the mean price by year. Note that this operation will delete any columns that are not in the group_by or being calculated in the summarize.\n\n\n\nPlotting a data frame is convenient to do using the ggplot2 package. This will (when used appropriately) produce beautiful figures. Let us plot the time series at hand. The ggplot2 follows the same logic with a pipeline, but instead of the %&gt;% operator, we add elements to the figure using +. We need to specify the data object and the name of the x and y columns to be plotted. Everything in the figure that is to vary based on values in the data frame needs to be wrapped in a aes (aesthetic) function (here the x and y arguments). By adding the geom_line() we insert a line.\n\nggplot(data = dat, \n       aes(x=date, y = price)) +\n  geom_line()\n\n\n\n\nWe could instead add geom_point()\n\n ggplot(data = dat,\n        aes(x=date, y = price)) +\n   geom_point()\n\n\n\n\nor do both\n\n ggplot(data = dat,\n        aes(x=date, y = price)) +\n   geom_line() +\n   geom_point()\n\n\n\n\nWe can change the colors and decrease the size of the points:\n\n ggplot(data = dat,\n        aes(x=date, y = price)) +\n   geom_line(color = \"blue\") +\n   geom_point(color = \"green\", size = .2)\n\n\n\n\nOr maybe we do not want to use the default theme: –&gt;\n\n ggplot(data = dat,\n        aes(x=date, y = price)) +\n   geom_line(color = \"blue\") +\n   geom_point(color = \"green\", size = .2) +\n   theme_bw()\n\n\n\n\nWe can also include the plotting in our data manipulation pipeline. For instance, lets summarize the data by year and plot the resulting yearly time series.\n\ndat %&gt;%\n  filter(between(year, 1988, 2021)) %&gt;%\n  group_by(year) %&gt;%\n  summarize(meanPrice = mean(price)) %&gt;%\n  # adding plotting to pipeline:\n  ggplot(aes(x=year, y = meanPrice)) +\n  geom_line(color = \"blue\") +\n  geom_point(color = \"green\") +\n  theme_bw()\n\n\n\n\n\n\n\nWe cannot illustrate all aspects here, but you will learn new elements by studying examples throughout the course. This recap is mostly for remembering the basics of data manipulation in R and simple plotting. As you will see in the continuation, the coding is not much more complex then what you have seen here and the fpp3 package uses the same type of logic and syntax as the tidyverse. There will however be some new functions specific for time series analysis that you will need to learn.\n\n\n\n\nSet working directory.\nLoad the data.\nFilter away observations prior to 2010.\nRemove columns except .. and time\nSummarize data to monthly means\nMake a plot with time on x-axis and monthly means on y-axis\nSave the figure to file."
  },
  {
    "objectID": "1_randrstudio.html#r-recap",
    "href": "1_randrstudio.html#r-recap",
    "title": "R and Rstudio",
    "section": "",
    "text": "If R and Rstudio are completely new tools for you, this section will probably not be detailed enough to get you started, but fear not. There are lots of good and useful online material for learning basic R. One possibility is to work through the first section of (chapter 1-8) of the book R for data science by @wickham2016r available online. There is also a free Coursera course on R programming, recommended by the textbook authors.\nWe will mostly be using the tidyverse approach to doing data manipulation. This is in line with what you learn in courses like BAN400 R programming for Data Science or BAN420 Introduction to R and also with what the authors of the textbook does (@hyndman2018).\nSay you are given an .xlsx file (MS excel format) of daily prices of an US 10 year Treasury bond. The excel file contains several sheets with the\n\nClosing ask price (“Ask”)\nClosing bid price (“Bid”)\nClosing mid price (“Mid”)\n\nEach contains two columns: date and price. In the figure below we have taken a screen shot of the Mid sheet.\n\n\n\nUS 10-year Treasury bonds index collected from the Refinitiv Eikon data base.\n\n\nYou are interesting in reading in the closing mid price. To read in this data, you may use the following code.\n\nlibrary(fpp3)   # loading textbook package\nlibrary(tidyverse)\nlibrary(readxl) # loading package for reading excel files\ndat &lt;- read_excel(\"data/US10YTRR.xlsx\", sheet = \"Mid\")  \nhead(dat) # printing out the first 6 rows\n\n# A tibble: 6 × 2\n  date                price\n  &lt;dttm&gt;              &lt;dbl&gt;\n1 2022-08-30 00:00:00  96.9\n2 2022-08-29 00:00:00  96.9\n3 2022-08-26 00:00:00  97.6\n4 2022-08-25 00:00:00  97.6\n5 2022-08-24 00:00:00  96.9\n6 2022-08-23 00:00:00  97.4\n\n\nThe sheet argument specifies which sheet in the excel file we want to read. The read_excel function is also quite smart so it recognizes that the date column is a date and automatically format it accordingly. It is however perhaps not so useful to also include the time of the day (all is 00:00:00), so let us remove this part.\n\ndat %&gt;% \n  mutate(date = as.Date(date))\n\n# A tibble: 8,804 × 2\n   date       price\n   &lt;date&gt;     &lt;dbl&gt;\n 1 2022-08-30  96.9\n 2 2022-08-29  96.9\n 3 2022-08-26  97.6\n 4 2022-08-25  97.6\n 5 2022-08-24  96.9\n 6 2022-08-23  97.4\n 7 2022-08-22  97.7\n 8 2022-08-19  98.1\n 9 2022-08-18  98.8\n10 2022-08-17  98.7\n# ℹ 8,794 more rows\n\n\nHere I have used the mutate function. This is a function we use to either mutate an existing column or create a new one. In this case we mutated the date column transforming it to a “Date” object. We could also be intersted in adding a column for which year the observation is from.\n\ndat %&gt;% \n  mutate(date = as.Date(date),\n         year = year(date))\n\n# A tibble: 8,804 × 3\n   date       price  year\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 2022-08-30  96.9  2022\n 2 2022-08-29  96.9  2022\n 3 2022-08-26  97.6  2022\n 4 2022-08-25  97.6  2022\n 5 2022-08-24  96.9  2022\n 6 2022-08-23  97.4  2022\n 7 2022-08-22  97.7  2022\n 8 2022-08-19  98.1  2022\n 9 2022-08-18  98.8  2022\n10 2022-08-17  98.7  2022\n# ℹ 8,794 more rows\n\n\nHere we have used the year function from the lubridate package, which is loaded with the fpp3 package. The operator %&gt;% is used to add operations to the data manipulation pipeline in the given order. We start with the data object (a tibble) and add a mutate operation to that where we first transform the date column and add a year column. Now that we are pleased with our pipeline, let us save this to the dat object.\n\ndat &lt;- dat %&gt;% \n  mutate(date = as.Date(date),\n         year = year(date))\ndat %&gt;% glimpse()\n\nRows: 8,804\nColumns: 3\n$ date  &lt;date&gt; 2022-08-30, 2022-08-29, 2022-08-26, 2022-08-25, 2022-08-24, 202…\n$ price &lt;dbl&gt; 96.92969, 96.92188, 97.62500, 97.60156, 96.94531, 97.38281, 97.6…\n$ year  &lt;dbl&gt; 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022…\n\n\nThe glimpse function summarizes the tibble/data frame.\n\n\nNow, the data ranges from/to\n\nrange(dat$date)\n\n[1] \"1987-08-03\" \"2022-08-30\"\n\n\nbut say you only want to use data from 2010 onwards. To do this, we use the filter function. This function is useful for selecting rows that fulfil some condition, in this case year &gt;= 2010. Let us make a pipeline for this\n\ndat %&gt;% \n  filter(year &gt;= 2010)\n\n# A tibble: 3,178 × 3\n   date       price  year\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 2022-08-30  96.9  2022\n 2 2022-08-29  96.9  2022\n 3 2022-08-26  97.6  2022\n 4 2022-08-25  97.6  2022\n 5 2022-08-24  96.9  2022\n 6 2022-08-23  97.4  2022\n 7 2022-08-22  97.7  2022\n 8 2022-08-19  98.1  2022\n 9 2022-08-18  98.8  2022\n10 2022-08-17  98.7  2022\n# ℹ 3,168 more rows\n\n\nSince 2022 is not a complete year (in the data), you also don’t want observations after 2021. Then you can add this as an extra condition.\n\ndat %&gt;% \n  filter(year &gt;= 2010, year &lt;=2021)\n\n# A tibble: 3,012 × 3\n   date       price  year\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 2021-12-31  98.8  2021\n 2 2021-12-30  98.8  2021\n 3 2021-12-29  98.4  2021\n 4 2021-12-28  99.0  2021\n 5 2021-12-27  99.1  2021\n 6 2021-12-23  98.9  2021\n 7 2021-12-22  99.3  2021\n 8 2021-12-21  99.2  2021\n 9 2021-12-20  99.5  2021\n10 2021-12-17  99.7  2021\n# ℹ 3,002 more rows\n\n\nAlternatively, you can use the between function\n\ndat %&gt;% \n  filter(between(year, 2010, 2021))\n\nwhich will produce the same result. Another useful function is called select. While filter is used on the rows of your data, select is for columns. Say we don’t need the year column after having filtered out the years we don’t want. We can then either select the columns we want to keep\n\ndat %&gt;% \n  filter(between(year, 2010, 2021)) %&gt;%\n  select(date, price)\n\nor remove the columns we do not want\n\ndat %&gt;% \n  filter(between(year, 2010, 2021)) %&gt;%\n  select(-year)\n\n# A tibble: 3,012 × 2\n   date       price\n   &lt;date&gt;     &lt;dbl&gt;\n 1 2021-12-31  98.8\n 2 2021-12-30  98.8\n 3 2021-12-29  98.4\n 4 2021-12-28  99.0\n 5 2021-12-27  99.1\n 6 2021-12-23  98.9\n 7 2021-12-22  99.3\n 8 2021-12-21  99.2\n 9 2021-12-20  99.5\n10 2021-12-17  99.7\n# ℹ 3,002 more rows\n\n\n\n\n\nSay we are interested in calculating the yearly mean price. In the tidyverse pipeline this means we want to group our observations according to year and summarize by year the mean of the observations. We will filter to avoid having the first and last years that are incomplete.\n\ndat %&gt;% \n  filter(between(year, 1988, 2021)) %&gt;%\n  group_by(year) %&gt;%\n  summarize(meanPrice = mean(price))\n\n# A tibble: 34 × 2\n    year meanPrice\n   &lt;dbl&gt;     &lt;dbl&gt;\n 1  1988      99.7\n 2  1989     101. \n 3  1990     100. \n 4  1991     100. \n 5  1992     100. \n 6  1993     102. \n 7  1994      98.3\n 8  1995     102. \n 9  1996      99.8\n10  1997      99.9\n# ℹ 24 more rows\n\n\nThis pipeline could be read as first we take out observations prior to 1988 and after 2021, then we group the observations according to year and summarize the mean price by year. Note that this operation will delete any columns that are not in the group_by or being calculated in the summarize.\n\n\n\nPlotting a data frame is convenient to do using the ggplot2 package. This will (when used appropriately) produce beautiful figures. Let us plot the time series at hand. The ggplot2 follows the same logic with a pipeline, but instead of the %&gt;% operator, we add elements to the figure using +. We need to specify the data object and the name of the x and y columns to be plotted. Everything in the figure that is to vary based on values in the data frame needs to be wrapped in a aes (aesthetic) function (here the x and y arguments). By adding the geom_line() we insert a line.\n\nggplot(data = dat, \n       aes(x=date, y = price)) +\n  geom_line()\n\n\n\n\nWe could instead add geom_point()\n\n ggplot(data = dat,\n        aes(x=date, y = price)) +\n   geom_point()\n\n\n\n\nor do both\n\n ggplot(data = dat,\n        aes(x=date, y = price)) +\n   geom_line() +\n   geom_point()\n\n\n\n\nWe can change the colors and decrease the size of the points:\n\n ggplot(data = dat,\n        aes(x=date, y = price)) +\n   geom_line(color = \"blue\") +\n   geom_point(color = \"green\", size = .2)\n\n\n\n\nOr maybe we do not want to use the default theme: –&gt;\n\n ggplot(data = dat,\n        aes(x=date, y = price)) +\n   geom_line(color = \"blue\") +\n   geom_point(color = \"green\", size = .2) +\n   theme_bw()\n\n\n\n\nWe can also include the plotting in our data manipulation pipeline. For instance, lets summarize the data by year and plot the resulting yearly time series.\n\ndat %&gt;%\n  filter(between(year, 1988, 2021)) %&gt;%\n  group_by(year) %&gt;%\n  summarize(meanPrice = mean(price)) %&gt;%\n  # adding plotting to pipeline:\n  ggplot(aes(x=year, y = meanPrice)) +\n  geom_line(color = \"blue\") +\n  geom_point(color = \"green\") +\n  theme_bw()\n\n\n\n\n\n\n\nWe cannot illustrate all aspects here, but you will learn new elements by studying examples throughout the course. This recap is mostly for remembering the basics of data manipulation in R and simple plotting. As you will see in the continuation, the coding is not much more complex then what you have seen here and the fpp3 package uses the same type of logic and syntax as the tidyverse. There will however be some new functions specific for time series analysis that you will need to learn.\n\n\n\n\nSet working directory.\nLoad the data.\nFilter away observations prior to 2010.\nRemove columns except .. and time\nSummarize data to monthly means\nMake a plot with time on x-axis and monthly means on y-axis\nSave the figure to file."
  },
  {
    "objectID": "2_timeseriesgraphics.html",
    "href": "2_timeseriesgraphics.html",
    "title": "Time series basics",
    "section": "",
    "text": "In this chapter, we will cover some key concepts of time series that you need to know. You should build up an intuition of what a stationary time series is, and what autocorrelation is. White noise is a simplest time series process you can think of and the building block of ARIMA models. Here we try to give a short introduction to these concepts. Some of these have already been introduced in MET4 at NHH. So if you have taken that course, this may serve as a short repetition.\n\n\nBelow is a video from Aric LaBarr’s youtube series explaing time series concepts in less than 5 minutes. In the video below he explains what time series data is - as opposed to cross sectional data. As he points out, we also some times have a combination which we can e.g. aggregate to cross sectional data or time series data.\n\n\n\n\n\nIt is quite common in statistics in general, but also in time series analysis, to distinguish between capital and non-capital letters. While Y_t denotes the stochastic variable Y at time t, y_t is the realization of said stochastic variable Y_t, i.e. an observed value. Y_t is a specific variable, while \\{Y_t\\} is the entire time series. While a stochastic variable has for instance an expectation, a variance, a dependence structure and a distribution, an observation is just a number.\nLet \\mu_t=\\mu_Y(t)=\\mathrm{E}(Y_t) denote the mean function of Y_t. Assuming that E(Y_t^2)&lt;\\infty, the covariance function of \\{Y_t\\} is denoted \\gamma(r,s)=\\gamma_Y(r,s)= \\mathrm{Cov}(Y_r,Y_s)=\\mathrm{E}(Y_r-\\mu_r)(Y_s-\\mu_s). If the covariance function does not depend on the specific values of r and s, but rather the distance between them h = |r-s|, we write \\gamma(r,s)=\\gamma(h). Likewise, if \\mu_t does not depend on t, we write \\mu_t=\\mu. This will be used in the section about stationarity below. Note that the variance of Y_t is given by \\gamma(t,t) = \\mathrm{Cov}(Y_t,Y_t) = \\mathrm{Var}(Y_t) or \\gamma(0) in the case where the covariance does not depend on t.\n\n\n\nThe tsibble object will be important in this course. Since we are working in the tidyverse, things will be easier if we commit to it. Converting your data to a time series tibble (tsibble) is essential. The tsibble extends the tidy tibble data frame by introducing temporal structure. Read the chapter 2.1 about tsibble objects in the textbook. Here you find several coding examples.\n\n\n\nWe return to Labarr’s time series in 5 minutes series on youtube explaining stationarity. This is a key concept in time series. The video by Labarr gives a nice intuition for the concept. It separates between strong and weak stationarity. In this course, the weak stationarity condition will be the one we focus on, but it is nice to know that there are other definitions of stationarity as well. For a time series to be (weakly) stationary the mean and variance of the time series variable Y_t should not depend on t. More formally, a time series is weakly stationary if\n\nThe expectation is constant: \\mu_t = \\mu\nThe variance is constant and finite: \\sigma_t^2 = \\sigma^2 &lt; \\infty\nThe covariance between two lagged variables only depend on the lag: \\gamma(r,s) = \\gamma(h), where h=|r-s|.\n\n\n\nIn the video, Labarr shows some examples of non-stationary time series which can be transformed to stationary time series by differencing. We will come back to this later in the course, when we talk about transformations and again when we study ARIMA models. We will also study examples of stationary time / non-stationary time series in the voluntary homework. You can also read about the stationarity in the textbook, but for now, you may stop after the paragraph on stationarity. The book is less specific about their definition, but we will stick the weakly stationary definition above.\n\n\n\n\n\nA white noise series is a time series of uncorrelated observations with mean zero and finite variance. We will often write it as Z_t where Z_t \\sim \\mathrm{WN}(0,\\sigma^2). The standard is that the series is uncorrelated, but we may require it to be independent (stronger assumption) and very often normally distributed. In that case we call it iid Gaussian white noise (iid = independent and identically distributed). Notation for this may be Z_t \\sim \\text{iid}\\, \\mathrm{WN}(0,\\sigma^2). Let us generate a white noise series in R.\n\nlibrary(fpp3)\nset.seed(123) # To produce the same output\nwn &lt;- tsibble(\n  t = 1:100,\n  Z = rnorm(100, sd = 3), # Draws from N(0, 3^2) distribution\n  index = \"t\"\n) \nwn %&gt;% \n  autoplot() + \n  labs(title = \"Gaussian White noise\",\n       subtitle = \"iid WN(0,9)\")\n\nPlot variable not specified, automatically selected `.vars = Z`\n\n\n\n\n\nWe can then plot the acf of the series:\n\nwn %&gt;%\n  ACF(Z) %&gt;%\n  autoplot() + \n  labs(title=\"White noise ACF\")\n\n\n\n\nAs you can see, all the correlations fall within the confidence bands. The series is uncorrelated. You can also find a similar example in the textbook."
  },
  {
    "objectID": "2_timeseriesgraphics.html#what-is-time-series-data",
    "href": "2_timeseriesgraphics.html#what-is-time-series-data",
    "title": "Time series basics",
    "section": "",
    "text": "Below is a video from Aric LaBarr’s youtube series explaing time series concepts in less than 5 minutes. In the video below he explains what time series data is - as opposed to cross sectional data. As he points out, we also some times have a combination which we can e.g. aggregate to cross sectional data or time series data."
  },
  {
    "objectID": "2_timeseriesgraphics.html#time-series-notation",
    "href": "2_timeseriesgraphics.html#time-series-notation",
    "title": "Time series basics",
    "section": "",
    "text": "It is quite common in statistics in general, but also in time series analysis, to distinguish between capital and non-capital letters. While Y_t denotes the stochastic variable Y at time t, y_t is the realization of said stochastic variable Y_t, i.e. an observed value. Y_t is a specific variable, while \\{Y_t\\} is the entire time series. While a stochastic variable has for instance an expectation, a variance, a dependence structure and a distribution, an observation is just a number.\nLet \\mu_t=\\mu_Y(t)=\\mathrm{E}(Y_t) denote the mean function of Y_t. Assuming that E(Y_t^2)&lt;\\infty, the covariance function of \\{Y_t\\} is denoted \\gamma(r,s)=\\gamma_Y(r,s)= \\mathrm{Cov}(Y_r,Y_s)=\\mathrm{E}(Y_r-\\mu_r)(Y_s-\\mu_s). If the covariance function does not depend on the specific values of r and s, but rather the distance between them h = |r-s|, we write \\gamma(r,s)=\\gamma(h). Likewise, if \\mu_t does not depend on t, we write \\mu_t=\\mu. This will be used in the section about stationarity below. Note that the variance of Y_t is given by \\gamma(t,t) = \\mathrm{Cov}(Y_t,Y_t) = \\mathrm{Var}(Y_t) or \\gamma(0) in the case where the covariance does not depend on t."
  },
  {
    "objectID": "2_timeseriesgraphics.html#tsibble",
    "href": "2_timeseriesgraphics.html#tsibble",
    "title": "Time series basics",
    "section": "",
    "text": "The tsibble object will be important in this course. Since we are working in the tidyverse, things will be easier if we commit to it. Converting your data to a time series tibble (tsibble) is essential. The tsibble extends the tidy tibble data frame by introducing temporal structure. Read the chapter 2.1 about tsibble objects in the textbook. Here you find several coding examples."
  },
  {
    "objectID": "2_timeseriesgraphics.html#stationarity",
    "href": "2_timeseriesgraphics.html#stationarity",
    "title": "Time series basics",
    "section": "",
    "text": "We return to Labarr’s time series in 5 minutes series on youtube explaining stationarity. This is a key concept in time series. The video by Labarr gives a nice intuition for the concept. It separates between strong and weak stationarity. In this course, the weak stationarity condition will be the one we focus on, but it is nice to know that there are other definitions of stationarity as well. For a time series to be (weakly) stationary the mean and variance of the time series variable Y_t should not depend on t. More formally, a time series is weakly stationary if\n\nThe expectation is constant: \\mu_t = \\mu\nThe variance is constant and finite: \\sigma_t^2 = \\sigma^2 &lt; \\infty\nThe covariance between two lagged variables only depend on the lag: \\gamma(r,s) = \\gamma(h), where h=|r-s|.\n\n\n\nIn the video, Labarr shows some examples of non-stationary time series which can be transformed to stationary time series by differencing. We will come back to this later in the course, when we talk about transformations and again when we study ARIMA models. We will also study examples of stationary time / non-stationary time series in the voluntary homework. You can also read about the stationarity in the textbook, but for now, you may stop after the paragraph on stationarity. The book is less specific about their definition, but we will stick the weakly stationary definition above."
  },
  {
    "objectID": "2_timeseriesgraphics.html#white-noise",
    "href": "2_timeseriesgraphics.html#white-noise",
    "title": "Time series basics",
    "section": "",
    "text": "A white noise series is a time series of uncorrelated observations with mean zero and finite variance. We will often write it as Z_t where Z_t \\sim \\mathrm{WN}(0,\\sigma^2). The standard is that the series is uncorrelated, but we may require it to be independent (stronger assumption) and very often normally distributed. In that case we call it iid Gaussian white noise (iid = independent and identically distributed). Notation for this may be Z_t \\sim \\text{iid}\\, \\mathrm{WN}(0,\\sigma^2). Let us generate a white noise series in R.\n\nlibrary(fpp3)\nset.seed(123) # To produce the same output\nwn &lt;- tsibble(\n  t = 1:100,\n  Z = rnorm(100, sd = 3), # Draws from N(0, 3^2) distribution\n  index = \"t\"\n) \nwn %&gt;% \n  autoplot() + \n  labs(title = \"Gaussian White noise\",\n       subtitle = \"iid WN(0,9)\")\n\nPlot variable not specified, automatically selected `.vars = Z`\n\n\n\n\n\nWe can then plot the acf of the series:\n\nwn %&gt;%\n  ACF(Z) %&gt;%\n  autoplot() + \n  labs(title=\"White noise ACF\")\n\n\n\n\nAs you can see, all the correlations fall within the confidence bands. The series is uncorrelated. You can also find a similar example in the textbook."
  },
  {
    "objectID": "3_decomposition.html",
    "href": "3_decomposition.html",
    "title": "Decomposition",
    "section": "",
    "text": "In this chapter we will consider different adjusments and transformations one can do prior to a model task. Then we move on to techniques for decomposing a time series into a trend-cycle, season and remainder component.\n\n\n\n\nIn the example in the video above, we are not interested in a proxy for working days per month, and to avoid the effect of this we use the mean (average) production per working day within each month instead of total production per month. The code to generate the example can be found below:\n\n\nCode for generating examples:\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(fpp3)\n\n# ggplot theme:\ntheme_set(\n  theme_bw() + \n    theme(panel.grid.minor = element_blank(),\n          panel.grid.major = element_blank())\n)\n\n# Daily production: \ndat &lt;- tibble(\n  date = seq(as.Date(\"2015-01-01\"), as.Date(\"2019-12-31\"), by = \"1 day\"),\n  price = pi\n) %&gt;% \n  #Removing the weekends:\n  filter(wday(date, week_start = 1) %in% 1:5) %&gt;%  \n  #Note: We do not remove public holidays, and the worker never takes a day off\n  mutate(YearMonth = yearmonth(date))\n\n# -- TOTAL PRODUCTION FIGURE --\ndat %&gt;%\n  group_by(YearMonth) %&gt;%\n  summarize(`Total production` = sum(price)) %&gt;%\n  as_tsibble(index = \"YearMonth\") %&gt;% \n  ggplot(aes(x = YearMonth, \n             y = `Total production`)) + \n  geom_point(color = \"skyblue\") + \n  geom_line(color = \"skyblue\") + \n  scale_y_continuous(breaks = seq(60, 100, 5),\n                     labels = paste0(\"$\",seq(60, 100, 5),\"k\"), \n                     limits = c(60,75))\n# -- MEAN PRODUCTION FIGURE --\ndat %&gt;% group_by(YearMonth) %&gt;%\n  summarize(`Mean production` = mean(price)) %&gt;%\n  as_tsibble(index = \"YearMonth\") %&gt;% \n  ggplot(aes(x = YearMonth, \n             y = `Mean production`)) + \n  geom_point(color = \"skyblue\") + \n  geom_line(color=\"skyblue\")+ \n  scale_y_continuous(breaks = seq(3, 4, .02), \n                     labels = paste0(\"$\",seq(3, 4, .02),\"k\"))\n\n\n\n\n\nAdjusting for population size is usually a good idea when studying a quantity that is affected by it. The most obvious example is to study GDP (Gross Domestic Product) per capita instead of GDP.\n\n\n\n\nCode for generating examples in the video\n# --- Setting up the data --\nscandinaviaUSA &lt;- global_economy %&gt;% \n  filter(Country %in% c(\"Norway\",\"Sweden\",\"Denmark\", \"United States\"))\nscandinaviaUSA %&gt;% head()\n\n# --- GDP by country in $US --\nscandinaviaUSA %&gt;%\n  autoplot(GDP)+\n  labs(title= \"GDP\", y = \"$US\")\n\n# --- Population by country  --\nscandinaviaUSA %&gt;%\n  autoplot(Population)+\n  labs(title= \"Population\", y = \"Number of people\")\n\n# --- Population by country  (log-scale on y-axis) --\nscandinaviaUSA %&gt;% \n  autoplot(Population)+\n  scale_y_log10()+\n  labs(title= \"Population (log-scale)\",y = \"Number of people\")\n\n# --- GDP per capita by country ---\nscandinaviaUSA %&gt;%\n  autoplot(GDP/Population) +\n  labs(title= \"GDP per capita = GDP / Population\", y = \"$US\")\n\n\n\n\n\nAdjusting for inflation is a simple way of taking into account that 5$ in 1950 would get you much more than 5$ would today. This compensation is usually done by a consumer price index, which is standardized to a specific year (in the video below we show examples with 2010 and 2015 as reference years).\nLet Y_t denote the raw time series and Y_t^\\star the inflation adjusted. Let \\text{CPI}_t denote a relevant consumer price index defined to be 100 in the reference year t^\\star. Then Y_t^\\star = Y_t \\cdot \\frac{100}{\\text{CPI}_t}. More generally, we can choose the reference year t^\\star and write this as Y_t^\\star = Y_t \\cdot \\frac{\\text{CPI}_{t^\\star}}{\\text{CPI}_t}.\nThe inflation adjusted series is then measured in the unit “year t^\\star-money”.\n\n\n\n\nCode for generating examples:\n# --- Inflation adjusted GDP per capita by country ---\nscandinaviaUSA %&gt;%\n  autoplot(GDP/Population *100 / CPI) +\n  labs(title= \"GDP per capita = GDP / Population\", y = \"$US\")\n\n# --- CPI FOR NORWAY (data from Statistics Norway)---\nCPI &lt;- read.csv(\"data/CPI_norway.csv\", sep = \";\") %&gt;% as_tibble() %&gt;%\n  select(1:2) %&gt;%\n  rename(Year = X,CPI = Y.avg2) %&gt;% \n  mutate(Year = as.numeric(Year), CPI = as.numeric(CPI))%&gt;%\n  filter(Year &lt; 2022) %&gt;%\n  as_tsibble(index = Year)\n\n# --- CPI figure ---\nCPI %&gt;% \n  autoplot(CPI, color = \"blue\", lwd = 1.2) +\n  labs(title= \"Consumer Price Index\", y = \"NOK\",\n       subtitle = \"Data source: Statistics Norway\")+\n  geom_hline(yintercept = 100, lty = 2) + geom_vline(xintercept = 2015, lty = 2)+\n  scale_x_continuous(breaks = seq(1925,2025,10))+\n  scale_y_continuous(breaks = seq(0,120,10))\n\n# --- BIG MAC price index ---\nbigMac &lt;- read_csv(\"https://raw.githubusercontent.com/TheEconomist/big-mac-data/master/output-data/big-mac-raw-index.csv\")\nnorBigMac &lt;- bigMac %&gt;% \n  filter(name %in% c(\"Norway\")) %&gt;% \n  mutate(Year = lubridate::year(date)) %&gt;%\n  as_tsibble(index = \"date\")%&gt;%\n  filter(Year &lt;2022) %&gt;%\n  left_join(CPI, by = \"Year\") \n\n# --- BIG MAC price index figure ---\nnorBigMac %&gt;% \n  autoplot(local_price) +\n  labs(title= \"Big Mac price in Norway\", y = \"NOK\",\n       subtitle = \"Data source: The Economist\") +\n  geom_smooth(method = \"lm\", se=FALSE)\n\n#--- Inflation adjusted BIG MAC price index figure ---\nnorBigMac %&gt;%\n  mutate(cpiAdjusted =local_price / CPI * 100)  %&gt;%\n  as_tsibble(index = date) %&gt;%\n  autoplot(cpiAdjusted)+\n  labs(title= \"Inflation adjusted Big Mac price in Norway\", y = \"NOK (2015)\",\n       subtitle = \"Data sources: The Economist (big mac index), Statistics Norway (CPI)\") +\n  geom_smooth(method = \"lm\", se=FALSE)\n\n\n\n\n\nIn many situations it can be necessary to do a mathematical transformation of a time series. There can be different reasons for doing so, but a main one is to make it stationary (or at least more stationary). For instance, if you see that the variation increases or decreases with the level of the series. The most common transformation (for positive time series) is probably using the logarithm. It is often effective and interpretable as changes in the log value correspond to relative changes in the original scale. We write the transformed series, w_t, as w_t =\\log y_t, where y_t is the original time series.\nThe textbook also mentions power transformations of the form w_t = y_t^p (squarte roots - p=\\frac12, cube roots - p=\\frac13, etc). These are not as common to use, but there are situations where these may be better than the logarithm.\nA family of transformations (including log- and a class of power transformations) is the Box-Cox transform. For any value of \\lambda\\in \\mathbb R, \\begin{equation*}\nw_t = \\begin{cases}\n\\log(y_t),&\\text{if }\\lambda = 0;\\\\\n(y_t^\\lambda -1)/\\lambda, &\\text{otherwise}.\n\\end{cases}\n\\end{equation*} As you can see, if \\lambda = 0 we have a simple natural logarithm transform. This version of the Box-Cox transform is also defined for negative values of y_t as long as \\lambda &gt;0.\nThe book has a very nice shiny app for experimenting with different values of \\lambda on a time series of gas production in Australia. We have borrowed it below, but you find it also here. They write that: ” A good value of \\lambda is one which makes the size of the seasonal variation about the same across the whole series, as that makes the forecasting model simpler.” This pretty much sums up why one does mathematical transformations as a preprocessing step before fitting a model - it makes the model simpler.\n\n\nFor financial assets, such as stocks, it is often better to consider the returns rather than the price series. This is also a mathematical transformation and involves differencing. First order differencing means subtracting the previous observation from the present, i.e. w_t=y_t-y_{t-1}. Taking differences is an effective way of potentially making a non-stationary time series stationary. E.g. if a time series has a linear trend: Y_t = \\alpha t + Z_t, where \\alpha is a real constant and Z_t is a white noise, we get that W_t = Y_t-Y_{t-1} = \\alpha t + Z_t - \\alpha(t-1) - Z_{t-1} = Z_t-Z_{t-1} + \\alpha, effectively removing the trend in the transformed series. We will return to this when considering ARIMA models.\nThere are different definitions of returns, but the most common ones are the standard returns, r_t, and log-returns, \\textrm{lr}_t, defined respectively by \\begin{equation*}\n\\begin{split}\nr_t &= \\frac{y_t-y_{t-1}}{y_{t-1}},\\\\\n\\textrm{lr}_t &= \\log y_t-\\log y_{t-1} = \\log\\frac{y_t}{y_{t-1}}.\n\\end{split}\n\\end{equation*} A daily return series for a stock usually has expectation close to zero and little autocorrelation, which can be convenient in many situations. However, they are typically hetereoskedastic (non-constant variance) and the squared returns are often autocorrelated. We will come back to this, when discussing volatility forecasting towards the end of the course.\n\n\nCode on mathematical transformations\n\n\n# Package for downloading stock data (primarily from Yahoo! Finance)\nlibrary(quantmod)\n\n# -- Download the data: --\ngetSymbols(\"AAPL\")\n\n[1] \"AAPL\"\n\n# -- Extract the closing price and create a tsibble: --\nclose.price &lt;- tibble(\n  close = as.numeric(AAPL$AAPL.Close),\n  date  = time(AAPL)\n) %&gt;% as_tsibble(index = date) \n\n# -- Plot closing price: --\nclose.price %&gt;% autoplot(close) +\n  labs(title = \"Apple Closing price\", \n       y     = \"US$\")\n\n\n\n# -- Adding transformations : -- \nclose.price &lt;- close.price %&gt;% \n  mutate(logclose  = log(close), # log-transform\n         logreturn = c(NA, diff(logclose)), # log returns\n         return    = c(NA, diff(close)/close[-nrow(close.price)]) # Returns\n         ) \n\n\n# -- Box-cox-transform --\nlambda &lt;- close.price %&gt;%\n  features(close, features = guerrero) %&gt;%\n  pull(lambda_guerrero)\nclose.price &lt;- close.price %&gt;%\n  mutate(boxcox = box_cox(close,lambda))\nclose.price %&gt;%\n  autoplot(boxcox) +\n  labs(y = \"\",\n       title = latex2exp::TeX(paste0(\n         \"Transformed apple closing price with $\\\\lambda$ = \",\n         round(lambda,3))))+\n  # Adding red curve with log-transform\n  geom_line(aes(y=logclose), col = 2) \n\n\n\n# -- Plotting the different transformations --\nclose.price %&gt;%\n  pivot_longer(-date) %&gt;% \n  autoplot(value) + \n  facet_wrap(~name, scales=\"free_y\", strip.position = \"left\")+\n  labs(title = \"Apple Closing Price transformations\") +\n  theme(strip.placement = \"outside\")\n\n\n\n\n\n\n\n\n\n\n\n\nCode on mathematical transformations\n\n\nset.seed(1344)\nlibrary(tidyverse)\nlibrary(fpp3)\ndat &lt;- tibble(\n  date = seq(as.Date(\"2015-01-01\"), as.Date(\"2020-12-31\"), by = \"1 day\"),\n  t = 1:length(date),\n  Tt = 100+9*t/300 + .5*((t-1000)/200)^3+(.87*cos(2*pi*t/1200)+.42*sin(2*pi*t/600))*11,\n  St = (.94*cos(2*pi*t/365) -1.2*sin(2* pi*t/365))*13,\n  Rt = rnorm(length(date), sd = 15),\n  Yt = Tt + St +  Rt\n) \n\n# -- Plotting Y and its components --\ndat %&gt;%\n  pivot_longer(cols = -c(date,t)) %&gt;% \n  mutate(name = factor(name, levels = c(\"Yt\", \"Tt\", \"St\", \"Rt\"))) %&gt;% # to order the panels\n  ggplot(aes(x=date, y = value, col = name)) + \n  geom_line() + facet_wrap(~name, ncol = 1, scales = \"free_y\", strip.position = \"left\") +\n  theme(strip.placement = \"outside\", axis.title = element_blank(), legend.position = \"none\")\n\n\n\n# -- Plotting seasonally adjusted Y --\nggplot(dat, aes(x = date, y = Yt-St)) + \n  geom_line() +\n  labs(title = \"Seasonally adjusted\", x = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\nlibrary(fpp3)\nlibrary(tidyverse)\n\n# ggplot theme: \ntheme_set(theme_bw() + \n            theme(panel.grid.major = element_blank(),\n                  panel.grid.minor = element_blank()))\n\n# -- Read in data: --\ndat &lt;- readxl::read_excel(\n  \"data/NorwayEmployment_15-74years_bySex.xlsx\") %&gt;%\n  as_tibble() %&gt;%\n  mutate(Quarter = str_replace(Quarter, \"K\",\"Q\"),\n         Quarter = yearquarter(Quarter)) \nnames(dat)[3] &lt;- \"Employed\"\n\n# -- Aggregating from Employed by sex to total --\ndat &lt;- dat %&gt;% \n  group_by(Quarter) %&gt;% \n  summarize(Employed = sum(Employed)) %&gt;%\n  as_tsibble(index = Quarter) # Time series table\ndat %&gt;% \n  autoplot(Employed, lwd = 1, colour = \"blue\")\n\n\n\ndat &lt;- dat %&gt;%\n  mutate(\n    `12-MA` = slider::slide_dbl(Employed, mean,\n                                .before = 5, .after = 6, .complete = TRUE),\n    `2x12-MA` = slider::slide_dbl(`12-MA`, mean,\n                                  .before = 1, .after = 0, .complete = TRUE)\n  )\ndat %&gt;%\n  ggplot(aes(x=Quarter, y =Employed))+\n  geom_line(colour = \"gray\") +\n  geom_line(aes(y = `2x12-MA`), colour = \"#D55E00\") +\n  theme_bw()+\n  labs(y = \"Persons (thousands)\",\n       title = \"Total employment in US retail\")\n\nWarning: Removed 12 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\ndat %&gt;% \n  model(\n    classical_decomposition(Employed, type = \"additive\")\n  ) %&gt;%\n  components() %&gt;%\n  autoplot()\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\ndat %&gt;% \n  model(\n    classical = classical_decomposition(Employed, \n                                        type = \"multiplicative\"),\n    x11 = X_13ARIMA_SEATS(Employed ~ x11()),\n    seats = X_13ARIMA_SEATS(Employed ~ seats())\n  ) %&gt;%\n  components() %&gt;% \n  mutate(random = ifelse(.model == \"classical\", \n                         random, \n                         irregular)) %&gt;%\n  autoplot(lwd = 1)\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\ndat %&gt;%\n  model(\n    STL0 = STL(Employed),\n    STL1 = STL(Employed ~ trend(window = 5) + # default 7\n                 season(window = 19),         # default 11\n               robust = FALSE)\n  ) %&gt;%\n  components() %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n\n\nUse the global_economy data, select a country (e.g. Austria). Plot GDP, GDP per capita and GDP per capita inflation adjusted, GDP inflation adjusted.\n\n\n\nSolution\n\n\nlibrary(fpp3)\nlibrary(tidyverse)\ndat &lt;- global_economy %&gt;% \n  filter(Country == \"Austria\")\ndat %&gt;% autoplot(GDP)\n\n\n\ndat %&gt;% autoplot(GDP/Population)\n\n\n\ndat %&gt;% autoplot(GDP/Population * 100/CPI)\n\n\n\ndat %&gt;% autoplot(GDP * 100/CPI)\n\n\n\n\n\n\nIn the global_economy data set, the CPI has a reference year of 2010. Do the necessary changes to inflation adjust GDP per capita with 1990 as reference year.\n\n\n\nSolution\n\n\nlibrary(fpp3)\nlibrary(tidyverse)\ndat &lt;- global_economy %&gt;% \n  filter(Country == \"Austria\")\n\n# -- Extracting the CPI in 1990: --\ncpi1990 &lt;- dat %&gt;% filter(Year ==1990) %&gt;% pull(CPI)\n\n# -- Transforming such that CPI1990 is 100 in 1990: --\ndat &lt;- dat %&gt;% mutate(CPI1990 = CPI / cpi1990 * 100)\n\n# -- Plotting Inflation adjusted GDP per capita: --\ndat %&gt;% autoplot(GDP/Population * 100/CPI1990) + \n  labs(y = \"Inflation adjusted GDP per capita (1990 US$)\") +\n  geom_line(aes(y= GDP/Population * 100/CPI), col = 2)\n\n\n\n# -- Comparing the two CPIs: --\ndat %&gt;% \n  pivot_longer(cols = c(CPI,CPI1990)) %&gt;%\n  ggplot(aes(x = Year, y = value, col = name)) + geom_line()+\n  geom_hline(yintercept = 100) +\n  scale_color_manual(values = c(\"red\",\"blue\"))+\n  geom_segment(x = 1990,xend = 1990, y = -Inf, yend =100, lty = 2, col = \"blue\")+\n  geom_segment(x = 2010,xend = 2010, y = -Inf, yend =100, lty = 2, col = \"red\")+\n  labs(title = \"Differences between CPI with reference year 1990 and 2010\",\n       y = \"Consumer Price Index\") +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\nUse the quantmod package to download data for another stock. What transformation would you prefer for this stock?\nUse the guerrero feature to select a \\lambda for the Box-Cox transformation on the data from the previous exercise."
  },
  {
    "objectID": "3_decomposition.html#calender-adjustments",
    "href": "3_decomposition.html#calender-adjustments",
    "title": "Decomposition",
    "section": "",
    "text": "In the example in the video above, we are not interested in a proxy for working days per month, and to avoid the effect of this we use the mean (average) production per working day within each month instead of total production per month. The code to generate the example can be found below:\n\n\nCode for generating examples:\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(fpp3)\n\n# ggplot theme:\ntheme_set(\n  theme_bw() + \n    theme(panel.grid.minor = element_blank(),\n          panel.grid.major = element_blank())\n)\n\n# Daily production: \ndat &lt;- tibble(\n  date = seq(as.Date(\"2015-01-01\"), as.Date(\"2019-12-31\"), by = \"1 day\"),\n  price = pi\n) %&gt;% \n  #Removing the weekends:\n  filter(wday(date, week_start = 1) %in% 1:5) %&gt;%  \n  #Note: We do not remove public holidays, and the worker never takes a day off\n  mutate(YearMonth = yearmonth(date))\n\n# -- TOTAL PRODUCTION FIGURE --\ndat %&gt;%\n  group_by(YearMonth) %&gt;%\n  summarize(`Total production` = sum(price)) %&gt;%\n  as_tsibble(index = \"YearMonth\") %&gt;% \n  ggplot(aes(x = YearMonth, \n             y = `Total production`)) + \n  geom_point(color = \"skyblue\") + \n  geom_line(color = \"skyblue\") + \n  scale_y_continuous(breaks = seq(60, 100, 5),\n                     labels = paste0(\"$\",seq(60, 100, 5),\"k\"), \n                     limits = c(60,75))\n# -- MEAN PRODUCTION FIGURE --\ndat %&gt;% group_by(YearMonth) %&gt;%\n  summarize(`Mean production` = mean(price)) %&gt;%\n  as_tsibble(index = \"YearMonth\") %&gt;% \n  ggplot(aes(x = YearMonth, \n             y = `Mean production`)) + \n  geom_point(color = \"skyblue\") + \n  geom_line(color=\"skyblue\")+ \n  scale_y_continuous(breaks = seq(3, 4, .02), \n                     labels = paste0(\"$\",seq(3, 4, .02),\"k\"))"
  },
  {
    "objectID": "3_decomposition.html#population-adjustment",
    "href": "3_decomposition.html#population-adjustment",
    "title": "Decomposition",
    "section": "",
    "text": "Adjusting for population size is usually a good idea when studying a quantity that is affected by it. The most obvious example is to study GDP (Gross Domestic Product) per capita instead of GDP.\n\n\n\n\nCode for generating examples in the video\n# --- Setting up the data --\nscandinaviaUSA &lt;- global_economy %&gt;% \n  filter(Country %in% c(\"Norway\",\"Sweden\",\"Denmark\", \"United States\"))\nscandinaviaUSA %&gt;% head()\n\n# --- GDP by country in $US --\nscandinaviaUSA %&gt;%\n  autoplot(GDP)+\n  labs(title= \"GDP\", y = \"$US\")\n\n# --- Population by country  --\nscandinaviaUSA %&gt;%\n  autoplot(Population)+\n  labs(title= \"Population\", y = \"Number of people\")\n\n# --- Population by country  (log-scale on y-axis) --\nscandinaviaUSA %&gt;% \n  autoplot(Population)+\n  scale_y_log10()+\n  labs(title= \"Population (log-scale)\",y = \"Number of people\")\n\n# --- GDP per capita by country ---\nscandinaviaUSA %&gt;%\n  autoplot(GDP/Population) +\n  labs(title= \"GDP per capita = GDP / Population\", y = \"$US\")"
  },
  {
    "objectID": "3_decomposition.html#inflation-adjustment",
    "href": "3_decomposition.html#inflation-adjustment",
    "title": "Decomposition",
    "section": "",
    "text": "Adjusting for inflation is a simple way of taking into account that 5$ in 1950 would get you much more than 5$ would today. This compensation is usually done by a consumer price index, which is standardized to a specific year (in the video below we show examples with 2010 and 2015 as reference years).\nLet Y_t denote the raw time series and Y_t^\\star the inflation adjusted. Let \\text{CPI}_t denote a relevant consumer price index defined to be 100 in the reference year t^\\star. Then Y_t^\\star = Y_t \\cdot \\frac{100}{\\text{CPI}_t}. More generally, we can choose the reference year t^\\star and write this as Y_t^\\star = Y_t \\cdot \\frac{\\text{CPI}_{t^\\star}}{\\text{CPI}_t}.\nThe inflation adjusted series is then measured in the unit “year t^\\star-money”.\n\n\n\n\nCode for generating examples:\n# --- Inflation adjusted GDP per capita by country ---\nscandinaviaUSA %&gt;%\n  autoplot(GDP/Population *100 / CPI) +\n  labs(title= \"GDP per capita = GDP / Population\", y = \"$US\")\n\n# --- CPI FOR NORWAY (data from Statistics Norway)---\nCPI &lt;- read.csv(\"data/CPI_norway.csv\", sep = \";\") %&gt;% as_tibble() %&gt;%\n  select(1:2) %&gt;%\n  rename(Year = X,CPI = Y.avg2) %&gt;% \n  mutate(Year = as.numeric(Year), CPI = as.numeric(CPI))%&gt;%\n  filter(Year &lt; 2022) %&gt;%\n  as_tsibble(index = Year)\n\n# --- CPI figure ---\nCPI %&gt;% \n  autoplot(CPI, color = \"blue\", lwd = 1.2) +\n  labs(title= \"Consumer Price Index\", y = \"NOK\",\n       subtitle = \"Data source: Statistics Norway\")+\n  geom_hline(yintercept = 100, lty = 2) + geom_vline(xintercept = 2015, lty = 2)+\n  scale_x_continuous(breaks = seq(1925,2025,10))+\n  scale_y_continuous(breaks = seq(0,120,10))\n\n# --- BIG MAC price index ---\nbigMac &lt;- read_csv(\"https://raw.githubusercontent.com/TheEconomist/big-mac-data/master/output-data/big-mac-raw-index.csv\")\nnorBigMac &lt;- bigMac %&gt;% \n  filter(name %in% c(\"Norway\")) %&gt;% \n  mutate(Year = lubridate::year(date)) %&gt;%\n  as_tsibble(index = \"date\")%&gt;%\n  filter(Year &lt;2022) %&gt;%\n  left_join(CPI, by = \"Year\") \n\n# --- BIG MAC price index figure ---\nnorBigMac %&gt;% \n  autoplot(local_price) +\n  labs(title= \"Big Mac price in Norway\", y = \"NOK\",\n       subtitle = \"Data source: The Economist\") +\n  geom_smooth(method = \"lm\", se=FALSE)\n\n#--- Inflation adjusted BIG MAC price index figure ---\nnorBigMac %&gt;%\n  mutate(cpiAdjusted =local_price / CPI * 100)  %&gt;%\n  as_tsibble(index = date) %&gt;%\n  autoplot(cpiAdjusted)+\n  labs(title= \"Inflation adjusted Big Mac price in Norway\", y = \"NOK (2015)\",\n       subtitle = \"Data sources: The Economist (big mac index), Statistics Norway (CPI)\") +\n  geom_smooth(method = \"lm\", se=FALSE)"
  },
  {
    "objectID": "3_decomposition.html#mathematical-transformations",
    "href": "3_decomposition.html#mathematical-transformations",
    "title": "Decomposition",
    "section": "",
    "text": "In many situations it can be necessary to do a mathematical transformation of a time series. There can be different reasons for doing so, but a main one is to make it stationary (or at least more stationary). For instance, if you see that the variation increases or decreases with the level of the series. The most common transformation (for positive time series) is probably using the logarithm. It is often effective and interpretable as changes in the log value correspond to relative changes in the original scale. We write the transformed series, w_t, as w_t =\\log y_t, where y_t is the original time series.\nThe textbook also mentions power transformations of the form w_t = y_t^p (squarte roots - p=\\frac12, cube roots - p=\\frac13, etc). These are not as common to use, but there are situations where these may be better than the logarithm.\nA family of transformations (including log- and a class of power transformations) is the Box-Cox transform. For any value of \\lambda\\in \\mathbb R, \\begin{equation*}\nw_t = \\begin{cases}\n\\log(y_t),&\\text{if }\\lambda = 0;\\\\\n(y_t^\\lambda -1)/\\lambda, &\\text{otherwise}.\n\\end{cases}\n\\end{equation*} As you can see, if \\lambda = 0 we have a simple natural logarithm transform. This version of the Box-Cox transform is also defined for negative values of y_t as long as \\lambda &gt;0.\nThe book has a very nice shiny app for experimenting with different values of \\lambda on a time series of gas production in Australia. We have borrowed it below, but you find it also here. They write that: ” A good value of \\lambda is one which makes the size of the seasonal variation about the same across the whole series, as that makes the forecasting model simpler.” This pretty much sums up why one does mathematical transformations as a preprocessing step before fitting a model - it makes the model simpler.\n\n\nFor financial assets, such as stocks, it is often better to consider the returns rather than the price series. This is also a mathematical transformation and involves differencing. First order differencing means subtracting the previous observation from the present, i.e. w_t=y_t-y_{t-1}. Taking differences is an effective way of potentially making a non-stationary time series stationary. E.g. if a time series has a linear trend: Y_t = \\alpha t + Z_t, where \\alpha is a real constant and Z_t is a white noise, we get that W_t = Y_t-Y_{t-1} = \\alpha t + Z_t - \\alpha(t-1) - Z_{t-1} = Z_t-Z_{t-1} + \\alpha, effectively removing the trend in the transformed series. We will return to this when considering ARIMA models.\nThere are different definitions of returns, but the most common ones are the standard returns, r_t, and log-returns, \\textrm{lr}_t, defined respectively by \\begin{equation*}\n\\begin{split}\nr_t &= \\frac{y_t-y_{t-1}}{y_{t-1}},\\\\\n\\textrm{lr}_t &= \\log y_t-\\log y_{t-1} = \\log\\frac{y_t}{y_{t-1}}.\n\\end{split}\n\\end{equation*} A daily return series for a stock usually has expectation close to zero and little autocorrelation, which can be convenient in many situations. However, they are typically hetereoskedastic (non-constant variance) and the squared returns are often autocorrelated. We will come back to this, when discussing volatility forecasting towards the end of the course.\n\n\nCode on mathematical transformations\n\n\n# Package for downloading stock data (primarily from Yahoo! Finance)\nlibrary(quantmod)\n\n# -- Download the data: --\ngetSymbols(\"AAPL\")\n\n[1] \"AAPL\"\n\n# -- Extract the closing price and create a tsibble: --\nclose.price &lt;- tibble(\n  close = as.numeric(AAPL$AAPL.Close),\n  date  = time(AAPL)\n) %&gt;% as_tsibble(index = date) \n\n# -- Plot closing price: --\nclose.price %&gt;% autoplot(close) +\n  labs(title = \"Apple Closing price\", \n       y     = \"US$\")\n\n\n\n# -- Adding transformations : -- \nclose.price &lt;- close.price %&gt;% \n  mutate(logclose  = log(close), # log-transform\n         logreturn = c(NA, diff(logclose)), # log returns\n         return    = c(NA, diff(close)/close[-nrow(close.price)]) # Returns\n         ) \n\n\n# -- Box-cox-transform --\nlambda &lt;- close.price %&gt;%\n  features(close, features = guerrero) %&gt;%\n  pull(lambda_guerrero)\nclose.price &lt;- close.price %&gt;%\n  mutate(boxcox = box_cox(close,lambda))\nclose.price %&gt;%\n  autoplot(boxcox) +\n  labs(y = \"\",\n       title = latex2exp::TeX(paste0(\n         \"Transformed apple closing price with $\\\\lambda$ = \",\n         round(lambda,3))))+\n  # Adding red curve with log-transform\n  geom_line(aes(y=logclose), col = 2) \n\n\n\n# -- Plotting the different transformations --\nclose.price %&gt;%\n  pivot_longer(-date) %&gt;% \n  autoplot(value) + \n  facet_wrap(~name, scales=\"free_y\", strip.position = \"left\")+\n  labs(title = \"Apple Closing Price transformations\") +\n  theme(strip.placement = \"outside\")"
  },
  {
    "objectID": "3_decomposition.html#time-series-components-and-seasonal-adjustment",
    "href": "3_decomposition.html#time-series-components-and-seasonal-adjustment",
    "title": "Decomposition",
    "section": "",
    "text": "Code on mathematical transformations\n\n\nset.seed(1344)\nlibrary(tidyverse)\nlibrary(fpp3)\ndat &lt;- tibble(\n  date = seq(as.Date(\"2015-01-01\"), as.Date(\"2020-12-31\"), by = \"1 day\"),\n  t = 1:length(date),\n  Tt = 100+9*t/300 + .5*((t-1000)/200)^3+(.87*cos(2*pi*t/1200)+.42*sin(2*pi*t/600))*11,\n  St = (.94*cos(2*pi*t/365) -1.2*sin(2* pi*t/365))*13,\n  Rt = rnorm(length(date), sd = 15),\n  Yt = Tt + St +  Rt\n) \n\n# -- Plotting Y and its components --\ndat %&gt;%\n  pivot_longer(cols = -c(date,t)) %&gt;% \n  mutate(name = factor(name, levels = c(\"Yt\", \"Tt\", \"St\", \"Rt\"))) %&gt;% # to order the panels\n  ggplot(aes(x=date, y = value, col = name)) + \n  geom_line() + facet_wrap(~name, ncol = 1, scales = \"free_y\", strip.position = \"left\") +\n  theme(strip.placement = \"outside\", axis.title = element_blank(), legend.position = \"none\")\n\n\n\n# -- Plotting seasonally adjusted Y --\nggplot(dat, aes(x = date, y = Yt-St)) + \n  geom_line() +\n  labs(title = \"Seasonally adjusted\", x = \"\")"
  },
  {
    "objectID": "3_decomposition.html#moving-averages",
    "href": "3_decomposition.html#moving-averages",
    "title": "Decomposition",
    "section": "",
    "text": "Code\n\n\nlibrary(fpp3)\nlibrary(tidyverse)\n\n# ggplot theme: \ntheme_set(theme_bw() + \n            theme(panel.grid.major = element_blank(),\n                  panel.grid.minor = element_blank()))\n\n# -- Read in data: --\ndat &lt;- readxl::read_excel(\n  \"data/NorwayEmployment_15-74years_bySex.xlsx\") %&gt;%\n  as_tibble() %&gt;%\n  mutate(Quarter = str_replace(Quarter, \"K\",\"Q\"),\n         Quarter = yearquarter(Quarter)) \nnames(dat)[3] &lt;- \"Employed\"\n\n# -- Aggregating from Employed by sex to total --\ndat &lt;- dat %&gt;% \n  group_by(Quarter) %&gt;% \n  summarize(Employed = sum(Employed)) %&gt;%\n  as_tsibble(index = Quarter) # Time series table\ndat %&gt;% \n  autoplot(Employed, lwd = 1, colour = \"blue\")\n\n\n\ndat &lt;- dat %&gt;%\n  mutate(\n    `12-MA` = slider::slide_dbl(Employed, mean,\n                                .before = 5, .after = 6, .complete = TRUE),\n    `2x12-MA` = slider::slide_dbl(`12-MA`, mean,\n                                  .before = 1, .after = 0, .complete = TRUE)\n  )\ndat %&gt;%\n  ggplot(aes(x=Quarter, y =Employed))+\n  geom_line(colour = \"gray\") +\n  geom_line(aes(y = `2x12-MA`), colour = \"#D55E00\") +\n  theme_bw()+\n  labs(y = \"Persons (thousands)\",\n       title = \"Total employment in US retail\")\n\nWarning: Removed 12 rows containing missing values (`geom_line()`)."
  },
  {
    "objectID": "3_decomposition.html#classical-decomposition",
    "href": "3_decomposition.html#classical-decomposition",
    "title": "Decomposition",
    "section": "",
    "text": "Code\n\n\ndat %&gt;% \n  model(\n    classical_decomposition(Employed, type = \"additive\")\n  ) %&gt;%\n  components() %&gt;%\n  autoplot()\n\nWarning: Removed 2 rows containing missing values (`geom_line()`)."
  },
  {
    "objectID": "3_decomposition.html#statistics-agencies-x11-and-seats",
    "href": "3_decomposition.html#statistics-agencies-x11-and-seats",
    "title": "Decomposition",
    "section": "",
    "text": "Code\n\n\ndat %&gt;% \n  model(\n    classical = classical_decomposition(Employed, \n                                        type = \"multiplicative\"),\n    x11 = X_13ARIMA_SEATS(Employed ~ x11()),\n    seats = X_13ARIMA_SEATS(Employed ~ seats())\n  ) %&gt;%\n  components() %&gt;% \n  mutate(random = ifelse(.model == \"classical\", \n                         random, \n                         irregular)) %&gt;%\n  autoplot(lwd = 1)\n\nWarning: Removed 2 rows containing missing values (`geom_line()`)."
  },
  {
    "objectID": "3_decomposition.html#stl-seasonal-and-trend-decomposition-using-loess",
    "href": "3_decomposition.html#stl-seasonal-and-trend-decomposition-using-loess",
    "title": "Decomposition",
    "section": "",
    "text": "Code\n\n\ndat %&gt;%\n  model(\n    STL0 = STL(Employed),\n    STL1 = STL(Employed ~ trend(window = 5) + # default 7\n                 season(window = 19),         # default 11\n               robust = FALSE)\n  ) %&gt;%\n  components() %&gt;%\n  autoplot()"
  },
  {
    "objectID": "3_decomposition.html#exercises",
    "href": "3_decomposition.html#exercises",
    "title": "Decomposition",
    "section": "",
    "text": "Use the global_economy data, select a country (e.g. Austria). Plot GDP, GDP per capita and GDP per capita inflation adjusted, GDP inflation adjusted.\n\n\n\nSolution\n\n\nlibrary(fpp3)\nlibrary(tidyverse)\ndat &lt;- global_economy %&gt;% \n  filter(Country == \"Austria\")\ndat %&gt;% autoplot(GDP)\n\n\n\ndat %&gt;% autoplot(GDP/Population)\n\n\n\ndat %&gt;% autoplot(GDP/Population * 100/CPI)\n\n\n\ndat %&gt;% autoplot(GDP * 100/CPI)\n\n\n\n\n\n\nIn the global_economy data set, the CPI has a reference year of 2010. Do the necessary changes to inflation adjust GDP per capita with 1990 as reference year.\n\n\n\nSolution\n\n\nlibrary(fpp3)\nlibrary(tidyverse)\ndat &lt;- global_economy %&gt;% \n  filter(Country == \"Austria\")\n\n# -- Extracting the CPI in 1990: --\ncpi1990 &lt;- dat %&gt;% filter(Year ==1990) %&gt;% pull(CPI)\n\n# -- Transforming such that CPI1990 is 100 in 1990: --\ndat &lt;- dat %&gt;% mutate(CPI1990 = CPI / cpi1990 * 100)\n\n# -- Plotting Inflation adjusted GDP per capita: --\ndat %&gt;% autoplot(GDP/Population * 100/CPI1990) + \n  labs(y = \"Inflation adjusted GDP per capita (1990 US$)\") +\n  geom_line(aes(y= GDP/Population * 100/CPI), col = 2)\n\n\n\n# -- Comparing the two CPIs: --\ndat %&gt;% \n  pivot_longer(cols = c(CPI,CPI1990)) %&gt;%\n  ggplot(aes(x = Year, y = value, col = name)) + geom_line()+\n  geom_hline(yintercept = 100) +\n  scale_color_manual(values = c(\"red\",\"blue\"))+\n  geom_segment(x = 1990,xend = 1990, y = -Inf, yend =100, lty = 2, col = \"blue\")+\n  geom_segment(x = 2010,xend = 2010, y = -Inf, yend =100, lty = 2, col = \"red\")+\n  labs(title = \"Differences between CPI with reference year 1990 and 2010\",\n       y = \"Consumer Price Index\") +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\nUse the quantmod package to download data for another stock. What transformation would you prefer for this stock?\nUse the guerrero feature to select a \\lambda for the Box-Cox transformation on the data from the previous exercise."
  },
  {
    "objectID": "3_exercises.html",
    "href": "3_exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Exercises\nIn addition to the exercises in Chapter 3.7 of the textbook, here are some additional ones with solution. Try to solve them yourself before looking at the solution. The textbook exercise solutions are published on canvas.\n\nUse the simulated data from here. Compare different methods with the truth from the simulation. Are there large differences?\n\n\n\nSolution\n\nSince the simulated data is daily, we cannot use the x11 and seats methods. We will therefore only compare the STL and classical decomposition.\nWe start by re-running the simulation and creating a tsibble.\n\nset.seed(1344)\nlibrary(fpp3)\ntheme_set(theme_bw())\ndat &lt;- tibble(\n  date = seq(as.Date(\"2015-01-01\"), as.Date(\"2020-12-31\"), by = \"1 day\"),\n  t = 1:length(date),\n  Tt = 100+9*t/300 + .5*((t-1000)/200)^3+(.87*cos(2*pi*t/1200)+.42*sin(2*pi*t/600))*11,\n  St = (.94*cos(2*pi*t/365) -1.2*sin(2* pi*t/365))*13,\n  Rt = rnorm(length(date), sd = 15),\n  Yt = Tt + St +  Rt\n) \n\ndat &lt;- as_tsibble(dat, index = date)\n\nThen we do the decomposition, classical and STL:\n\ndecomp &lt;- dat %&gt;% \n  model(\n    classic = classical_decomposition(Yt~season(period = \"1 year\"), type = \"additive\"),\n    stl = STL(Yt~season(period = \"1 year\"), robust = TRUE)\n) %&gt;% components() %&gt;%\n  # For some reason, the classic and stl uses differnt naming conventions:\n  mutate(random = ifelse(is.na(remainder), random, remainder),\n          seasonal = ifelse(is.na(`season_1 year`), seasonal, `season_1 year`))\n# Adjusting the names of dat to fit with the decomp\nnames(dat) &lt;- c(\"date\", \"t\", \"trend\", \"seasonal\", \"random\", \"Yt\")\ndat$.model = \"simulation\"\n# Classical decomposition: \ndecomp %&gt;% filter(.model == \"classic\") %&gt;% autoplot()+ labs(title= \"Classical\")\n\nWarning: Removed 182 rows containing missing values (`geom_line()`).\n\n\n\n\n# STL: \ndecomp %&gt;% filter(.model == \"stl\") %&gt;% autoplot() + labs(title= \"STL\")\n\n\n\n# Simulation: \ndecomp %&gt;% bind_rows(dat) %&gt;% filter(.model == \"simulation\") %&gt;% autoplot()\n\n\n\n# All\ndecomp %&gt;% bind_rows(dat) %&gt;% autoplot()\n\nWarning: Removed 182 rows containing missing values (`geom_line()`).\n\n\n\n\n\nWith the setting I used here, it does not seem that the decomposition methods manage to get as smooth estimate for the seasonal component as we see in the simulation, but for the trend part the fit is almost perfect.\n\n\nUse the global_economy data, select a country (e.g. Austria). Plot GDP, GDP per capita and GDP per capita inflation adjusted, GDP inflation adjusted.\n\n\n\nSolution\n\n\nlibrary(fpp3)\nlibrary(tidyverse)\ndat &lt;- global_economy %&gt;% \n  filter(Country == \"Austria\")\ndat %&gt;% autoplot(GDP)\n\n\n\ndat %&gt;% autoplot(GDP/Population)\n\n\n\ndat %&gt;% autoplot(GDP/Population * 100/CPI)\n\n\n\ndat %&gt;% autoplot(GDP * 100/CPI)\n\n\n\n\n\n\nIn the global_economy data set, the CPI has a reference year of 2010. Do the necessary changes to inflation adjust GDP per capita with 1990 as reference year.\n\n\n\nSolution\n\n\nlibrary(fpp3)\nlibrary(tidyverse)\ndat &lt;- global_economy %&gt;% \n  filter(Country == \"Austria\")\n\n# -- Extracting the CPI in 1990: --\ncpi1990 &lt;- dat %&gt;% filter(Year ==1990) %&gt;% pull(CPI)\n\n# -- Transforming such that CPI1990 is 100 in 1990: --\ndat &lt;- dat %&gt;% mutate(CPI1990 = CPI / cpi1990 * 100)\n\n# -- Plotting Inflation adjusted GDP per capita: --\ndat %&gt;% autoplot(GDP/Population * 100/CPI1990) + \n  labs(y = \"Inflation adjusted GDP per capita (1990 US$)\") +\n  geom_line(aes(y= GDP/Population * 100/CPI), col = 2)\n\n\n\n# -- Comparing the two CPIs: --\ndat %&gt;% \n  pivot_longer(cols = c(CPI,CPI1990)) %&gt;%\n  ggplot(aes(x = Year, y = value, col = name)) + geom_line()+\n  geom_hline(yintercept = 100) +\n  scale_color_manual(values = c(\"red\",\"blue\"))+\n  geom_segment(x = 1990,xend = 1990, y = -Inf, yend =100, lty = 2, col = \"blue\")+\n  geom_segment(x = 2010,xend = 2010, y = -Inf, yend =100, lty = 2, col = \"red\")+\n  labs(title = \"Differences between CPI with reference year 1990 and 2010\",\n       y = \"Consumer Price Index\") +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\nUse the quantmod package to download data for another stock (e.g. Equinor = EQNR). What kind of transformation would you prefer for your stock?\n\n\n\nSolution\n\nI choose the Norwegian company Equinor.\n\nlibrary(quantmod)\n# Equinor: \ngetSymbols(\"EQNR\")\n\n[1] \"EQNR\"\n\nclose.price &lt;- tibble(\n  close = as.numeric(EQNR$EQNR.Close),\n  date  = time(EQNR)\n) %&gt;% as_tsibble(index = date) \n\n# -- Plot closing price: --\nclose.price %&gt;% autoplot(close) +\n  labs(title = \"Equinor Closing price\", \n       y     = \"US$\")\n\n\n\nclose.price &lt;- close.price %&gt;% \n  mutate(logclose  = log(close), # log-transform\n         logreturn = c(NA, diff(logclose)), # log returns\n         return    = c(NA, diff(close)/close[-nrow(close.price)]) # Returns\n         ) \n\nclose.price %&gt;% \n  pivot_longer(-date) %&gt;%\n  ggplot(aes(x=date, y = value))+ geom_line()+ \n  facet_wrap(~name, scales = \"free_y\")\n\n\n\n\nThe return series seem to be good alternatives, also for this stock! The Box-Cox transformation seem to take away some of the trend-cycles in the raw data.\n\n\nUse the guerrero feature to select a \\lambda for the Box-Cox transformation on the data from the previous exercise.\n\n\n\nSolution\n\n\n# -- Box-cox-transform --\nlambda &lt;- close.price %&gt;%\n  features(close, features = guerrero) %&gt;%\n  pull(lambda_guerrero)\nlambda\n\n[1] 0.5801099\n\nclose.price &lt;- close.price %&gt;%\n  mutate(boxcox = box_cox(close,lambda))\nclose.price %&gt;%\n  autoplot(boxcox) +\n  labs(y = \"\",\n       title = latex2exp::TeX(paste0(\n         \"Transformed Equinor closing price with $\\\\lambda$ = \",\n         round(lambda,3))))+\n  # adding the blue close price\n  geom_line(aes(y=close), col = 4)\n\n\n\n\n\n\nImplement the additive classical decomposition method for the Norwegian Wholesale and Retails sales index. To adjust the seasonal component to sum to zero, use the following transformation S_t^\\star = S_t - |S_t| \\cdot \\frac{\\sum_r S_r}{\\sum_r |S_r|} where S_t is the seasonal component before the adjustment and S_t^\\star is the adjusted such that \\sum_{t}S_t^\\star = 0. Compare your results to the output from the function classical_decomposition().\n\n\n\nSolution\n\n\n# Load the data: \nwholesale  &lt;- readRDS(file= \"data/wholesale_and_retails_index_norway.rds\")\n\n# Home-made classical decomposition: \nclassic_decomp &lt;- wholesale %&gt;% \n  rename(Index = `Wholesale and retail sales index`) %&gt;%\n  mutate(`12-MA` = slider::slide_dbl(Index, mean, .before = 5, .after = 6, .complete = TRUE),\n         trend = slider::slide_dbl(`12-MA`, mean, .before = 1, .after = 0, .complete = TRUE), #`2x12-MA` TREND ESTIMATE\n         detrend = Index - trend,\n         MONTH = month(yearmonth)\n         )  %&gt;% \n  group_by(MONTH) %&gt;%\n  mutate(season = mean(detrend, na.rm=T)) %&gt;% # SEASON ESTIMATE\n  ungroup()\n\nTo adjust the seasonal component to sum-to-zero, we update it as S_t^\\star = S_t - |S_t| \\cdot \\frac{\\sum_r S_r}{\\sum_r |S_r|}.\n\n# Adjust season to sum-to-zero\nS &lt;- sum(classic_decomp$season)\nA &lt;- sum(abs(classic_decomp$season))\nclassic_decomp &lt;- classic_decomp %&gt;% \n  mutate(seasonal = season - abs(season)*S/A) %&gt;% \n# Calculate remainder R = Y - T - S\n  mutate(random = Index - trend - seasonal) %&gt;% # REMAINDER\n  ungroup() %&gt;%\n  select(yearmonth, Index, trend, seasonal, random) %&gt;%\n  mutate(.model = \"homemade\")\n# Plot results: \nclassic_decomp %&gt;% \n  pivot_longer(cols =c(-yearmonth,-.model)) %&gt;%\n  mutate(name = factor(name, \n                       levels = c(\"Index\", \"trend\", \"seasonal\", \"random\"))) %&gt;%\nggplot(aes(x= yearmonth, y = value))+geom_line()+\nfacet_wrap( ~ name, ncol = 1, scales = \"free_y\")\n\n\n\n# Let's compare with the automatic one: \nwholesale %&gt;% \n  rename(Index = `Wholesale and retail sales index`) %&gt;%\n  model(\n    classic = classical_decomposition(Index)\n  ) %&gt;%\n  components() %&gt;% \n  bind_rows(classic_decomp) %&gt;% \n  autoplot()\n\n\n\n\nPerfect match!"
  },
  {
    "objectID": "4_exercises.html",
    "href": "4_exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Exercises\n\nExercise 1 in Chapter 4.6.\nExercise 2 in Chapter 4.6.\nExercise 3 in Chapter 4.6."
  },
  {
    "objectID": "4_timeseriesfeatures.html",
    "href": "4_timeseriesfeatures.html",
    "title": "Time series features",
    "section": "",
    "text": "Time series features\nThis chapter is maybe mostly about learning how to use features in time series analysis. It has mostly a technical side that you should learn to master. Calculating features such a means, standard deviation or quantiles is maybe not new to you, but the ACF and STL features I would imagine are. Using features to detect outlyers among many time series is a nice application of the material you should learn in this chapter."
  },
  {
    "objectID": "5_forecasterstoolbox.html",
    "href": "5_forecasterstoolbox.html",
    "title": "Forecasters toolbox",
    "section": "",
    "text": "Forecasters toolbox"
  },
  {
    "objectID": "7_regressionmodels.html",
    "href": "7_regressionmodels.html",
    "title": "Regression models",
    "section": "",
    "text": "Regression models"
  },
  {
    "objectID": "9_arch.html",
    "href": "9_arch.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Warning: package 'ggplot2' was built under R version 4.3.2\n\n\nWe start by introducing the first parameteric volatility model, the AutoRegressive Conditional Hetereskedasticity model (ARCH). This model was first introduce by Engle (1982). When writing this, his article has 32 369 citations on Google Scholar, and Robert Engle was awarded the Nobel Memorial Prize in Economic Sciences in 2003 with Clive Granger for “for methods of analyzing economic time series with time-varying volatility (ARCH)” (see nobelprize.org).\n\n\nLet y_t denote the time series of interest, \\sigma_t the volatility and \\varepsilon_t \\sim \\text{iid WN}(0,1). Then an ARCH(1) model can be written as \\begin{equation*}\n\\begin{split}\ny_t &= \\sigma_t \\varepsilon_t,\\\\\n\\sigma_t^2 &= \\omega + \\alpha_1 y_{t-1}^2, \\quad \\omega&gt;0,\\, \\alpha_1\\ge0,\n\\end{split}\n\\end{equation*} where t is denotes the time index. As you can see, the conditional squared volatility depends on the previous squared observations. This means if we get a large return, either positive or negative, the conditional variance will be large in the next time step. With a high conditional variance the we would expect to see a large return, in either direction, also for the next time step, leading to a volatility cluster. With only one lagged variable in the model, this can “die out” quite quickly, but we will use this simple model to do some more detailed calculations before extending the model.\n\n\n\nNote that we assume here that y_t has zero expectation, because \\mu=\\mathbb E\\, y_t = \\mathbb E( \\underbrace{\\sigma_t\\, \\varepsilon_t}_\\text{independent}) = \\mathbb E\\,\\sigma_t \\underbrace{\\mathbb E\\,\\varepsilon_t}_{=0} = 0 We thus have that the variance of y_t is\n\\mathrm{Var}(y_t) = \\underbrace{\\mathbb E(y_t-\\mu)^2}_{\\mu=0} = \\mathbb E\\,y_t^2 = \\mathbb E(\\sigma_t^2\\varepsilon_t^2) = \\mathbb E\\,\\sigma_t^2 \\underbrace{\\mathbb E\\,\\varepsilon_t^2}_{=1}=\\mathbb E\\,\\sigma_t^2\nLet’s assume that y_t is stationary, implying that \\mathrm{Var}(y_t) = \\mathbb E\\,\\sigma_t^2 = \\sigma^2. We then have that \\mathbb E\\,\\sigma_t^2 = \\omega+\\alpha_1\\mathbb E\\,y_{t-1}^2 = \\omega +\\alpha_1 \\mathbb E\\,\\sigma_{t-1}^2. Since \\mathbb E\\,\\sigma_t^2 = \\mathbb E\\,\\sigma_{t-1}^2 = \\sigma^2, we get that \\sigma^2 = \\omega+\\alpha_1\\sigma^2 \\quad\\Leftrightarrow\\quad \\sigma^2 = \\frac{\\omega}{1-\\alpha_1}. That is, if y_t is stationary the (unconditional) variance is \\omega/(1-\\alpha_1), requiring \\alpha_1\\in[0,1). This is in fact a necessary and sufficient condition for y_t being stationary.\n\n\n\nThere is a strong link between ARCH models for volatility an AR models for the level. In fact, we can write a ARCH(1) model as an AR(1) model. By adding and subtracting \\sigma_t^2, we get y_t^2 = \\sigma_t^2\\, \\varepsilon_t^2 = \\sigma_t^2 + \\underbrace{\\sigma_t^2(\\varepsilon_t^2-1)}_{v_t}=\\sigma_t^2 + v_t. It is easy to show that \\mathbb E\\,v_t =0 and under certain conditions (\\mathbb E\\,y_t^4&lt;\\infty) v_t will be a white noise process. We can then write y_t^2 = \\sigma_t^2+v_t = \\omega+\\alpha_1 y_{t-1}^2 +v_t\\quad \\text{or} \\quad (1-\\alpha_1\\,B)y_t^2 = \\omega+v_t. Thus, we have written the squared ARCH(1) as an AR(1) time series with intercept.\nThe \\varepsilon_t is often referred to as the innovations of the ARCH process, similar to innovation residuals in the fable environment. We can write a ARCH(1) only in terms of the parameters and the infinite past innovations. \\sigma_t^2 = \\omega + \\alpha_1 y_{t-1}^2= \\omega+\\alpha_1\\sigma_{t-1}^2\\varepsilon_{t-1}^2=\\omega+\\alpha_1(\\omega+\\alpha_1 \\sigma_{t-2}^2\\varepsilon_{t-2}^2)\\varepsilon_{t-1}^2. If we continue this iteration backwards, we end up with \\sigma_t^2=\\cdots = \\omega+\\omega\\sum_{j=1}^\\infty\\alpha_1^j\\varepsilon_{t-j}^2. This representation is actually the MA(\\infty) representation corresponding to the AR(1). One can take expectations on both sides of this equation and derive the formula for the variance of a stationary ARCH(1) process.\n\n\n\nSimilar to the relationship between an AR(1) and an AR(p), we can set up a ARCH(r). We use r to denote the order of the ARCH model to distinguish it from the p and q of ARIMA, but in the literature it is quite common to use p to denote the order of an ARCH model.\nAn ARCH(r) model can be set up as y_t = \\sigma_t\\,\\varepsilon_t,\\quad \\sigma_t^2 = \\omega + \\alpha_1\\,y_{t-1}^2+\\alpha_2\\,y_{t-2}^2+\\cdots \\alpha_r\\,y_{t-r}^2=\\omega + \\sum_{j=1}^r\\alpha_jy_{t-j}^2, where \\omega&gt;0,\\,\\alpha_j\\ge0, j=1,\\ldots, r.\nIncluding more lags, will make the ARCH model’s volatility more persistent, because a large return will stay in the model for some time. We could do similar calculations as above to show that the expectation of y_t is zero and if y_t is stationary the variance is given by \\sigma^2= \\frac{\\omega}{1-\\sum_{j=1}^r\\alpha_j},\\quad \\sum_{j=1}^r\\alpha_j&lt;1 You can also find an AR(r) representation of y_t^2 using the same approach and assumptions as above.\n\n\n\nLet us simulate an ARCH(1) model by basic R code, assuming Gaussian innovations.\n\nlibrary(fpp3)\nlibrary(tidyverse)\n# Setting seed for reproduciblity\nset.seed(12345)\nnT = 1000\nomg = 1.4\na1 = 0.95\n# Initiating y and sigma\ny &lt;- sig &lt;- rep(sqrt(omg/(1-a1)),\n                nT)\n# Simulation: \nfor(t in 2:nT){\n  sig[t] &lt;- sqrt(omg+a1*y[t-1]^2)\n  y[t]   &lt;- sig[t]*rnorm(1)\n}\n# tsibble object: \narch &lt;- tsibble(\n  t = 1:nT,\n  y = y,\n  sig = sig,\n  index = t\n) \n# Plotting: \narch %&gt;% pivot_longer(-t) %&gt;% \n  mutate(name = factor(name, levels = c(\"y\",\"sig\"))) %&gt;% \n  ggplot(aes(x=t,y=value)) + \n  geom_line()+ \n  facet_wrap(~name, scales = \"free_y\", nrow=2, \n             strip.position = \"left\")+\n  theme(axis.title =element_blank())\n\n\n\n\nIn the top panel you see the time series (think of the Microsoft returns from the previous chapter). When \\sigma_t is high, you see high variation in y_t. When \\sigma_t is high, it stays high for some time, leading to volatility clusters.\nLet’s look at the autocorrelation of the simulated time series and its squared values. We also do a qq-plot and a distribution plot.\n\narch %&gt;% ACF(y) %&gt;% autoplot() + labs(y = \"ACF of y\")\narch %&gt;% ACF(y^2) %&gt;% autoplot() + labs(y = \"ACF of squared y\")\narch %&gt;%  ggplot(aes(sample = y)) + geom_qq() +geom_qq_line()\narch %&gt;%  ggplot(aes(x = y)) + stat_density(fill = \"blue\", alpha = .2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe patterns are very similar to the pattern we saw for Microsoft returns - as is the main goal of these models!"
  },
  {
    "objectID": "9_arch.html#arch-models",
    "href": "9_arch.html#arch-models",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Warning: package 'ggplot2' was built under R version 4.3.2\n\n\nWe start by introducing the first parameteric volatility model, the AutoRegressive Conditional Hetereskedasticity model (ARCH). This model was first introduce by Engle (1982). When writing this, his article has 32 369 citations on Google Scholar, and Robert Engle was awarded the Nobel Memorial Prize in Economic Sciences in 2003 with Clive Granger for “for methods of analyzing economic time series with time-varying volatility (ARCH)” (see nobelprize.org).\n\n\nLet y_t denote the time series of interest, \\sigma_t the volatility and \\varepsilon_t \\sim \\text{iid WN}(0,1). Then an ARCH(1) model can be written as \\begin{equation*}\n\\begin{split}\ny_t &= \\sigma_t \\varepsilon_t,\\\\\n\\sigma_t^2 &= \\omega + \\alpha_1 y_{t-1}^2, \\quad \\omega&gt;0,\\, \\alpha_1\\ge0,\n\\end{split}\n\\end{equation*} where t is denotes the time index. As you can see, the conditional squared volatility depends on the previous squared observations. This means if we get a large return, either positive or negative, the conditional variance will be large in the next time step. With a high conditional variance the we would expect to see a large return, in either direction, also for the next time step, leading to a volatility cluster. With only one lagged variable in the model, this can “die out” quite quickly, but we will use this simple model to do some more detailed calculations before extending the model.\n\n\n\nNote that we assume here that y_t has zero expectation, because \\mu=\\mathbb E\\, y_t = \\mathbb E( \\underbrace{\\sigma_t\\, \\varepsilon_t}_\\text{independent}) = \\mathbb E\\,\\sigma_t \\underbrace{\\mathbb E\\,\\varepsilon_t}_{=0} = 0 We thus have that the variance of y_t is\n\\mathrm{Var}(y_t) = \\underbrace{\\mathbb E(y_t-\\mu)^2}_{\\mu=0} = \\mathbb E\\,y_t^2 = \\mathbb E(\\sigma_t^2\\varepsilon_t^2) = \\mathbb E\\,\\sigma_t^2 \\underbrace{\\mathbb E\\,\\varepsilon_t^2}_{=1}=\\mathbb E\\,\\sigma_t^2\nLet’s assume that y_t is stationary, implying that \\mathrm{Var}(y_t) = \\mathbb E\\,\\sigma_t^2 = \\sigma^2. We then have that \\mathbb E\\,\\sigma_t^2 = \\omega+\\alpha_1\\mathbb E\\,y_{t-1}^2 = \\omega +\\alpha_1 \\mathbb E\\,\\sigma_{t-1}^2. Since \\mathbb E\\,\\sigma_t^2 = \\mathbb E\\,\\sigma_{t-1}^2 = \\sigma^2, we get that \\sigma^2 = \\omega+\\alpha_1\\sigma^2 \\quad\\Leftrightarrow\\quad \\sigma^2 = \\frac{\\omega}{1-\\alpha_1}. That is, if y_t is stationary the (unconditional) variance is \\omega/(1-\\alpha_1), requiring \\alpha_1\\in[0,1). This is in fact a necessary and sufficient condition for y_t being stationary.\n\n\n\nThere is a strong link between ARCH models for volatility an AR models for the level. In fact, we can write a ARCH(1) model as an AR(1) model. By adding and subtracting \\sigma_t^2, we get y_t^2 = \\sigma_t^2\\, \\varepsilon_t^2 = \\sigma_t^2 + \\underbrace{\\sigma_t^2(\\varepsilon_t^2-1)}_{v_t}=\\sigma_t^2 + v_t. It is easy to show that \\mathbb E\\,v_t =0 and under certain conditions (\\mathbb E\\,y_t^4&lt;\\infty) v_t will be a white noise process. We can then write y_t^2 = \\sigma_t^2+v_t = \\omega+\\alpha_1 y_{t-1}^2 +v_t\\quad \\text{or} \\quad (1-\\alpha_1\\,B)y_t^2 = \\omega+v_t. Thus, we have written the squared ARCH(1) as an AR(1) time series with intercept.\nThe \\varepsilon_t is often referred to as the innovations of the ARCH process, similar to innovation residuals in the fable environment. We can write a ARCH(1) only in terms of the parameters and the infinite past innovations. \\sigma_t^2 = \\omega + \\alpha_1 y_{t-1}^2= \\omega+\\alpha_1\\sigma_{t-1}^2\\varepsilon_{t-1}^2=\\omega+\\alpha_1(\\omega+\\alpha_1 \\sigma_{t-2}^2\\varepsilon_{t-2}^2)\\varepsilon_{t-1}^2. If we continue this iteration backwards, we end up with \\sigma_t^2=\\cdots = \\omega+\\omega\\sum_{j=1}^\\infty\\alpha_1^j\\varepsilon_{t-j}^2. This representation is actually the MA(\\infty) representation corresponding to the AR(1). One can take expectations on both sides of this equation and derive the formula for the variance of a stationary ARCH(1) process.\n\n\n\nSimilar to the relationship between an AR(1) and an AR(p), we can set up a ARCH(r). We use r to denote the order of the ARCH model to distinguish it from the p and q of ARIMA, but in the literature it is quite common to use p to denote the order of an ARCH model.\nAn ARCH(r) model can be set up as y_t = \\sigma_t\\,\\varepsilon_t,\\quad \\sigma_t^2 = \\omega + \\alpha_1\\,y_{t-1}^2+\\alpha_2\\,y_{t-2}^2+\\cdots \\alpha_r\\,y_{t-r}^2=\\omega + \\sum_{j=1}^r\\alpha_jy_{t-j}^2, where \\omega&gt;0,\\,\\alpha_j\\ge0, j=1,\\ldots, r.\nIncluding more lags, will make the ARCH model’s volatility more persistent, because a large return will stay in the model for some time. We could do similar calculations as above to show that the expectation of y_t is zero and if y_t is stationary the variance is given by \\sigma^2= \\frac{\\omega}{1-\\sum_{j=1}^r\\alpha_j},\\quad \\sum_{j=1}^r\\alpha_j&lt;1 You can also find an AR(r) representation of y_t^2 using the same approach and assumptions as above.\n\n\n\nLet us simulate an ARCH(1) model by basic R code, assuming Gaussian innovations.\n\nlibrary(fpp3)\nlibrary(tidyverse)\n# Setting seed for reproduciblity\nset.seed(12345)\nnT = 1000\nomg = 1.4\na1 = 0.95\n# Initiating y and sigma\ny &lt;- sig &lt;- rep(sqrt(omg/(1-a1)),\n                nT)\n# Simulation: \nfor(t in 2:nT){\n  sig[t] &lt;- sqrt(omg+a1*y[t-1]^2)\n  y[t]   &lt;- sig[t]*rnorm(1)\n}\n# tsibble object: \narch &lt;- tsibble(\n  t = 1:nT,\n  y = y,\n  sig = sig,\n  index = t\n) \n# Plotting: \narch %&gt;% pivot_longer(-t) %&gt;% \n  mutate(name = factor(name, levels = c(\"y\",\"sig\"))) %&gt;% \n  ggplot(aes(x=t,y=value)) + \n  geom_line()+ \n  facet_wrap(~name, scales = \"free_y\", nrow=2, \n             strip.position = \"left\")+\n  theme(axis.title =element_blank())\n\n\n\n\nIn the top panel you see the time series (think of the Microsoft returns from the previous chapter). When \\sigma_t is high, you see high variation in y_t. When \\sigma_t is high, it stays high for some time, leading to volatility clusters.\nLet’s look at the autocorrelation of the simulated time series and its squared values. We also do a qq-plot and a distribution plot.\n\narch %&gt;% ACF(y) %&gt;% autoplot() + labs(y = \"ACF of y\")\narch %&gt;% ACF(y^2) %&gt;% autoplot() + labs(y = \"ACF of squared y\")\narch %&gt;%  ggplot(aes(sample = y)) + geom_qq() +geom_qq_line()\narch %&gt;%  ggplot(aes(x = y)) + stat_density(fill = \"blue\", alpha = .2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe patterns are very similar to the pattern we saw for Microsoft returns - as is the main goal of these models!"
  },
  {
    "objectID": "9_arch.html#references",
    "href": "9_arch.html#references",
    "title": "BAN430 Forecasting",
    "section": "References",
    "text": "References\n\nEngle, R. F. (1982). Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation. Econometrica: Journal of the econometric society, 987-1007."
  },
  {
    "objectID": "9_forecast_garch.html",
    "href": "9_forecast_garch.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Warning: package 'ggplot2' was built under R version 4.3.2\n\n\nForecasting with an ARCH/GARCH model is done by continuing the iteration of the model beyond the data that we have. If we set t=T+1 for an GARCH(1,1) we get the equation \\widehat\\sigma_{T+1|T}^2 =\\mathbb E\\,( y_{T+1}^2|\\mathcal F_T) =\\widehat\\omega+\\widehat\\alpha_1\\,y_{T}^2 +\\widehat\\beta_1\\, \\widehat\\sigma_{T}^2 where \\widehat\\omega,\\widehat\\alpha_1,\\widehat\\beta_1 are estimated parameters, y_T^2 is observed at time T and \\widehat\\sigma_{T}^2  = \\widehat\\omega\\sum_{j=0}^{T-1} \\widehat\\beta_1^j+\\widehat\\alpha_1\\sum_{j=1}^{T-1}\\widehat\\beta_1^{j-1}\\,y_{T-j}^2 +\\widehat\\beta_1^T\\, \\widehat\\sigma_{1}^2, and \\widehat\\sigma_1^2 = \\widehat\\omega/(1-\\widehat\\alpha_1-\\widehat\\beta_1). If \\widehat\\beta_1 is small and T large, the initial value for \\widehat\\sigma_1^2 will not matter much.\nMultistep forecast is achieved by noticing that \\widehat\\sigma_{T+h|T}^2= \\mathbb E\\,(\\sigma_{T+h}^2|\\mathcal F_T)=\\mathbb E\\,(y_{T+h}^2|\\mathcal F_T), such that \\begin{equation*}\n\\begin{split}\n\\widehat\\sigma_{T+h|T}^2 &= \\omega+\\alpha_1\\mathbb E(y_{T+h-1}^2|\\mathcal F_T) +\\beta_1\\mathbb E\\,(\\sigma_{T+h-1}|\\mathcal F_T)\\\\&=\\omega+\\alpha_1\\,\\widehat\\sigma_{T+h-1|T}^2 +\\beta_1\\,\\widehat\\sigma_{T+h-1|T}^2\\\\\n&=\\omega+(\\alpha_1+\\beta_1)\\widehat\\sigma_{T+h-1|T}^2\n\\end{split}\n\\end{equation*} If we simply iterate this formula backwards until we reach something that is known at time T. To simplify notation, let \\gamma = \\alpha_1+\\beta_1 and we also skip the hats for the time being. \\begin{equation*}\n\\begin{split}\n\\widehat\\sigma_{T+h|T}^2 &=\\omega+\\gamma \\widehat\\sigma_{T+h-1|T}^2 \\\\\n&= \\omega+\\gamma(\\omega+\\gamma\\, \\widehat\\sigma_{T+h-2|T}^2) \\\\\n&= \\omega+\\omega\\gamma+\\gamma^2\\, \\widehat\\sigma_{T+h-2|T}^2 \\\\\n&= \\omega+\\omega\\gamma+\\omega\\gamma^2 + \\gamma^3\\, \\widehat\\sigma_{T+h-3|T}^2 \\\\\n&\\quad\\vdots\\\\\n&= \\omega\\sum_{j=0}^{h-2}\\gamma^j + \\gamma^{h-1}\\,\\widehat\\sigma_{T+1|T}^2\\\\\n&= \\omega\\sum_{j=0}^{h-2}\\gamma^j + \\gamma^{h-1}\\,(\\omega+\\alpha_1 y_T^2+\\beta_1\\widehat\\sigma_T^2)\\\\\n&= \\omega\\sum_{j=0}^{h-1}\\gamma^j + \\gamma^{h-1}\\,(\\alpha_1 y_T^2+\\beta_1\\widehat\\sigma_T^2)\\\\\n\\end{split}\n\\end{equation*} Thus, inserting the \\alpha_1+\\beta_1 for \\gamma we have that \\widehat\\sigma_{T+h|T}^2 = \\widehat\\omega\\sum_{j=0}^{h-1}(\\widehat\\alpha_1+\\widehat\\beta_1)^j + (\\widehat\\alpha_1+\\widehat\\beta_1)^{h-1}\\,(\\widehat\\alpha_1 y_T^2+\\widehat\\beta_1\\widehat\\sigma_T^2). Assuming \\widehat\\alpha_1+\\widehat\\beta_1&lt;1, we get that when h\\to\\infty, \\lim_{h\\to\\infty}\\widehat\\sigma_{T+h|T} = \\frac{\\widehat\\omega}{1-\\widehat\\alpha_1-\\widehat\\beta_1}, i.e. the forecast will approach the unconditional variance, which intuitively makes sense.\nFor the conditional variance we are typically not so interested in creating predictions intervals for the volatility, but rather use the point forecast for the volatility to make prediction intervals for the variable of interest (i.e. the stock returns). We forecast the varying forecast variance and use that instead of the fixed one we have seen used in other settings. Since the expectation of a GARCH model is zero, a 100(1-\\alpha)\\% prediction interval (under Gaussian assumptions) is given as \\pm z_{\\alpha/2}\\,\\widehat \\sigma_{T+h|T}. We can also use volatility forecasting to calculate risk measures, such as Value At Risk (VaR) or Expected Shortfall (ES) (see McNeil et al, 2005, page 161)."
  },
  {
    "objectID": "9_forecast_garch.html#forecasting-with-garch",
    "href": "9_forecast_garch.html#forecasting-with-garch",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Warning: package 'ggplot2' was built under R version 4.3.2\n\n\nForecasting with an ARCH/GARCH model is done by continuing the iteration of the model beyond the data that we have. If we set t=T+1 for an GARCH(1,1) we get the equation \\widehat\\sigma_{T+1|T}^2 =\\mathbb E\\,( y_{T+1}^2|\\mathcal F_T) =\\widehat\\omega+\\widehat\\alpha_1\\,y_{T}^2 +\\widehat\\beta_1\\, \\widehat\\sigma_{T}^2 where \\widehat\\omega,\\widehat\\alpha_1,\\widehat\\beta_1 are estimated parameters, y_T^2 is observed at time T and \\widehat\\sigma_{T}^2  = \\widehat\\omega\\sum_{j=0}^{T-1} \\widehat\\beta_1^j+\\widehat\\alpha_1\\sum_{j=1}^{T-1}\\widehat\\beta_1^{j-1}\\,y_{T-j}^2 +\\widehat\\beta_1^T\\, \\widehat\\sigma_{1}^2, and \\widehat\\sigma_1^2 = \\widehat\\omega/(1-\\widehat\\alpha_1-\\widehat\\beta_1). If \\widehat\\beta_1 is small and T large, the initial value for \\widehat\\sigma_1^2 will not matter much.\nMultistep forecast is achieved by noticing that \\widehat\\sigma_{T+h|T}^2= \\mathbb E\\,(\\sigma_{T+h}^2|\\mathcal F_T)=\\mathbb E\\,(y_{T+h}^2|\\mathcal F_T), such that \\begin{equation*}\n\\begin{split}\n\\widehat\\sigma_{T+h|T}^2 &= \\omega+\\alpha_1\\mathbb E(y_{T+h-1}^2|\\mathcal F_T) +\\beta_1\\mathbb E\\,(\\sigma_{T+h-1}|\\mathcal F_T)\\\\&=\\omega+\\alpha_1\\,\\widehat\\sigma_{T+h-1|T}^2 +\\beta_1\\,\\widehat\\sigma_{T+h-1|T}^2\\\\\n&=\\omega+(\\alpha_1+\\beta_1)\\widehat\\sigma_{T+h-1|T}^2\n\\end{split}\n\\end{equation*} If we simply iterate this formula backwards until we reach something that is known at time T. To simplify notation, let \\gamma = \\alpha_1+\\beta_1 and we also skip the hats for the time being. \\begin{equation*}\n\\begin{split}\n\\widehat\\sigma_{T+h|T}^2 &=\\omega+\\gamma \\widehat\\sigma_{T+h-1|T}^2 \\\\\n&= \\omega+\\gamma(\\omega+\\gamma\\, \\widehat\\sigma_{T+h-2|T}^2) \\\\\n&= \\omega+\\omega\\gamma+\\gamma^2\\, \\widehat\\sigma_{T+h-2|T}^2 \\\\\n&= \\omega+\\omega\\gamma+\\omega\\gamma^2 + \\gamma^3\\, \\widehat\\sigma_{T+h-3|T}^2 \\\\\n&\\quad\\vdots\\\\\n&= \\omega\\sum_{j=0}^{h-2}\\gamma^j + \\gamma^{h-1}\\,\\widehat\\sigma_{T+1|T}^2\\\\\n&= \\omega\\sum_{j=0}^{h-2}\\gamma^j + \\gamma^{h-1}\\,(\\omega+\\alpha_1 y_T^2+\\beta_1\\widehat\\sigma_T^2)\\\\\n&= \\omega\\sum_{j=0}^{h-1}\\gamma^j + \\gamma^{h-1}\\,(\\alpha_1 y_T^2+\\beta_1\\widehat\\sigma_T^2)\\\\\n\\end{split}\n\\end{equation*} Thus, inserting the \\alpha_1+\\beta_1 for \\gamma we have that \\widehat\\sigma_{T+h|T}^2 = \\widehat\\omega\\sum_{j=0}^{h-1}(\\widehat\\alpha_1+\\widehat\\beta_1)^j + (\\widehat\\alpha_1+\\widehat\\beta_1)^{h-1}\\,(\\widehat\\alpha_1 y_T^2+\\widehat\\beta_1\\widehat\\sigma_T^2). Assuming \\widehat\\alpha_1+\\widehat\\beta_1&lt;1, we get that when h\\to\\infty, \\lim_{h\\to\\infty}\\widehat\\sigma_{T+h|T} = \\frac{\\widehat\\omega}{1-\\widehat\\alpha_1-\\widehat\\beta_1}, i.e. the forecast will approach the unconditional variance, which intuitively makes sense.\nFor the conditional variance we are typically not so interested in creating predictions intervals for the volatility, but rather use the point forecast for the volatility to make prediction intervals for the variable of interest (i.e. the stock returns). We forecast the varying forecast variance and use that instead of the fixed one we have seen used in other settings. Since the expectation of a GARCH model is zero, a 100(1-\\alpha)\\% prediction interval (under Gaussian assumptions) is given as \\pm z_{\\alpha/2}\\,\\widehat \\sigma_{T+h|T}. We can also use volatility forecasting to calculate risk measures, such as Value At Risk (VaR) or Expected Shortfall (ES) (see McNeil et al, 2005, page 161)."
  },
  {
    "objectID": "9_forecast_garch.html#references",
    "href": "9_forecast_garch.html#references",
    "title": "BAN430 Forecasting",
    "section": "References",
    "text": "References\n\nEngle, R. F. (1982). Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation. Econometrica: Journal of the econometric society, 987-1007.\nMcNeil, A. J., Frey, R., & Embrechts, P. (2005). Quantitative risk management: concepts, techniques and tools-revised edition. Princeton university press."
  },
  {
    "objectID": "9_Volatitiliy_forecasting.html",
    "href": "9_Volatitiliy_forecasting.html",
    "title": "Volatility forecasting",
    "section": "",
    "text": "Warning: package 'ggplot2' was built under R version 4.3.2\n\n\nAll models considered so far have had a constant residual variance, i.e.\\sigma^2 in \\varepsilon\\sim\\text{WN}(0,\\sigma^2). This is also true for the models we will consider in this chapter, but we allow the conditional variance, or the volatility, to vary. That is the variance conditioned on the past of the time series of interest. We will define it more closely below.\nVolatility can be an important concept for investment, security valuation, risk management and monetary policy making. When it is interpreted as uncertainty, it becomes a key to many investments decisions and portfolio creations (Poon and Granger, 2003). The models we consider is used a lot for financial time series, such as stock returns, currency exchange rates, etc.\n\n\nMcNeil et al (2005) list stylized facts about financial times series:\n\nSF1: Return series are not independent and identically distributed although they show little autocorrelation.\nSF2: Series of absolute or squared returns show profound autocorrelation.\nSF3: Conditional expected returns are close to zero.\nSF4: Volatility appear to vary over time.\nSF5: Return series are leptokurtic or heavy-tailed.\nSF6: Extreme returns appear in clusters.\n\nLet us consider a financial time series and see if these stylized facts hold for that. We will look closer at the Microsoft stock and use the tidyquant package for downloading the data starting from 2005 until the newest available data point. Since the stock exchange is closed on some days, we index the time series by trading day (i.e. the row number).\n\nlibrary(tidyquant) # Package getting stock data\nlibrary(tidyverse)\nlibrary(fpp3)\nMSFT &lt;- tq_get(\"MSFT\", \n               get = \"stock.prices\",\n               from = as.Date(\"2005-01-01\")) %&gt;% \n  select(date, close) %&gt;% \n  mutate(tradingday = row_number()) %&gt;% \n  as_tsibble(index = tradingday) %&gt;% \n  mutate(return = difference(close)/close)\nMSFT %&gt;% autoplot(return) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(y = \"Daily returns\",\n       title = \"Microsoft closing return series\")\n\n\n\n\nLet us also include autocorrelation plots of the returns and the squared returns.\n\nMSFT %&gt;% ACF(return) %&gt;% autoplot() + labs(y = \"ACF of returns\")\nMSFT %&gt;% ACF(return^2) %&gt;% autoplot() + labs(y = \"ACF of squared returns\")\nMSFT %&gt;%  ggplot(aes(sample = return)) + geom_qq() +geom_qq_line()\nMSFT %&gt;%  ggplot(aes(x = return)) + stat_density(fill = \"blue\", alpha = .2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere we clearly see that the returns have little autocorrelation (SF1), but the squared returns have a lot (SF2). Conditional returns is maybe not so easy to quantify, but the mean return of MSFT is 0.034\\% \\approx 0 (SF3). We can clearly see from the time plot of the returns that the volatility varies across time (SF4) and that the extreme returns tend to cluster together (SF6). Since we are using trading days for the time index, it is a bit hard to tell when these events occured, but the first big cluster (t\\approx 1000) is around the financial crisis in 2008-09 and the second big one is around 2020 (t\\approx 3900) when covid-19 went pandemic. Leptokurtic means that the kurtosis is positive (here it is 9.8). We can see from the qq-plot that the distribution is heavy tailed in both ends compared to a Gaussian distribution (SF5). It is also very concentrated around zero except for the extreme observations.\nThese stylized facts are dynamics we want a good model of financial returns to be able to capture.\n\n\n\nLet Y_t be a time series stochastic variable associated with time t and let \\mathcal F_t denote all the past history of the time series up to and including time t, i.e. \\mathcal F_t = \\{\\ldots, Y_{t-2},Y_{t-1}, Y_t\\}. Then Y_t|\\mathcal F_t is a constant (not stochastic). If the time series is stationary, the expectation is \\mu = \\mathbb E Y_t and the variance is defined as \\sigma^2 = \\mathbb E (Y_t-\\mu)^2= \\mathbb E Y_t^2 - \\mu^2. We define the volatility as the conditional standard deviation of the times series, conditioned on its history. Thus, the squared volatility is the conditional variance, given by \\sigma_t^2 = \\sigma_{t+1|t}^2 = \\mathbb E\\big[ (Y_{t+1}-\\mu)^2|\\mathcal F_{t}\\big].\nThe \\mathcal F_t is called a sigma algebra, which is a quite complex mathematical structure, but you can think of it as simply the infinite history of the time series.\n\n\n\nWe will here exclusively focus on the most basic volatility models in the GARCH family. The list of different model specifications available in the rugarch package is\n\nStandard GARCH (sGARCH)\nIntegrated GARCH (iGARCH)\nExponential GARCH (EGARCH)\nGJR-GARCH (gjrGARCH)\nasymmetric power ARCH (apARCH)\nFamily GARCH (fGARCH)\nComponent sGARCH (csGARCH)\nMultiplicative Component sGARCH (mcsGARCH)\nRealized GARCH (realGARCH)\nFractionally integrated GARCH (fiGARCH)\n\nWe will focus on the so-called standard (G)ARCH. If you are interested in digging deeper into some of the other models, see e.g. the package documention. This list is just scratching the surface of available GARCH-type models. The father of the original GARCH model, Tim Bollerslev, has published a Glossary of ARCH-GARCH models (Bollerslev, 2008) to give an overview of all the acronyms and abbreviations used for these type of models, but this has not been updated since 2008 (as far as I know). Bollerslev(2023) recently published a paper entitled the Story of GARCH which also discuss the many specialized GARCH models.\nThe volatility \\sigma_t is the quantity of interest in this chapter. In the next section we will introduce the first volatility model; ARCH models before turning the generalized ARCH models (or standard GARCH).\n\n\n\n\nBollerslev, T. (2008). Glossary to arch (garch). CREATES Research paper, 49.\nBollerslev, T. (2023). The story of GARCH: A personal odyssey. Journal of Econometrics.\nMcNeil, A. J., Frey, R., & Embrechts, P. (2005). Quantitative risk management: concepts, techniques and tools-revised edition. Princeton university press.\nPoon, S. H., & Granger, C. W. J. (2003). Forecasting volatility in financial markets: A review. Journal of economic literature, 41(2), 478-539."
  },
  {
    "objectID": "9_Volatitiliy_forecasting.html#stylized-facts-about-financial-return-series",
    "href": "9_Volatitiliy_forecasting.html#stylized-facts-about-financial-return-series",
    "title": "Volatility forecasting",
    "section": "",
    "text": "McNeil et al (2005) list stylized facts about financial times series:\n\nSF1: Return series are not independent and identically distributed although they show little autocorrelation.\nSF2: Series of absolute or squared returns show profound autocorrelation.\nSF3: Conditional expected returns are close to zero.\nSF4: Volatility appear to vary over time.\nSF5: Return series are leptokurtic or heavy-tailed.\nSF6: Extreme returns appear in clusters.\n\nLet us consider a financial time series and see if these stylized facts hold for that. We will look closer at the Microsoft stock and use the tidyquant package for downloading the data starting from 2005 until the newest available data point. Since the stock exchange is closed on some days, we index the time series by trading day (i.e. the row number).\n\nlibrary(tidyquant) # Package getting stock data\nlibrary(tidyverse)\nlibrary(fpp3)\nMSFT &lt;- tq_get(\"MSFT\", \n               get = \"stock.prices\",\n               from = as.Date(\"2005-01-01\")) %&gt;% \n  select(date, close) %&gt;% \n  mutate(tradingday = row_number()) %&gt;% \n  as_tsibble(index = tradingday) %&gt;% \n  mutate(return = difference(close)/close)\nMSFT %&gt;% autoplot(return) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(y = \"Daily returns\",\n       title = \"Microsoft closing return series\")\n\n\n\n\nLet us also include autocorrelation plots of the returns and the squared returns.\n\nMSFT %&gt;% ACF(return) %&gt;% autoplot() + labs(y = \"ACF of returns\")\nMSFT %&gt;% ACF(return^2) %&gt;% autoplot() + labs(y = \"ACF of squared returns\")\nMSFT %&gt;%  ggplot(aes(sample = return)) + geom_qq() +geom_qq_line()\nMSFT %&gt;%  ggplot(aes(x = return)) + stat_density(fill = \"blue\", alpha = .2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere we clearly see that the returns have little autocorrelation (SF1), but the squared returns have a lot (SF2). Conditional returns is maybe not so easy to quantify, but the mean return of MSFT is 0.034\\% \\approx 0 (SF3). We can clearly see from the time plot of the returns that the volatility varies across time (SF4) and that the extreme returns tend to cluster together (SF6). Since we are using trading days for the time index, it is a bit hard to tell when these events occured, but the first big cluster (t\\approx 1000) is around the financial crisis in 2008-09 and the second big one is around 2020 (t\\approx 3900) when covid-19 went pandemic. Leptokurtic means that the kurtosis is positive (here it is 9.8). We can see from the qq-plot that the distribution is heavy tailed in both ends compared to a Gaussian distribution (SF5). It is also very concentrated around zero except for the extreme observations.\nThese stylized facts are dynamics we want a good model of financial returns to be able to capture."
  },
  {
    "objectID": "9_Volatitiliy_forecasting.html#defining-volatility-mathematically",
    "href": "9_Volatitiliy_forecasting.html#defining-volatility-mathematically",
    "title": "Volatility forecasting",
    "section": "",
    "text": "Let Y_t be a time series stochastic variable associated with time t and let \\mathcal F_t denote all the past history of the time series up to and including time t, i.e. \\mathcal F_t = \\{\\ldots, Y_{t-2},Y_{t-1}, Y_t\\}. Then Y_t|\\mathcal F_t is a constant (not stochastic). If the time series is stationary, the expectation is \\mu = \\mathbb E Y_t and the variance is defined as \\sigma^2 = \\mathbb E (Y_t-\\mu)^2= \\mathbb E Y_t^2 - \\mu^2. We define the volatility as the conditional standard deviation of the times series, conditioned on its history. Thus, the squared volatility is the conditional variance, given by \\sigma_t^2 = \\sigma_{t+1|t}^2 = \\mathbb E\\big[ (Y_{t+1}-\\mu)^2|\\mathcal F_{t}\\big].\nThe \\mathcal F_t is called a sigma algebra, which is a quite complex mathematical structure, but you can think of it as simply the infinite history of the time series."
  },
  {
    "objectID": "9_Volatitiliy_forecasting.html#scratching-the-surface-of-garch",
    "href": "9_Volatitiliy_forecasting.html#scratching-the-surface-of-garch",
    "title": "Volatility forecasting",
    "section": "",
    "text": "We will here exclusively focus on the most basic volatility models in the GARCH family. The list of different model specifications available in the rugarch package is\n\nStandard GARCH (sGARCH)\nIntegrated GARCH (iGARCH)\nExponential GARCH (EGARCH)\nGJR-GARCH (gjrGARCH)\nasymmetric power ARCH (apARCH)\nFamily GARCH (fGARCH)\nComponent sGARCH (csGARCH)\nMultiplicative Component sGARCH (mcsGARCH)\nRealized GARCH (realGARCH)\nFractionally integrated GARCH (fiGARCH)\n\nWe will focus on the so-called standard (G)ARCH. If you are interested in digging deeper into some of the other models, see e.g. the package documention. This list is just scratching the surface of available GARCH-type models. The father of the original GARCH model, Tim Bollerslev, has published a Glossary of ARCH-GARCH models (Bollerslev, 2008) to give an overview of all the acronyms and abbreviations used for these type of models, but this has not been updated since 2008 (as far as I know). Bollerslev(2023) recently published a paper entitled the Story of GARCH which also discuss the many specialized GARCH models.\nThe volatility \\sigma_t is the quantity of interest in this chapter. In the next section we will introduce the first volatility model; ARCH models before turning the generalized ARCH models (or standard GARCH)."
  },
  {
    "objectID": "9_Volatitiliy_forecasting.html#references",
    "href": "9_Volatitiliy_forecasting.html#references",
    "title": "Volatility forecasting",
    "section": "",
    "text": "Bollerslev, T. (2008). Glossary to arch (garch). CREATES Research paper, 49.\nBollerslev, T. (2023). The story of GARCH: A personal odyssey. Journal of Econometrics.\nMcNeil, A. J., Frey, R., & Embrechts, P. (2005). Quantitative risk management: concepts, techniques and tools-revised edition. Princeton university press.\nPoon, S. H., & Granger, C. W. J. (2003). Forecasting volatility in financial markets: A review. Journal of economic literature, 41(2), 478-539."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Welcome to the website for BAN430 Forecasting. We will use this website as a supplement to lectures. The website is an ongoing development, so not all subjects will have content yet. Below you will find a detailed (preliminary) lecture plan, link to the textbook and curriculum.\n\n\n\n\n\n\n\nMonday 10:15-12:00\nThursday 12:15-14:00\n\n\n\n\n15.01.2024\n18.01.2024\n\n\nIntroduction lecture\nTime series graphics\n\n\n22.01.2024\n25.01.2024\n\n\nTime series decomposition\nTime series decomposition\n\n\n29.01.2024\n01.02.2024\n\n\nForecasters toolbox\nForecasters toolbox\n\n\n05.02.2024\n08.02.2024\n\n\nRegression models\nRegression models\n\n\n12.02.2024\n15.02.2024\n\n\nExponential smoothing\nExponential smoothing\n\n\n19.02.2024\n22.02.2024\n\n\nARIMA\nARIMA\n\n\n26.02.2024\n29.02.2024\n\n\nNo lecture: Selfstudy time series features (website)\nNo lecture: Selfstudy judgmental forecasts (website)\n\n\n04.03.024\n07.03.2024\n\n\nNo lecture / selfstudy\nARIMA\n\n\n11.03.2024\n14.03.2024\n\n\nARIMA + Dynamic regression models\nWorkshop 2\n\n\n18.03.2024\n21.03.2024\n\n\nVolatility forecasting\nVolatility forecasting\n\n\n25.03.2024\n28.03.2024\n\n\nEaster holiday\nEaster holiday\n\n\n01.04.2024\n04.04.2024\n\n\nEaster holiday\nVolatility forecasting\n\n\n08.04.2024\n11.04.2024\n\n\nSolutions to Workshop 2\nSummary lecture\n\n\nStudy period prior to exam\n\n\n\n22.04.2024\n\n\n\n8 hour home exam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFPP Third edition\n\n\nWe will use the textbook Forecasting: Principles and Practice, 3rd edition, by Hyndman and Athanasopoulos, i.e. the online version which can be accessed at https://otexts.com/fpp3/.\n\n\n\nTextbook Hyndman and Athanasopoulos(2021) chapters 1-10 and 13. Additional notes by lecturer on volatility forecasting. All the material on this website."
  },
  {
    "objectID": "index.html#lecture-plan",
    "href": "index.html#lecture-plan",
    "title": "Introduction",
    "section": "",
    "text": "Monday 10:15-12:00\nThursday 12:15-14:00\n\n\n\n\n15.01.2024\n18.01.2024\n\n\nIntroduction lecture\nTime series graphics\n\n\n22.01.2024\n25.01.2024\n\n\nTime series decomposition\nTime series decomposition\n\n\n29.01.2024\n01.02.2024\n\n\nForecasters toolbox\nForecasters toolbox\n\n\n05.02.2024\n08.02.2024\n\n\nRegression models\nRegression models\n\n\n12.02.2024\n15.02.2024\n\n\nExponential smoothing\nExponential smoothing\n\n\n19.02.2024\n22.02.2024\n\n\nARIMA\nARIMA\n\n\n26.02.2024\n29.02.2024\n\n\nNo lecture: Selfstudy time series features (website)\nNo lecture: Selfstudy judgmental forecasts (website)\n\n\n04.03.024\n07.03.2024\n\n\nNo lecture / selfstudy\nARIMA\n\n\n11.03.2024\n14.03.2024\n\n\nARIMA + Dynamic regression models\nWorkshop 2\n\n\n18.03.2024\n21.03.2024\n\n\nVolatility forecasting\nVolatility forecasting\n\n\n25.03.2024\n28.03.2024\n\n\nEaster holiday\nEaster holiday\n\n\n01.04.2024\n04.04.2024\n\n\nEaster holiday\nVolatility forecasting\n\n\n08.04.2024\n11.04.2024\n\n\nSolutions to Workshop 2\nSummary lecture\n\n\nStudy period prior to exam\n\n\n\n22.04.2024\n\n\n\n8 hour home exam"
  },
  {
    "objectID": "index.html#literature",
    "href": "index.html#literature",
    "title": "Introduction",
    "section": "",
    "text": "FPP Third edition\n\n\nWe will use the textbook Forecasting: Principles and Practice, 3rd edition, by Hyndman and Athanasopoulos, i.e. the online version which can be accessed at https://otexts.com/fpp3/."
  },
  {
    "objectID": "index.html#curriculum",
    "href": "index.html#curriculum",
    "title": "Introduction",
    "section": "",
    "text": "Textbook Hyndman and Athanasopoulos(2021) chapters 1-10 and 13. Additional notes by lecturer on volatility forecasting. All the material on this website."
  },
  {
    "objectID": "W1_tsibble.html",
    "href": "W1_tsibble.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "The point of this workshop is to get you familiarized with the fpp3 package and especially the tsibble object. This is the most important R structure of this course and we will therefore spend some time getting to know it better. The tsibble is the time series equivalent of a tibble object in the tidyverse. It is a data frame which is indexed by a time variable. To create a tsibble, we need to specify which column in the data is the time variable (the index variable). Here is an example, creating a tibble object from data on the temperature in Bergen:\n\nlibrary(fpp3) \nlibrary(tidyverse)\n# read the data: \ntemp &lt;- read_csv2(\n  \"https://raw.githubusercontent.com/holleland/BAN430/master/data/bergen_temp.csv\")\ntemp\n\n# A tibble: 516 × 2\n   date       temp \n   &lt;chr&gt;      &lt;chr&gt;\n 1 01.10.2021 12.9 \n 2 02.10.2021 11.7 \n 3 03.10.2021 13.6 \n 4 04.10.2021 11.7 \n 5 05.10.2021 11   \n 6 06.10.2021 10   \n 7 07.10.2021 11.1 \n 8 08.10.2021 14.2 \n 9 09.10.2021 13.8 \n10 10.10.2021 11.3 \n# ℹ 506 more rows\n\n# Convert date column from character to date and temp to numeric:\ntemp &lt;- temp %&gt;% \n  mutate(date = as.Date(date, format = \"%d.%m.%Y\"),\n         temp = as.numeric(temp))\ntemp\n\n# A tibble: 516 × 2\n   date        temp\n   &lt;date&gt;     &lt;dbl&gt;\n 1 2021-10-01  12.9\n 2 2021-10-02  11.7\n 3 2021-10-03  13.6\n 4 2021-10-04  11.7\n 5 2021-10-05  11  \n 6 2021-10-06  10  \n 7 2021-10-07  11.1\n 8 2021-10-08  14.2\n 9 2021-10-09  13.8\n10 2021-10-10  11.3\n# ℹ 506 more rows\n\n# Creating a tsibble\ntemp.ts &lt;- temp %&gt;% as_tsibble(index = date)\ntemp.ts\n\n# A tsibble: 516 x 2 [1D]\n   date        temp\n   &lt;date&gt;     &lt;dbl&gt;\n 1 2021-10-01  12.9\n 2 2021-10-02  11.7\n 3 2021-10-03  13.6\n 4 2021-10-04  11.7\n 5 2021-10-05  11  \n 6 2021-10-06  10  \n 7 2021-10-07  11.1\n 8 2021-10-08  14.2\n 9 2021-10-09  13.8\n10 2021-10-10  11.3\n# ℹ 506 more rows\n\n\nOnce we have made our tsibble object, the fpp3 package (or really the tsibble package) has many useful functions we can apply to that object, such as plotting functions. For instance, we can create a timeplot of the Bergen temperatures, by the following code:\n\ntemp.ts %&gt;% autoplot(temp)\n\n\n\n\nThis is a simple time series with only one variable (the daily mean temperature in Bergen). Often the data we study will consist of multiple time series. Then we need to provide information on which columns the identify the individual time series. This is called the key variable of the tsibble object. Let us consider an example with temperature in Bergen, Oslo, Trondheim and Stavanger.\n\ncitytemp &lt;- read_csv2(\n  \"https://raw.githubusercontent.com/holleland/BAN430/master/data//citytemp.csv\")\ncitytemp.ts &lt;- citytemp %&gt;% \n  mutate(date = as.Date(date, format = \"%d.%m.%Y\")) %&gt;% \n  filter(!is.na(date)) %&gt;%  # Remove NA-values\n  # Create tsibble object\n  as_tsibble(index = date, \n             key = c(\"name\",\"station\"))\ncitytemp.ts\n\n# A tsibble: 34,930 x 4 [1D]\n# Key:       name, station [4]\n   name             station date       meanTemp\n   &lt;chr&gt;            &lt;chr&gt;   &lt;date&gt;        &lt;dbl&gt;\n 1 Bergen - Florida SN50540 2000-01-01      6.3\n 2 Bergen - Florida SN50540 2000-01-02      6.3\n 3 Bergen - Florida SN50540 2000-01-03      6.7\n 4 Bergen - Florida SN50540 2000-01-04      4.6\n 5 Bergen - Florida SN50540 2000-01-05      4.6\n 6 Bergen - Florida SN50540 2000-01-06      6.5\n 7 Bergen - Florida SN50540 2000-01-07      6.3\n 8 Bergen - Florida SN50540 2000-01-08      6.4\n 9 Bergen - Florida SN50540 2000-01-09      4.1\n10 Bergen - Florida SN50540 2000-01-10      5.3\n# ℹ 34,920 more rows\n\ncitytemp.ts %&gt;% autoplot()\n\n\n\n\n\n\n\n\nRun the code above on your own. We will use the same data sets later, but for now, just check that you get the same figures.\nDo Exercise 3. All the code you need is given in the exercise. Note, you do not need to download the csv-file, but can load it directly to R using this link (replacing the first line of code):\n\n\ntute1 &lt;- readr::read_csv(\"https://bit.ly/fpptute1\")\n\n\nDo Exercise 4.\n\n\n# install.packages(\"USgas\")\nlibrary(USgas)\nhead(us_total)\n\n  year   state      y\n1 1997 Alabama 324158\n2 1998 Alabama 329134\n3 1999 Alabama 337270\n4 2000 Alabama 353614\n5 2001 Alabama 332693\n6 2002 Alabama 379343\n\n\n\n\nContinue with the temperature data from the largest cities in Norway. Create a\n\n\n\nTimeplot,\nSeasonal plot,\nSeasonal subseries plot.\n\n\n\nAggregate the temperature time series for the 4 largest cities in Norway from daily to weekly and monthly average temperatures. Create two illustrative figures for each.\nTemperatures in the Norwegian cities all follow the same seasonality (cold in winter - warm in summer). Create a graphic illustrating the correlation between these time series. Hint: GGally::ggpairs().\nCreate a autocorrelation plot of the temperature data for Bergen using default settings. Increase the maximum number of lags to 400. Interpret the latter plot. Is this a stationary time series?\nCreate a tsibble consisting of white noise (uncorrelated variables) of length 100. Create an autocorrelation plot for the time series you have simulated. Interpret the plot. Is this a stationary time series?\nThe Norwegian government has decided to work towards a goal of installing 30GW of offshore wind power. The two locations they have decided to start building the first wind parks is called Sørlige Nordsjø 2 and Utsira Nord. On Canvas, you find derived power production from modelled wind speed at these two locations on hourly time scale for 5 years.\n\n\n\nUsing the offshore wind power data, create illustrative figures for Utsira Nord and Sørlige Nordsjø 2 for different time scale aggregates (hourly, daily, weekly, monthly).\nCan you a detect trend/cycle/season based on your figures?\nWhat about the relationship between the two locations? Is the dependence linear?\nDoes your answer in (c) depend on the time scale you use?\nIf you were to decide where to build the first wind farm solely based on the data you have, which would you choose and why? Discuss with your neighbors.\n\n\n# Hint: \nwind &lt;- readRDS(\"OffshoreWindtwoLocationsFiveYears.rds\")\n\n\nCreate a tsibble containing the daily wind power data from Sørlige Nordsjø 2. Decompose the time series into three components; trend-cycle T_t, season S_t and remainder R_t, using a suitable decomposition method. Why did you choose the method you did? Is there any seasonal/trend patterns in the data?\nThe data used in this exercise is the wholesale and retail sales index from Statistics Norway. More specifically, the data is an index for Retail trade, except of motor vehicles and motorcycles. Data starts in Jan 2000 to what is presently available from Statistics Norway.\n\n\n\nLoad the data using e.g. read.csv2 indicated below. Convert the month column to a yearmonth type and wholesale as a tsibble.\n\n\nwholesale  &lt;- read.csv2(\n  \"https://raw.githubusercontent.com/holleland/BAN430/master/data/wholesale_and_retails_index_norway.csv\", \n  sep = \";\")\nhead(wholesale,3)\n\n    month wholesale_and_retail_sales_index\n1 2000M01                             50.5\n2 2000M02                             49.3\n3 2000M03                             53.8\n\n\n\nMake a time plot.\nDecompose the time series using the classical, x11, seats and STL methods. Can you detect any prominent differences between the methods?\nTry adjusting the trend and season windows of the STL. What happens? (default values are respectively 21 and 11).\nUsing your method of choice, plot the seasonally-adjusted time series.\nUsing your method of choice, plot the detrended series.\nOptional: Implement your own additive classical decomposition on this example (solution: see exercise 6).\n\n\nExercises from chapter 3: 1-3\n\nAdditional recommended exercises from chapter 2: 5, 7, 9 and 10"
  },
  {
    "objectID": "W1_tsibble.html#ws1-tsibble-graphics-and-decomposition",
    "href": "W1_tsibble.html#ws1-tsibble-graphics-and-decomposition",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "The point of this workshop is to get you familiarized with the fpp3 package and especially the tsibble object. This is the most important R structure of this course and we will therefore spend some time getting to know it better. The tsibble is the time series equivalent of a tibble object in the tidyverse. It is a data frame which is indexed by a time variable. To create a tsibble, we need to specify which column in the data is the time variable (the index variable). Here is an example, creating a tibble object from data on the temperature in Bergen:\n\nlibrary(fpp3) \nlibrary(tidyverse)\n# read the data: \ntemp &lt;- read_csv2(\n  \"https://raw.githubusercontent.com/holleland/BAN430/master/data/bergen_temp.csv\")\ntemp\n\n# A tibble: 516 × 2\n   date       temp \n   &lt;chr&gt;      &lt;chr&gt;\n 1 01.10.2021 12.9 \n 2 02.10.2021 11.7 \n 3 03.10.2021 13.6 \n 4 04.10.2021 11.7 \n 5 05.10.2021 11   \n 6 06.10.2021 10   \n 7 07.10.2021 11.1 \n 8 08.10.2021 14.2 \n 9 09.10.2021 13.8 \n10 10.10.2021 11.3 \n# ℹ 506 more rows\n\n# Convert date column from character to date and temp to numeric:\ntemp &lt;- temp %&gt;% \n  mutate(date = as.Date(date, format = \"%d.%m.%Y\"),\n         temp = as.numeric(temp))\ntemp\n\n# A tibble: 516 × 2\n   date        temp\n   &lt;date&gt;     &lt;dbl&gt;\n 1 2021-10-01  12.9\n 2 2021-10-02  11.7\n 3 2021-10-03  13.6\n 4 2021-10-04  11.7\n 5 2021-10-05  11  \n 6 2021-10-06  10  \n 7 2021-10-07  11.1\n 8 2021-10-08  14.2\n 9 2021-10-09  13.8\n10 2021-10-10  11.3\n# ℹ 506 more rows\n\n# Creating a tsibble\ntemp.ts &lt;- temp %&gt;% as_tsibble(index = date)\ntemp.ts\n\n# A tsibble: 516 x 2 [1D]\n   date        temp\n   &lt;date&gt;     &lt;dbl&gt;\n 1 2021-10-01  12.9\n 2 2021-10-02  11.7\n 3 2021-10-03  13.6\n 4 2021-10-04  11.7\n 5 2021-10-05  11  \n 6 2021-10-06  10  \n 7 2021-10-07  11.1\n 8 2021-10-08  14.2\n 9 2021-10-09  13.8\n10 2021-10-10  11.3\n# ℹ 506 more rows\n\n\nOnce we have made our tsibble object, the fpp3 package (or really the tsibble package) has many useful functions we can apply to that object, such as plotting functions. For instance, we can create a timeplot of the Bergen temperatures, by the following code:\n\ntemp.ts %&gt;% autoplot(temp)\n\n\n\n\nThis is a simple time series with only one variable (the daily mean temperature in Bergen). Often the data we study will consist of multiple time series. Then we need to provide information on which columns the identify the individual time series. This is called the key variable of the tsibble object. Let us consider an example with temperature in Bergen, Oslo, Trondheim and Stavanger.\n\ncitytemp &lt;- read_csv2(\n  \"https://raw.githubusercontent.com/holleland/BAN430/master/data//citytemp.csv\")\ncitytemp.ts &lt;- citytemp %&gt;% \n  mutate(date = as.Date(date, format = \"%d.%m.%Y\")) %&gt;% \n  filter(!is.na(date)) %&gt;%  # Remove NA-values\n  # Create tsibble object\n  as_tsibble(index = date, \n             key = c(\"name\",\"station\"))\ncitytemp.ts\n\n# A tsibble: 34,930 x 4 [1D]\n# Key:       name, station [4]\n   name             station date       meanTemp\n   &lt;chr&gt;            &lt;chr&gt;   &lt;date&gt;        &lt;dbl&gt;\n 1 Bergen - Florida SN50540 2000-01-01      6.3\n 2 Bergen - Florida SN50540 2000-01-02      6.3\n 3 Bergen - Florida SN50540 2000-01-03      6.7\n 4 Bergen - Florida SN50540 2000-01-04      4.6\n 5 Bergen - Florida SN50540 2000-01-05      4.6\n 6 Bergen - Florida SN50540 2000-01-06      6.5\n 7 Bergen - Florida SN50540 2000-01-07      6.3\n 8 Bergen - Florida SN50540 2000-01-08      6.4\n 9 Bergen - Florida SN50540 2000-01-09      4.1\n10 Bergen - Florida SN50540 2000-01-10      5.3\n# ℹ 34,920 more rows\n\ncitytemp.ts %&gt;% autoplot()\n\n\n\n\n\n\n\n\nRun the code above on your own. We will use the same data sets later, but for now, just check that you get the same figures.\nDo Exercise 3. All the code you need is given in the exercise. Note, you do not need to download the csv-file, but can load it directly to R using this link (replacing the first line of code):\n\n\ntute1 &lt;- readr::read_csv(\"https://bit.ly/fpptute1\")\n\n\nDo Exercise 4.\n\n\n# install.packages(\"USgas\")\nlibrary(USgas)\nhead(us_total)\n\n  year   state      y\n1 1997 Alabama 324158\n2 1998 Alabama 329134\n3 1999 Alabama 337270\n4 2000 Alabama 353614\n5 2001 Alabama 332693\n6 2002 Alabama 379343\n\n\n\n\nContinue with the temperature data from the largest cities in Norway. Create a\n\n\n\nTimeplot,\nSeasonal plot,\nSeasonal subseries plot.\n\n\n\nAggregate the temperature time series for the 4 largest cities in Norway from daily to weekly and monthly average temperatures. Create two illustrative figures for each.\nTemperatures in the Norwegian cities all follow the same seasonality (cold in winter - warm in summer). Create a graphic illustrating the correlation between these time series. Hint: GGally::ggpairs().\nCreate a autocorrelation plot of the temperature data for Bergen using default settings. Increase the maximum number of lags to 400. Interpret the latter plot. Is this a stationary time series?\nCreate a tsibble consisting of white noise (uncorrelated variables) of length 100. Create an autocorrelation plot for the time series you have simulated. Interpret the plot. Is this a stationary time series?\nThe Norwegian government has decided to work towards a goal of installing 30GW of offshore wind power. The two locations they have decided to start building the first wind parks is called Sørlige Nordsjø 2 and Utsira Nord. On Canvas, you find derived power production from modelled wind speed at these two locations on hourly time scale for 5 years.\n\n\n\nUsing the offshore wind power data, create illustrative figures for Utsira Nord and Sørlige Nordsjø 2 for different time scale aggregates (hourly, daily, weekly, monthly).\nCan you a detect trend/cycle/season based on your figures?\nWhat about the relationship between the two locations? Is the dependence linear?\nDoes your answer in (c) depend on the time scale you use?\nIf you were to decide where to build the first wind farm solely based on the data you have, which would you choose and why? Discuss with your neighbors.\n\n\n# Hint: \nwind &lt;- readRDS(\"OffshoreWindtwoLocationsFiveYears.rds\")\n\n\nCreate a tsibble containing the daily wind power data from Sørlige Nordsjø 2. Decompose the time series into three components; trend-cycle T_t, season S_t and remainder R_t, using a suitable decomposition method. Why did you choose the method you did? Is there any seasonal/trend patterns in the data?\nThe data used in this exercise is the wholesale and retail sales index from Statistics Norway. More specifically, the data is an index for Retail trade, except of motor vehicles and motorcycles. Data starts in Jan 2000 to what is presently available from Statistics Norway.\n\n\n\nLoad the data using e.g. read.csv2 indicated below. Convert the month column to a yearmonth type and wholesale as a tsibble.\n\n\nwholesale  &lt;- read.csv2(\n  \"https://raw.githubusercontent.com/holleland/BAN430/master/data/wholesale_and_retails_index_norway.csv\", \n  sep = \";\")\nhead(wholesale,3)\n\n    month wholesale_and_retail_sales_index\n1 2000M01                             50.5\n2 2000M02                             49.3\n3 2000M03                             53.8\n\n\n\nMake a time plot.\nDecompose the time series using the classical, x11, seats and STL methods. Can you detect any prominent differences between the methods?\nTry adjusting the trend and season windows of the STL. What happens? (default values are respectively 21 and 11).\nUsing your method of choice, plot the seasonally-adjusted time series.\nUsing your method of choice, plot the detrended series.\nOptional: Implement your own additive classical decomposition on this example (solution: see exercise 6).\n\n\nExercises from chapter 3: 1-3\n\nAdditional recommended exercises from chapter 2: 5, 7, 9 and 10"
  },
  {
    "objectID": "W3_traffic.html",
    "href": "W3_traffic.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "In this workshop we will build a model for forecasting traffic in Bergen’s busiest intersection Danmarksplass. The data is publicly available at vegvesen.no/trafikkdata and published by The Norwegian Public Roads Administration. Forecasting traffic amount could be useful for anyone wanting to avoid traffic jams, but also important for government agencies with responsibilities related to road planning or public transport.\nAs you may imagine the amount of traffic varies depending on things such as time of day, which day of the week it is and public holidays. The peak hours are mostly associated with people traveling to or home from work, while the least busy hours will be in the middle of the night. In this exercise you will use raw observations from The Norwegian Public Roads Administration to forecast traffic passing through one of Bergen’s traffic bottlenecks.\nHopefully you will learn about\n\nGraphical presentation of a time series\nBuilding a linear time series regression model\nUseful transformations\nUsing dummy variables for seasonality\nUsing fourier sequences for seasonality\nExternal predictor variables\n\n\n\n\nThe data you will be using consists of the two columns\n\ndatetime: date and hour of observation\nvehicles: the number of vehicles that has passed a sensor in the road the previous hour\n\nThe observations are hourly and start from January 2018 to February 1st 2023 and are all from Danmarksplass (near the charging station). We add another location as predictor for Danmarksplass later in the case study to see if this may inform our forecasting model, but this data has the same structure (except that the vehicle column has a different name).\n\n\n\n\nLoad the data into your working environment. Make sure the columns are correctly formatted.\n\n\ndanmarksplass &lt;- readr::read_csv(\n  \"https://raw.githubusercontent.com/holleland/BAN430/master/data/traffic_danmarksplass.csv\")\n\n\nConvert the data to a tsibble and make a time plot.\n\n\nCreate a training data set. We will use January 2023 as our test set. Make a time plot with one panel per year for the training data. Can you detect any particular patterns or deviations from the patterns?\n\n\n\nCode hint:\ntrain &lt;- danmarksplass %&gt;% filter(year(datetime) &lt; 2023)\n\n\n\nUsing different graphics, visualize the training data. Look for trends and seasons on multiple scales.\nIn particular, create this graphic. What do you detect here?\n\n\n\nCode solution:\ntrain %&gt;% mutate(wday = wday(datetime, label =T, abbr = FALSE, week_start=1), \n                 hour = hour(datetime),\n                 date=date(datetime)) %&gt;%\n  ggplot(aes(x = hour, y = vehicles, col = date, group = date)) + geom_line() +\n  facet_wrap( ~ wday)+ scale_color_date(low = \"blue\", high = \"magenta\")\n\n\n\n\n\n\nAggregate the data to total traffic by year-week and do a STL decomposition. Do you have an plausible explanation for the estimated trend? Try reducing the trend window (default is 91).\n\n\n\nCode hint:\ntrain %&gt;% \n  index_by(date = ~yearweek(.)) %&gt;% \n  summarize(vehicles = sum(vehicles,na.rm=T))%&gt;% \n  model(stl = STL(vehicles ~ trend(window =91))\n\n\n\nFit a TSLM with weekly season as predictor (dummy variables for every hour of the week). Fit one model without transforming and one where you log-transform the forecast variable. Forecast for the first week of 2023 and plot it for the two models. Would you prefer to use the log-transform or no transform? Explain why.\n\n\n\nCode hint\nlog(vehicle) ~ season(period = \"week\")\n\n\n\nIn the model above, we used 24\\cdot 7= 168 predictors. We can do this, because we have so many data points. However, could we achieve something similar by using a fourier series? If K&lt;168/2 = 84, this will reduce the number of parameters. Use the same transformation as you favored above and compare the model with a model using fourier predictors for the weekly season. Try different values of K and forecast for the first week of 2023. Evaluate the accuracy of the two models with AIC, AICc and CV.\n\n\n\nCode hint:\nvehicles ~ fourier(K = 65, period = \"week\")\n\n\n\nBased on the number of observations here (T\\approx 45\\,000), which of AIC, AICc or CV would you prefer to use and why?\nOne issue you might see is that the model over-estimates the traffic on January 1st (which in 2023 is a Sunday, but it is also a public holiday). Could there be a public holiday effect? By loading the csv linked below, you will have a list of Norwegian holidays from 2008-Jan 2023. Add a dummy variable to your model of choice for public holiday (1 if holiday, 0 otherwise). Does this improve the model fit measures (AIC)?\n\n\nholidays &lt;- read_csv(\n  \"https://raw.githubusercontent.com/holleland/BAN430/master/data/NorwegianHolidays_2008-jan2023.csv\")$x\n\n\n\nCode hint:\nholiday &lt;- readr::read_csv(\n  \"https://raw.githubusercontent.com/holleland/BAN430/master/data/NorwegianHolidays_2008-jan2023.csv\")$x\ndanmarksplass &lt;- danmarksplass %&gt;% mutate(\n  holiday = factor(ifelse(date(datetime) %in% holidays, 1, 0))\n)\ntrain &lt;- danmarksplass %&gt;% filter(year(datetime) &lt; 2023)\ntrain %&gt;% \n  model(holiday = TSLM(log(vehicle) ~ factor(holiday) + ...))\n\n\n\nAre there other dummies you could think of adding? Special events that might impact the traffic on a certain day or during a period (spike dummy) or permanently changing it from a certain point in time (step dummy)? Discuss.\nFit a seasonal naive as benchmark model and compare it to the best model so far using RMSE, MAE, MAPE and MASE on the test set.\n\n\n\nCode hint:\nfit &lt;- model(\n  m1 = ...,\n  snaive = SNAIVE(...)\n)\naccuracy(fit)\n\n\n\nOne hypothesis is that some of the traffic at Danmarkplass can be explained by the arrival of ferries at Halhjem, creating a peak in the number of cars arriving. This will only contribute to the traffic heading North, but let us test by adding a predictor for traffic at Moberg v/Lekven (a station on the E39 immediately after the ferry). We will only use traffic towards Bergen here. The traffic heading North at Moberg can be found in traffic_moberg.csv (link below). Join it with the danmarksplass data (one tsibble with both columns). At Moberg there are some zero measurement. It may be useful to use log(1+moberg). Since it takes some time to drive from Moberg to Danmarksplass it may also be useful to use lagged observations at Moberg (hint: Use lag() function). Evaluate whether adding Moberg improves AIC.\n\n\nmoberg &lt;- readr::read_csv(\"https://raw.githubusercontent.com/holleland/BAN430/master/data/traffic_moberg.csv\") \n\n\n\nCode hint:\ndanmarksplass &lt;- left_join(danmarksplass,\n                           moberg, \n                           by = \"datetime\")\ntrain &lt;- danmarksplass %&gt;% filter(year(datetime) &lt; 2023)\ntrain %&gt;% \n  model(holiday = TSLM(log(vehicle) ~ lag(log(1+moberg)) + factor(holiday) + ...))\n\n\n\nThe way you have implemented the Moberg traffic in your forecast model, is it an ex ante or ex post forecast?\nLet us have a look at the residuals. Comment on potential issues.\nIncrease the forecast interval to include all of January 2023. Does this change any of the conclusions about the models? How does the forecast look?\nCan you think of other predictors to add to the model. Can you improve the model performance further?"
  },
  {
    "objectID": "W3_traffic.html#ws3-bergen-traffic-forecast",
    "href": "W3_traffic.html#ws3-bergen-traffic-forecast",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "In this workshop we will build a model for forecasting traffic in Bergen’s busiest intersection Danmarksplass. The data is publicly available at vegvesen.no/trafikkdata and published by The Norwegian Public Roads Administration. Forecasting traffic amount could be useful for anyone wanting to avoid traffic jams, but also important for government agencies with responsibilities related to road planning or public transport.\nAs you may imagine the amount of traffic varies depending on things such as time of day, which day of the week it is and public holidays. The peak hours are mostly associated with people traveling to or home from work, while the least busy hours will be in the middle of the night. In this exercise you will use raw observations from The Norwegian Public Roads Administration to forecast traffic passing through one of Bergen’s traffic bottlenecks.\nHopefully you will learn about\n\nGraphical presentation of a time series\nBuilding a linear time series regression model\nUseful transformations\nUsing dummy variables for seasonality\nUsing fourier sequences for seasonality\nExternal predictor variables\n\n\n\n\nThe data you will be using consists of the two columns\n\ndatetime: date and hour of observation\nvehicles: the number of vehicles that has passed a sensor in the road the previous hour\n\nThe observations are hourly and start from January 2018 to February 1st 2023 and are all from Danmarksplass (near the charging station). We add another location as predictor for Danmarksplass later in the case study to see if this may inform our forecasting model, but this data has the same structure (except that the vehicle column has a different name).\n\n\n\n\nLoad the data into your working environment. Make sure the columns are correctly formatted.\n\n\ndanmarksplass &lt;- readr::read_csv(\n  \"https://raw.githubusercontent.com/holleland/BAN430/master/data/traffic_danmarksplass.csv\")\n\n\nConvert the data to a tsibble and make a time plot.\n\n\nCreate a training data set. We will use January 2023 as our test set. Make a time plot with one panel per year for the training data. Can you detect any particular patterns or deviations from the patterns?\n\n\n\nCode hint:\ntrain &lt;- danmarksplass %&gt;% filter(year(datetime) &lt; 2023)\n\n\n\nUsing different graphics, visualize the training data. Look for trends and seasons on multiple scales.\nIn particular, create this graphic. What do you detect here?\n\n\n\nCode solution:\ntrain %&gt;% mutate(wday = wday(datetime, label =T, abbr = FALSE, week_start=1), \n                 hour = hour(datetime),\n                 date=date(datetime)) %&gt;%\n  ggplot(aes(x = hour, y = vehicles, col = date, group = date)) + geom_line() +\n  facet_wrap( ~ wday)+ scale_color_date(low = \"blue\", high = \"magenta\")\n\n\n\n\n\n\nAggregate the data to total traffic by year-week and do a STL decomposition. Do you have an plausible explanation for the estimated trend? Try reducing the trend window (default is 91).\n\n\n\nCode hint:\ntrain %&gt;% \n  index_by(date = ~yearweek(.)) %&gt;% \n  summarize(vehicles = sum(vehicles,na.rm=T))%&gt;% \n  model(stl = STL(vehicles ~ trend(window =91))\n\n\n\nFit a TSLM with weekly season as predictor (dummy variables for every hour of the week). Fit one model without transforming and one where you log-transform the forecast variable. Forecast for the first week of 2023 and plot it for the two models. Would you prefer to use the log-transform or no transform? Explain why.\n\n\n\nCode hint\nlog(vehicle) ~ season(period = \"week\")\n\n\n\nIn the model above, we used 24\\cdot 7= 168 predictors. We can do this, because we have so many data points. However, could we achieve something similar by using a fourier series? If K&lt;168/2 = 84, this will reduce the number of parameters. Use the same transformation as you favored above and compare the model with a model using fourier predictors for the weekly season. Try different values of K and forecast for the first week of 2023. Evaluate the accuracy of the two models with AIC, AICc and CV.\n\n\n\nCode hint:\nvehicles ~ fourier(K = 65, period = \"week\")\n\n\n\nBased on the number of observations here (T\\approx 45\\,000), which of AIC, AICc or CV would you prefer to use and why?\nOne issue you might see is that the model over-estimates the traffic on January 1st (which in 2023 is a Sunday, but it is also a public holiday). Could there be a public holiday effect? By loading the csv linked below, you will have a list of Norwegian holidays from 2008-Jan 2023. Add a dummy variable to your model of choice for public holiday (1 if holiday, 0 otherwise). Does this improve the model fit measures (AIC)?\n\n\nholidays &lt;- read_csv(\n  \"https://raw.githubusercontent.com/holleland/BAN430/master/data/NorwegianHolidays_2008-jan2023.csv\")$x\n\n\n\nCode hint:\nholiday &lt;- readr::read_csv(\n  \"https://raw.githubusercontent.com/holleland/BAN430/master/data/NorwegianHolidays_2008-jan2023.csv\")$x\ndanmarksplass &lt;- danmarksplass %&gt;% mutate(\n  holiday = factor(ifelse(date(datetime) %in% holidays, 1, 0))\n)\ntrain &lt;- danmarksplass %&gt;% filter(year(datetime) &lt; 2023)\ntrain %&gt;% \n  model(holiday = TSLM(log(vehicle) ~ factor(holiday) + ...))\n\n\n\nAre there other dummies you could think of adding? Special events that might impact the traffic on a certain day or during a period (spike dummy) or permanently changing it from a certain point in time (step dummy)? Discuss.\nFit a seasonal naive as benchmark model and compare it to the best model so far using RMSE, MAE, MAPE and MASE on the test set.\n\n\n\nCode hint:\nfit &lt;- model(\n  m1 = ...,\n  snaive = SNAIVE(...)\n)\naccuracy(fit)\n\n\n\nOne hypothesis is that some of the traffic at Danmarkplass can be explained by the arrival of ferries at Halhjem, creating a peak in the number of cars arriving. This will only contribute to the traffic heading North, but let us test by adding a predictor for traffic at Moberg v/Lekven (a station on the E39 immediately after the ferry). We will only use traffic towards Bergen here. The traffic heading North at Moberg can be found in traffic_moberg.csv (link below). Join it with the danmarksplass data (one tsibble with both columns). At Moberg there are some zero measurement. It may be useful to use log(1+moberg). Since it takes some time to drive from Moberg to Danmarksplass it may also be useful to use lagged observations at Moberg (hint: Use lag() function). Evaluate whether adding Moberg improves AIC.\n\n\nmoberg &lt;- readr::read_csv(\"https://raw.githubusercontent.com/holleland/BAN430/master/data/traffic_moberg.csv\") \n\n\n\nCode hint:\ndanmarksplass &lt;- left_join(danmarksplass,\n                           moberg, \n                           by = \"datetime\")\ntrain &lt;- danmarksplass %&gt;% filter(year(datetime) &lt; 2023)\ntrain %&gt;% \n  model(holiday = TSLM(log(vehicle) ~ lag(log(1+moberg)) + factor(holiday) + ...))\n\n\n\nThe way you have implemented the Moberg traffic in your forecast model, is it an ex ante or ex post forecast?\nLet us have a look at the residuals. Comment on potential issues.\nIncrease the forecast interval to include all of January 2023. Does this change any of the conclusions about the models? How does the forecast look?\nCan you think of other predictors to add to the model. Can you improve the model performance further?"
  }
]