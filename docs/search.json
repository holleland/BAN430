[
  {
    "objectID": "10_Practicle_forecasting_issues.html",
    "href": "10_Practicle_forecasting_issues.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Practicle forecasting issues\n\n1+2\n\n[1] 3"
  },
  {
    "objectID": "12_data_appendix.html",
    "href": "12_data_appendix.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Data\nYou can find a overview of the data that is included in the textbook package fpp3 by running the following code:\n\ndata(package = \"fpp3\")\n\n\n\n# A tibble: 13 × 2\n   Item              Title                                               \n   <chr>             <chr>                                               \n 1 aus_accommodation Australian accommodation data                       \n 2 aus_airpassengers Air Transport Passengers Australia                  \n 3 aus_arrivals      International Arrivals to Australia                 \n 4 bank_calls        Call volume for a large North American bank         \n 5 boston_marathon   Boston marathon winning times since 1897            \n 6 canadian_gas      Monthly Canadian gas production                     \n 7 guinea_rice       Rice production (Guinea)                            \n 8 insurance         Insurance quotations and advertising expenditure    \n 9 prices            Price series for various commodities                \n10 souvenirs         Sales for a souvenir shop                           \n11 us_change         Percentage changes in economic variables in the USA.\n12 us_employment     US monthly employment data                          \n13 us_gasoline       US finished motor gasoline product supplied.        \n\n\nThese are avaiable when the fpp3 package is loaded, i.e.\n\nlibrary(fpp3)\nbank_calls\n\n# A tsibble: 27,716 x 2 [5m] <UTC>\n   DateTime            Calls\n   <dttm>              <dbl>\n 1 2003-03-03 07:00:00   111\n 2 2003-03-03 07:05:00   113\n 3 2003-03-03 07:10:00    76\n 4 2003-03-03 07:15:00    82\n 5 2003-03-03 07:20:00    91\n 6 2003-03-03 07:25:00    87\n 7 2003-03-03 07:30:00    75\n 8 2003-03-03 07:35:00    89\n 9 2003-03-03 07:40:00    99\n10 2003-03-03 07:45:00   125\n# … with 27,706 more rows\n\n\nTo load a specific data set explicitly in your R environment:\n\ndata(\"bank_calls\")\n\nOther examples used in the videos and content on this website is available for download at github.com/holleland/BAN430/tree/master/data. You should also be able to load them directly into R using the raw link:\n\n# CPI Norway\nread.csv(\n  \"https://raw.githubusercontent.com/holleland/BAN430/master/data/CPI_norway.csv\", sep = \";\") %>% head()\n\n     X Y.avg2   Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov\n1 2022      . 117.8 119.1 119.8 121.2 121.5 122.6 124.2 123.9 125.6     .     .\n2 2021  116.1 114.1 114.9 114.6 115.0 114.9 115.3 116.3 116.3 117.5 117.2 118.1\n3 2020  112.2 111.3 111.2 111.2 111.7 111.9 112.1 112.9 112.5 112.9 113.2 112.4\n4 2019  110.8 109.3 110.2 110.4 110.8 110.5 110.6 111.4 110.6 111.1 111.3 111.6\n5 2018  108.4 106.0 107.0 107.3 107.7 107.8 108.5 109.3 108.9 109.5 109.3 109.8\n6 2017  105.5 104.3 104.7 105.0 105.2 105.4 105.8 106.1 105.3 105.9 106.0 106.1\n    Dec\n1     .\n2 118.9\n3 112.9\n4 111.3\n5 109.8\n6 106.1\n\n\nFor excel files it is a bit less convenient, because you will need to download the file. But you can let R do that for you (if you insist).\n\nloadExcel_url <- function(url) {\n    temp_file <- tempfile(fileext = \".xlsx\")\n    download.file(url = url, destfile = temp_file, mode = \"wb\", quiet = TRUE)\n    readxl::read_excel(temp_file)\n}\nloadExcel_url(\"https://github.com/holleland/BAN430/blob/master/data/NorwayEmployment_15-74years_bySex.xlsx?raw=true\")\n\n# A tibble: 214 × 3\n   Sex   Quarter `Employed persons (1 000 persons)`\n   <chr> <chr>                                <dbl>\n 1 Male  1996K1                                1133\n 2 Male  1996K2                                1152\n 3 Male  1996K3                                1171\n 4 Male  1996K4                                1161\n 5 Male  1997K1                                1164\n 6 Male  1997K2                                1189\n 7 Male  1997K3                                1200\n 8 Male  1997K4                                1189\n 9 Male  1998K1                                1195\n10 Male  1998K2                                1214\n# … with 204 more rows\n\n\nThe code above will save the file temporary in your computers temporary folder and load it into R from there. You could also adjust the code so that it stores the file in your working directory by adjusting the function.\n\ntemp_file <- paste0(getwd(), \"/NorwayEmployment.xlsx\")\n\nBut the easiest will maybe be to just download the files manually from github and save them in a data folder of your own."
  },
  {
    "objectID": "1_1_readingData.html",
    "href": "1_1_readingData.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "This is a book created from markdown and executable code."
  },
  {
    "objectID": "1_1_readingData.html#exercises",
    "href": "1_1_readingData.html#exercises",
    "title": "BAN430 Forecasting",
    "section": "Exercises",
    "text": "Exercises\nIs 1+1 the same as 2?\n\n\nSolution\n\n\n1+1 == 2"
  },
  {
    "objectID": "1_installingrandrstudio.html",
    "href": "1_installingrandrstudio.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "R and Rstudio\nIn this course, we will be using R and Rstudio to e.g. visualize time series, estimate model parameters, forecast, etc. It is therefore essential to have some basic knowledge of how to write an R script and how to read in data and do some simple data manipulation for preparing the data for different time series analysis. Hopefully, most of you have some experience with R and Rstudio before. If you have, this will be a short recap, if not this will be a very short introduction covering the most basic operations.\n\nInstalling R and Rstudio\n\nInstall R:\n\nGo to: cran.uib.no\nPress download R for Linux/MacOS/Windows\nPress base\nDownload R-4.x.x for Linux/MacOS/Windows\nRun the installation using default options\n\nInstall Rstudio\n\nGo to: rstudio.com\nSelect Rstudio desktop\nPress Download Rstudio desktop\nSelect the Rstudio desktop with open source licence, which is free\n\nSelect the version for your operating system\nRun the installation using default settings\n\nOpen Rstudio and check that it works (it should start without any error messages).\nInstall the R-package of the book “fpp3”.\n\nIn Rstudio, select Tools -> Install packages -> write “fpp3” and make sure install dependencies is marked. Press Install. You can also run the following code in the console\n\n\n\ninstall.packages(\"fpp3\", dependencies = TRUE)\n\n\n\nOther useful packages to install are\n\ninstall.packages(\"tidyverse\", dependencies = TRUE)\ninstall.packages(\"readxl\")\n\n(will add other packages as we go)"
  },
  {
    "objectID": "1_randrstudio.html",
    "href": "1_randrstudio.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "In this course, we will be using R and Rstudio to e.g. visualize time series, estimate model parameters, forecast, etc. It is therefore essential to have some basic knowledge of how to write an R script and how to read in data and do some simple data manipulation for preparing the data for different time series analysis. Hopefully, most of you have some experience with R and Rstudio before. If you have, this will be a short recap, if not this will be a very short introduction covering the most basic operations.\n\n\n\nInstall R:\n\nGo to: cran.uib.no\nPress download R for Linux/MacOS/Windows\nPress base\nDownload R-4.x.x for Linux/MacOS/Windows\nRun the installation using default options\n\nInstall Rstudio\n\nGo to: rstudio.com\nSelect Rstudio desktop\nPress Download Rstudio desktop\nSelect the Rstudio desktop with open source licence, which is free\n\nSelect the version for your operating system\nRun the installation using default settings\n\nOpen Rstudio and check that it works (it should start without any error messages).\nInstall the R-package of the book “fpp3”.\n\nIn Rstudio, select Tools -> Install packages -> write “fpp3” and make sure install dependencies is marked. Press Install. You can also run the following code in the console\n\n\n\ninstall.packages(\"fpp3\", dependencies = TRUE)\n\n\n\nOther useful packages to install are\n\ninstall.packages(\"tidyverse\", dependencies = TRUE)\ninstall.packages(\"readxl\")\n\n(will add other packages as we go)\n\n\n\nIf R and Rstudio are completely new tools for you, this section will probably not be detailed enough to get you started, but fear not. There are lots of good and useful online material for learning basic R. One possibility is to work through the first section of (chapter 1-8) of the book R for data science by @wickham2016r available online. There is also a free Coursera course on R programming, recommended by the textbook authors.\nWe will mostly be using the tidyverse approach to doing data manipulation. This is in line with what you learn in courses like BAN400 R programming for Data Science or BAN420 Introduction to R and also with what the authors of the textbook does (@hyndman2018).\nSay you are given an .xlsx file (MS excel format) of daily prices of an US 10 year Treasury bond. The excel file contains several sheets with the\n\nClosing ask price (“Ask”)\nClosing bid price (“Bid”)\nClosing mid price (“Mid”)\n\nEach contains two columns: date and price. In the figure below we have taken a screen shot of the Mid sheet.\n\n\n\nUS 10-year Treasury bonds index collected from the Refinitiv Eikon data base.\n\n\nYou are interesting in reading in the closing mid price. To read in this data, you may use the following code.\n\nlibrary(fpp3)   # loading textbook package\nlibrary(tidyverse)\nlibrary(readxl) # loading package for reading excel files\ndat <- read_excel(\"data/US10YTRR.xlsx\", sheet = \"Mid\")  \nhead(dat) # printing out the first 6 rows\n\n# A tibble: 6 × 2\n  date                price\n  <dttm>              <dbl>\n1 2022-08-30 00:00:00  96.9\n2 2022-08-29 00:00:00  96.9\n3 2022-08-26 00:00:00  97.6\n4 2022-08-25 00:00:00  97.6\n5 2022-08-24 00:00:00  96.9\n6 2022-08-23 00:00:00  97.4\n\n\nThe sheet argument specifies which sheet in the excel file we want to read. The read_excel function is also quite smart so it recognizes that the date column is a date and automatically format it accordingly. It is however perhaps not so useful to also include the time of the day (all is 00:00:00), so let us remove this part.\n\ndat %>% \n  mutate(date = as.Date(date))\n\n# A tibble: 8,804 × 2\n   date       price\n   <date>     <dbl>\n 1 2022-08-30  96.9\n 2 2022-08-29  96.9\n 3 2022-08-26  97.6\n 4 2022-08-25  97.6\n 5 2022-08-24  96.9\n 6 2022-08-23  97.4\n 7 2022-08-22  97.7\n 8 2022-08-19  98.1\n 9 2022-08-18  98.8\n10 2022-08-17  98.7\n# … with 8,794 more rows\n\n\nHere I have used the mutate function. This is a function we use to either mutate an existing column or create a new one. In this case we mutated the date column transforming it to a “Date” object. We could also be intersted in adding a column for which year the observation is from.\n\ndat %>% \n  mutate(date = as.Date(date),\n         year = year(date))\n\n# A tibble: 8,804 × 3\n   date       price  year\n   <date>     <dbl> <dbl>\n 1 2022-08-30  96.9  2022\n 2 2022-08-29  96.9  2022\n 3 2022-08-26  97.6  2022\n 4 2022-08-25  97.6  2022\n 5 2022-08-24  96.9  2022\n 6 2022-08-23  97.4  2022\n 7 2022-08-22  97.7  2022\n 8 2022-08-19  98.1  2022\n 9 2022-08-18  98.8  2022\n10 2022-08-17  98.7  2022\n# … with 8,794 more rows\n\n\nHere we have used the year function from the lubridate package, which is loaded with the fpp3 package. The operator %>% is used to add operations to the data manipulation pipeline in the given order. We start with the data object (a tibble) and add a mutate operation to that where we first transform the date column and add a year column. Now that we are pleased with our pipeline, let us save this to the dat object.\n\ndat <- dat %>% \n  mutate(date = as.Date(date),\n         year = year(date))\ndat %>% glimpse()\n\nRows: 8,804\nColumns: 3\n$ date  <date> 2022-08-30, 2022-08-29, 2022-08-26, 2022-08-25, 2022-08-24, 202…\n$ price <dbl> 96.92969, 96.92188, 97.62500, 97.60156, 96.94531, 97.38281, 97.6…\n$ year  <dbl> 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022…\n\n\nThe glimpse function summarizes the tibble/data frame.\n\n\nNow, the data ranges from/to\n\nrange(dat$date)\n\n[1] \"1987-08-03\" \"2022-08-30\"\n\n\nbut say you only want to use data from 2010 onwards. To do this, we use the filter function. This function is useful for selecting rows that fulfil some condition, in this case year >= 2010. Let us make a pipeline for this\n\ndat %>% \n  filter(year >= 2010)\n\n# A tibble: 3,178 × 3\n   date       price  year\n   <date>     <dbl> <dbl>\n 1 2022-08-30  96.9  2022\n 2 2022-08-29  96.9  2022\n 3 2022-08-26  97.6  2022\n 4 2022-08-25  97.6  2022\n 5 2022-08-24  96.9  2022\n 6 2022-08-23  97.4  2022\n 7 2022-08-22  97.7  2022\n 8 2022-08-19  98.1  2022\n 9 2022-08-18  98.8  2022\n10 2022-08-17  98.7  2022\n# … with 3,168 more rows\n\n\nSince 2022 is not a complete year (in the data), you also don’t want observations after 2021. Then you can add this as an extra condition.\n\ndat %>% \n  filter(year >= 2010, year <=2021)\n\n# A tibble: 3,012 × 3\n   date       price  year\n   <date>     <dbl> <dbl>\n 1 2021-12-31  98.8  2021\n 2 2021-12-30  98.8  2021\n 3 2021-12-29  98.4  2021\n 4 2021-12-28  99.0  2021\n 5 2021-12-27  99.1  2021\n 6 2021-12-23  98.9  2021\n 7 2021-12-22  99.3  2021\n 8 2021-12-21  99.2  2021\n 9 2021-12-20  99.5  2021\n10 2021-12-17  99.7  2021\n# … with 3,002 more rows\n\n\nAlternatively, you can use the between function\n\ndat %>% \n  filter(between(year, 2010, 2021))\n\nwhich will produce the same result. Another useful function is called select. While filter is used on the rows of your data, select is for columns. Say we don’t need the year column after having filtered out the years we don’t want. We can then either select the columns we want to keep\n\ndat %>% \n  filter(between(year, 2010, 2021)) %>%\n  select(date, price)\n\nor remove the columns we do not want\n\ndat %>% \n  filter(between(year, 2010, 2021)) %>%\n  select(-year)\n\n# A tibble: 3,012 × 2\n   date       price\n   <date>     <dbl>\n 1 2021-12-31  98.8\n 2 2021-12-30  98.8\n 3 2021-12-29  98.4\n 4 2021-12-28  99.0\n 5 2021-12-27  99.1\n 6 2021-12-23  98.9\n 7 2021-12-22  99.3\n 8 2021-12-21  99.2\n 9 2021-12-20  99.5\n10 2021-12-17  99.7\n# … with 3,002 more rows\n\n\n\n\n\nSay we are interested in calculating the yearly mean price. In the tidyverse pipeline this means we want to group our observations according to year and summarize by year the mean of the observations. We will filter to avoid having the first and last years that are incomplete.\n\ndat %>% \n  filter(between(year, 1988, 2021)) %>%\n  group_by(year) %>%\n  summarize(meanPrice = mean(price))\n\n# A tibble: 34 × 2\n    year meanPrice\n   <dbl>     <dbl>\n 1  1988      99.7\n 2  1989     101. \n 3  1990     100. \n 4  1991     100. \n 5  1992     100. \n 6  1993     102. \n 7  1994      98.3\n 8  1995     102. \n 9  1996      99.8\n10  1997      99.9\n# … with 24 more rows\n\n\nThis pipeline could be read as first we take out observations prior to 1988 and after 2021, then we group the observations according to year and summarize the mean price by year. Note that this operation will delete any columns that are not in the group_by or being calculated in the summarize.\n\n\n\nPlotting a data frame is convenient to do using the ggplot2 package. This will (when used appropriately) produce beautiful figures. Let us plot the time series at hand. The ggplot2 follows the same logic with a pipeline, but instead of the %>% operator, we add elements to the figure using +. We need to specify the data object and the name of the x and y columns to be plotted. Everything in the figure that is to vary based on values in the data frame needs to be wrapped in a aes (aesthetic) function (here the x and y arguments). By adding the geom_line() we insert a line.\n\nggplot(data = dat, \n       aes(x=date, y = price)) +\n  geom_line()\n\n\n\n\nWe could instead add geom_point()\n\n ggplot(data = dat,\n        aes(x=date, y = price)) +\n   geom_point()\n\n\n\n\nor do both\n\n ggplot(data = dat,\n        aes(x=date, y = price)) +\n   geom_line() +\n   geom_point()\n\n\n\n\nWe can change the colors and decrease the size of the points:\n\n ggplot(data = dat,\n        aes(x=date, y = price)) +\n   geom_line(color = \"blue\") +\n   geom_point(color = \"green\", size = .2)\n\n\n\n\nOr maybe we do not want to use the default theme: –>\n\n ggplot(data = dat,\n        aes(x=date, y = price)) +\n   geom_line(color = \"blue\") +\n   geom_point(color = \"green\", size = .2) +\n   theme_bw()\n\n\n\n\nWe can also include the plotting in our data manipulation pipeline. For instance, lets summarize the data by year and plot the resulting yearly time series.\n\ndat %>%\n  filter(between(year, 1988, 2021)) %>%\n  group_by(year) %>%\n  summarize(meanPrice = mean(price)) %>%\n  # adding plotting to pipeline:\n  ggplot(aes(x=year, y = meanPrice)) +\n  geom_line(color = \"blue\") +\n  geom_point(color = \"green\") +\n  theme_bw()\n\n\n\n\n\n\n\nWe cannot illustrate all aspects here, but you will learn new elements by studying examples throughout the course. This recap is mostly for remembering the basics of data manipulation in R and simple plotting. As you will see in the continuation, the coding is not much more complex then what you have seen here and the fpp3 package uses the same type of logic and syntax as the tidyverse. There will however be some new functions specific for time series analysis that you will need to learn.\n\n\n\n\nSet working directory.\nLoad the data.\nFilter away observations prior to 2010.\nRemove columns except .. and time\nSummarize data to monthly means\nMake a plot with time on x-axis and monthly means on y-axis\nSave the figure to file."
  },
  {
    "objectID": "1_rrecap.html",
    "href": "1_rrecap.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "If R and Rstudio are completely new tools for you, this section will probably not be detailed enough to get you started, but fear not. There are lots of good and useful online material for learning basic R. One possibility is to work through the first section (chapter 1-8) of the book R for data science by Wickham and Grolemund available online. There is also a free Coursera course on R programming, recommended by the textbook authors. The course BAN420 is recommended for taking this course. If you have not completed that course, you will find the material as videos on the BAN420 website. I recommend going through this material if you do not know it already. What follows below is a (very) short version of parts of Tuesday-Wednesday.\nWe will be using the tidyverse approach to doing data manipulation. This is in line with what you learn in courses like BAN400 R programming for Data Science or BAN420 Introduction to R and also with what the authors of the textbook does (Hyndman and Athanasopoulos,2021).\nOn canvas, you will find an excel file called US10YTRR.xlsx in the R recap folder. If you have experience with R, try to solve the following exercise without looking at the suggested solution.\n\n\nYou are given an .xlsx file (MS excel format) of daily prices of an US 10 year Treasury bond. The excel file contains several sheets with the\n\nClosing ask price (“Ask”)\nClosing bid price (“Bid”)\nClosing mid price (“Mid”)\n\nEach contains two columns: date and price. In the figure below we have taken a screen shot of the Mid sheet.\n\n\n\nUS 10-year Treasury bonds index collected from the Refinitiv Eikon data base.\n\n\nDo the following:\n\nSet working directory.\nLoad the data for the Mid price.\nFormat the date column in Date format.\nAdd a column for which year the observation is from.\nFilter away observations prior to 2010 and after 2021.\nRemove columns except date and price.\nSummarize data to monthly mean prices (hint: use tsibble::yearmonth function - this might be new to you!).\nMake a plot with months on x-axis and monthly means on y-axis.\nSave the figure to file.\n\nIf you get stuck, check out the (“hidden”) suggested solutions below.\n\n\nShow suggested solutions\n\nFirst, you need to set your working directory to the folder where you have saved the downloaded file and where you want to save the final figure. This can be done using the user interface in RStudio (“Session” -> “Set working directory” -> …) or using the command\n\nsetwd(\"../path/to/the/folder/\")\n\nYou are interested in reading in the closing mid price. To read in this data, you may use the following code.\n\nlibrary(fpp3)   # loading textbook package\nlibrary(tidyverse)\nlibrary(readxl) # loading package for reading excel files\ndat <- read_excel(\"data/US10YTRR.xlsx\", sheet = \"Mid\")  \nhead(dat) # printing out the first 6 rows\n\n# A tibble: 6 × 2\n  date                price\n  <dttm>              <dbl>\n1 2022-08-30 00:00:00  96.9\n2 2022-08-29 00:00:00  96.9\n3 2022-08-26 00:00:00  97.6\n4 2022-08-25 00:00:00  97.6\n5 2022-08-24 00:00:00  96.9\n6 2022-08-23 00:00:00  97.4\n\n\nThe sheet argument specifies which sheet in the excel file we want to read. The read_excel function is also quite smart so it recognizes that the date column is a date and automatically format it accordingly. It is however perhaps not so useful to also include the time of the day (all is 00:00:00), so let us remove this part.\n\ndat %>% \n  mutate(date = as.Date(date))\n\n# A tibble: 8,804 × 2\n   date       price\n   <date>     <dbl>\n 1 2022-08-30  96.9\n 2 2022-08-29  96.9\n 3 2022-08-26  97.6\n 4 2022-08-25  97.6\n 5 2022-08-24  96.9\n 6 2022-08-23  97.4\n 7 2022-08-22  97.7\n 8 2022-08-19  98.1\n 9 2022-08-18  98.8\n10 2022-08-17  98.7\n# … with 8,794 more rows\n\n\nHere I have used the mutate function. This is a function we use to either mutate (change) an existing column or create a new one. In this case we mutated the date column transforming it to a “Date” object/format. We could also be interested in adding a column for which year the observation is from.\n\ndat %>% \n  mutate(date = as.Date(date),\n         year = year(date))\n\n# A tibble: 8,804 × 3\n   date       price  year\n   <date>     <dbl> <dbl>\n 1 2022-08-30  96.9  2022\n 2 2022-08-29  96.9  2022\n 3 2022-08-26  97.6  2022\n 4 2022-08-25  97.6  2022\n 5 2022-08-24  96.9  2022\n 6 2022-08-23  97.4  2022\n 7 2022-08-22  97.7  2022\n 8 2022-08-19  98.1  2022\n 9 2022-08-18  98.8  2022\n10 2022-08-17  98.7  2022\n# … with 8,794 more rows\n\n\nHere we have used the year function from the lubridate package, which is loaded with the fpp3 package. The operator %>% is used to add operations to the data manipulation pipeline in the given order. We start with the data object (a tibble) and add a mutate operation to that where we first transform the date column and add a year column. Now that we are pleased with our pipeline, let us save this to the dat object.\n\ndat <- dat %>% \n  mutate(date = as.Date(date),\n         year = year(date))\ndat %>% glimpse()\n\nRows: 8,804\nColumns: 3\n$ date  <date> 2022-08-30, 2022-08-29, 2022-08-26, 2022-08-25, 2022-08-24, 202…\n$ price <dbl> 96.92969, 96.92188, 97.62500, 97.60156, 96.94531, 97.38281, 97.6…\n$ year  <dbl> 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022…\n\n\nThe glimpse function summarizes the tibble/data frame.\nfilter and select\nNow, the data ranges from/to\n\nrange(dat$date)\n\n[1] \"1987-08-03\" \"2022-08-30\"\n\n\nbut you only want to use data from 2010 onwards. To do this, we use the filter function. This function is useful for selecting rows that fulfill some condition, in this case year >= 2010. Let us make a pipeline for this\n\ndat %>% \n  filter(year >= 2010)\n\n# A tibble: 3,178 × 3\n   date       price  year\n   <date>     <dbl> <dbl>\n 1 2022-08-30  96.9  2022\n 2 2022-08-29  96.9  2022\n 3 2022-08-26  97.6  2022\n 4 2022-08-25  97.6  2022\n 5 2022-08-24  96.9  2022\n 6 2022-08-23  97.4  2022\n 7 2022-08-22  97.7  2022\n 8 2022-08-19  98.1  2022\n 9 2022-08-18  98.8  2022\n10 2022-08-17  98.7  2022\n# … with 3,168 more rows\n\n\nSince 2022 is not a complete year (in the data), you also do not want observations after 2021. Then you can add this as an extra condition.\n\ndat %>% \n  filter(year >= 2010, year <=2021)\n\n# A tibble: 3,012 × 3\n   date       price  year\n   <date>     <dbl> <dbl>\n 1 2021-12-31  98.8  2021\n 2 2021-12-30  98.8  2021\n 3 2021-12-29  98.4  2021\n 4 2021-12-28  99.0  2021\n 5 2021-12-27  99.1  2021\n 6 2021-12-23  98.9  2021\n 7 2021-12-22  99.3  2021\n 8 2021-12-21  99.2  2021\n 9 2021-12-20  99.5  2021\n10 2021-12-17  99.7  2021\n# … with 3,002 more rows\n\n\nAlternatively, you can use the between function\n\ndat %>% \n  filter(between(year, 2010, 2021))\n\nwhich will produce the same result. Another useful function is called select. While filter is used on the rows of your data, select is for columns. Say we don’t need the year column after having filtered out the years we don’t want. We can then either select the columns we want to keep\n\ndat %>% \n  filter(between(year, 2010, 2021)) %>%\n  select(date, price)\n\nor remove the columns we do not want\n\ndat %>% \n  filter(between(year, 2010, 2021)) %>%\n  select(-year)\n\n# A tibble: 3,012 × 2\n   date       price\n   <date>     <dbl>\n 1 2021-12-31  98.8\n 2 2021-12-30  98.8\n 3 2021-12-29  98.4\n 4 2021-12-28  99.0\n 5 2021-12-27  99.1\n 6 2021-12-23  98.9\n 7 2021-12-22  99.3\n 8 2021-12-21  99.2\n 9 2021-12-20  99.5\n10 2021-12-17  99.7\n# … with 3,002 more rows\n\n\ngroup_by and summarize\nWe are interested in calculating the monthly mean price. In the tidyverse pipeline this means we want to group our observations according to month and year and summarize by month and year the mean of the observations.\n\n(monthlyMeans <- dat %>% \n  filter(between(year,2010,2021)) %>%\n  mutate(monthyear = tsibble::yearmonth(date)) %>%\n  group_by(monthyear) %>%\n  summarize(meanPrice = mean(price)))\n\n# A tibble: 144 × 2\n   monthyear meanPrice\n       <mth>     <dbl>\n 1  2010 Jan      97.3\n 2  2010 Feb      98.7\n 3  2010 Mar      99.2\n 4  2010 Apr      98.4\n 5  2010 May     101. \n 6  2010 Jun     103. \n 7  2010 Jul     104. \n 8  2010 Aug     102. \n 9  2010 Sep      99.9\n10  2010 Oct     101. \n# … with 134 more rows\n\n\nThis pipeline could be read as first we take out observations prior to 1988 and after 2021, then we group the observations according to year and summarize the mean price by year. Note that this operation will delete any columns that are not in the group_by or being calculated in the summarize.\nggplot\nPlotting a data frame is convenient to do using the ggplot2 package. This will (when used appropriately) produce beautiful figures. Let us plot the time series at hand. The ggplot2 follows the same logic with a pipeline, but instead of the %>% operator, we add elements to the figure using +. We need to specify the data object and the name of the x and y columns to be plotted. Everything in the figure that is to vary based on values in the data frame needs to be wrapped in a aes (aesthetic) function (here the x and y arguments). By adding the geom_line() we insert a line.\n\nggplot(data = monthlyMeans, \n       aes(x=monthyear, y = meanPrice)) +\n  geom_line()\n\n\n\n\nWe could instead add geom_point()\n\n ggplot(data = monthlyMeans, \n       aes(x=monthyear, y = meanPrice)) +\n   geom_point()\n\n\n\n\nor do both\n\n ggplot(data = monthlyMeans, \n       aes(x=monthyear, y = meanPrice)) +\n   geom_line() +\n   geom_point()\n\n\n\n\nWe can change the colors and decrease the size of the points:\n\n ggplot(data =monthlyMeans, \n       aes(x=monthyear, y = meanPrice)) +\n   geom_line(color = \"blue\") +\n   geom_point(color = \"green\", size = .2)\n\n\n\n\nOr maybe we do not want to use the default theme: –>\n\n ggplot(data = monthlyMeans, \n       aes(x=monthyear, y = meanPrice)) +\n   geom_line(color = \"blue\") +\n   geom_point(color = \"green\", size = .2) +\n   theme_bw()\n\n\n\n\nWe can also include the plotting in our data manipulation pipeline. For instance, lets summarize the data by year and plot the resulting yearly time series.\n\ndat %>%\n  filter(between(year, 1988, 2021)) %>%\n  group_by(year) %>%\n  summarize(meanPrice = mean(price)) %>%\n  # adding plotting to pipeline:\n  ggplot(aes(x=year, y = meanPrice)) +\n  geom_line(color = \"blue\") +\n  geom_point(color = \"green\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe cannot illustrate all aspects here, but you will learn new elements by studying examples throughout the course. This recap is mostly for remembering the basics of data manipulation in R and simple plotting. As you will see in the continuation, the coding is not much more complex then what you have seen here and the fpp3 package uses the same type of logic and syntax as the tidyverse. There will however be some new functions specific for time series analysis that you will need to learn."
  },
  {
    "objectID": "2_timeseriesgraphics.html",
    "href": "2_timeseriesgraphics.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "In this chapter, we will cover some key concepts of time series that you need to know. You should build up an intuition of what a stationary time series is, and what autocorrelation is. White noise is a simplest time series process you can think of and the building block of ARIMA models. Here we try to give a short introduction to these concepts. Some of these have already been introduced in MET4 at NHH. So if you have taken that course, this may serve as a short repetition.\n\n\nBelow is a video from Aric LaBarr’s youtube series explaing time series concepts in less than 5 minutes. In the video below he explains what time series data is - as opposed to cross sectional data. As he points out, we also some times have a combination which we can e.g. aggregate to cross sectional data or time series data.\n\n\n\n\n\nIt is quite common in statistics in general, but also in time series analysis, to distinguish between capital and non-capital letters. While Y_t denotes the stochastic variable Y at time t, y_t is the realization of said stochastic variable Y_t, i.e. an observed value. Y_t is a specific variable, while \\{Y_t\\} is the entire time series. While a stochastic variable has for instance an expectation, a variance, a dependence structure and a distribution, an observation is just a number.\nLet \\mu_t=\\mu_Y(t)=\\mathrm{E}(Y_t) denote the mean function of Y_t. Assuming that E(Y_t^2)<\\infty, the covariance function of \\{Y_t\\} is denoted \\gamma(r,s)=\\gamma_Y(r,s)= \\mathrm{Cov}(Y_r,Y_s)=\\mathrm{E}(Y_r-\\mu_r)(Y_s-\\mu_s). If the covariance function does not depend on the specific values of r and s, but rather the distance between them h = |r-s|, we write \\gamma(r,s)=\\gamma(h). Likewise, if \\mu_t does not depend on t, we write \\mu_t=\\mu. This will be used in the section about stationarity below. Note that the variance of Y_t is given by \\gamma(t,t) = \\mathrm{Cov}(Y_t,Y_t) = \\mathrm{Var}(Y_t) or \\gamma(0) in the case where the covariance does not depend on t.\n\n\n\nThe tsibble object will be important in this course. Since we are working in the tidyverse, things will be easier if we commit to it. Converting your data to a time series tibble (tsibble) is essential. The tsibble extends the tidy tibble data frame by introducing temporal structure. Read the chapter 2.1 about tsibble objects in the textbook. Here you find several coding examples.\n\n\n\nWe return to Labarr’s time series in 5 minutes series on youtube explaining stationarity. This is a key concept in time series. The video by Labarr gives a nice intuition for the concept. It separates between strong and weak stationarity. In this course, the weak stationarity condition will be the one we focus on, but it is nice to know that there are other definitions of stationarity as well. For a time series to be (weakly) stationary the mean and variance of the time series variable Y_t should not depend on t. More formally, a time series is weakly stationary if\n\nThe expectation is constant: \\mu_t = \\mu\nThe variance is constant and finite: \\sigma_t^2 = \\sigma^2 < \\infty\nThe covariance between two lagged variables only depend on the lag: \\gamma(r,s) = \\gamma(h), where h=|r-s|.\n\n\n\nIn the video, Labarr shows some examples of non-stationary time series which can be transformed to stationary time series by differencing. We will come back to this later in the course, when we talk about transformations and again when we study ARIMA models. We will also study examples of stationary time / non-stationary time series in the voluntary homework. You can also read about the stationarity in the textbook, but for now, you may stop after the paragraph on stationarity. The book is less specific about their definition, but we will stick the weakly stationary definition above.\n\n\n\n\n\nA white noise series is a time series of uncorrelated observations with mean zero and finite variance. We will often write it as Z_t where Z_t \\sim \\mathrm{WN}(0,\\sigma^2). The standard is that the series is uncorrelated, but we may require it to be independent (stronger assumption) and very often normally distributed. In that case we call it iid Gaussian white noise (iid = independent and identically distributed). Notation for this may be Z_t \\sim \\text{iid}\\, \\mathrm{WN}(0,\\sigma^2). Let us generate a white noise series in R.\n\nlibrary(fpp3)\nset.seed(123) # To produce the same output\nwn <- tsibble(\n  t = 1:100,\n  Z = rnorm(100, sd = 3), # Draws from N(0, 3^2) distribution\n  index = \"t\"\n) \nwn %>% \n  autoplot() + \n  labs(title = \"Gaussian White noise\",\n       subtitle = \"iid WN(0,9)\")\n\nPlot variable not specified, automatically selected `.vars = Z`\n\n\n\n\n\nWe can then plot the acf of the series:\n\nwn %>%\n  ACF(Z) %>%\n  autoplot() + \n  labs(title=\"White noise ACF\")\n\n\n\n\nAs you can see, all the correlations fall within the confidence bands. The series is uncorrelated. You can also find a similar example in the textbook."
  },
  {
    "objectID": "3_adjustments.html",
    "href": "3_adjustments.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "In the example in the video above, we are not interested in a proxy for working days per month, and to avoid the effect of this we use the mean (average) production per working day within each month instead of total production per month. The code to generate the example can be found below:\n\n\nCode:\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(fpp3)\n\n# ggplot theme:\ntheme_set(\n  theme_bw() + \n    theme(panel.grid.minor = element_blank(),\n          panel.grid.major = element_blank())\n)\n\n# Daily production: \ndat <- tibble(\n  date = seq(as.Date(\"2015-01-01\"), as.Date(\"2019-12-31\"), by = \"1 day\"),\n  price = pi\n) %>% \n  #Removing the weekends:\n  filter(wday(date, week_start = 1) %in% 1:5) %>%  \n  #Note: We do not remove public holidays, and the worker never takes a day off\n  mutate(YearMonth = yearmonth(date))\n\n# -- TOTAL PRODUCTION FIGURE --\ndat %>%\n  group_by(YearMonth) %>%\n  summarize(`Total production` = sum(price)) %>%\n  as_tsibble(index = \"YearMonth\") %>% \n  ggplot(aes(x = YearMonth, \n             y = `Total production`)) + \n  geom_point(color = \"skyblue\") + \n  geom_line(color = \"skyblue\") + \n  scale_y_continuous(breaks = seq(60, 100, 5),\n                     labels = paste0(\"$\",seq(60, 100, 5),\"k\"), \n                     limits = c(60,75))\n# -- MEAN PRODUCTION FIGURE --\ndat %>% group_by(YearMonth) %>%\n  summarize(`Mean production` = mean(price)) %>%\n  as_tsibble(index = \"YearMonth\") %>% \n  ggplot(aes(x = YearMonth, \n             y = `Mean production`)) + \n  geom_point(color = \"skyblue\") + \n  geom_line(color=\"skyblue\")+ \n  scale_y_continuous(breaks = seq(3, 4, .02), \n                     labels = paste0(\"$\",seq(3, 4, .02),\"k\"))\n\n\n\n\n\nAdjusting for population size is usually a good idea when studying a quantity that is affected by it. The most obvious example is to study GDP (Gross Domestic Product) per capita instead of GDP.\n\n\n\n\nCode:\n# --- Setting up the data --\nscandinaviaUSA <- global_economy %>% \n  filter(Country %in% c(\"Norway\",\"Sweden\",\"Denmark\", \"United States\"))\nscandinaviaUSA %>% head()\n\n# --- GDP by country in $US --\nscandinaviaUSA %>%\n  autoplot(GDP)+\n  labs(title= \"GDP\", y = \"$US\")\n\n# --- Population by country  --\nscandinaviaUSA %>%\n  autoplot(Population)+\n  labs(title= \"Population\", y = \"Number of people\")\n\n# --- Population by country  (log-scale on y-axis) --\nscandinaviaUSA %>% \n  autoplot(Population)+\n  scale_y_log10()+\n  labs(title= \"Population (log-scale)\",y = \"Number of people\")\n\n# --- GDP per capita by country ---\nscandinaviaUSA %>%\n  autoplot(GDP/Population) +\n  labs(title= \"GDP per capita = GDP / Population\", y = \"$US\")\n\n\n\n\n\nAdjusting for inflation is a simple way of taking into account that 5$ in 1950 would get you much more than 5$ would today. This compensation is usually done by a consumer price index, which is standardized to a specific year (in the video below we show examples with 2010 and 2015 as reference years).\nLet Y_t denote the raw time series and Y_t^\\star the inflation adjusted. Let \\text{CPI}_t denote a relevant consumer price index defined to be 100 in the reference year t^\\star. Then Y_t^\\star = Y_t \\cdot \\frac{100}{\\text{CPI}_t}. More generally, we can choose the reference year t^\\star and write this as Y_t^\\star = Y_t \\cdot \\frac{\\text{CPI}_{t^\\star}}{\\text{CPI}_t}.\nThe inflation adjusted series is then measured in the unit “year t^\\star-money”.\n\n\n\n\nCode:\n# --- Inflation adjusted GDP per capita by country ---\nscandinaviaUSA %>%\n  autoplot(GDP/Population *100 / CPI) +\n  labs(title= \"GDP per capita = GDP / Population\", y = \"$US\")\n\n# --- CPI FOR NORWAY (data from Statistics Norway)---\nCPI <- read.csv(\"data/CPI_norway.csv\", sep = \";\") %>% as_tibble() %>%\n  select(1:2) %>%\n  rename(Year = X,CPI = Y.avg2) %>% \n  mutate(Year = as.numeric(Year), CPI = as.numeric(CPI))%>%\n  filter(Year < 2022) %>%\n  as_tsibble(index = Year)\n\n# --- CPI figure ---\nCPI %>% \n  autoplot(CPI, color = \"blue\", lwd = 1.2) +\n  labs(title= \"Consumer Price Index\", y = \"NOK\",\n       subtitle = \"Data source: Statistics Norway\")+\n  geom_hline(yintercept = 100, lty = 2) + geom_vline(xintercept = 2015, lty = 2)+\n  scale_x_continuous(breaks = seq(1925,2025,10))+\n  scale_y_continuous(breaks = seq(0,120,10))\n\n# --- BIG MAC price index ---\nbigMac <- read_csv(\"https://raw.githubusercontent.com/TheEconomist/big-mac-data/master/output-data/big-mac-raw-index.csv\")\nnorBigMac <- bigMac %>% \n  filter(name %in% c(\"Norway\")) %>% \n  mutate(Year = lubridate::year(date)) %>%\n  as_tsibble(index = \"date\")%>%\n  filter(Year <2022) %>%\n  left_join(CPI, by = \"Year\") \n\n# --- BIG MAC price index figure ---\nnorBigMac %>% \n  autoplot(local_price) +\n  labs(title= \"Big Mac price in Norway\", y = \"NOK\",\n       subtitle = \"Data source: The Economist\") +\n  geom_smooth(method = \"lm\", se=FALSE)\n\n#--- Inflation adjusted BIG MAC price index figure ---\nnorBigMac %>%\n  mutate(cpiAdjusted =local_price / CPI * 100)  %>%\n  as_tsibble(index = date) %>%\n  autoplot(cpiAdjusted)+\n  labs(title= \"Inflation adjusted Big Mac price in Norway\", y = \"NOK (2015)\",\n       subtitle = \"Data sources: The Economist (big mac index), Statistics Norway (CPI)\") +\n  geom_smooth(method = \"lm\", se=FALSE)\n\n\n\n\n\nIn many situations it can be necessary to do a mathematical transformation of a time series. There can be different reasons for doing so, but a main one is to make it stationary (or at least more stationary). For instance, if you see that the variation increases or decreases with the level of the series. The most common transformation (for positive time series) is probably using the logarithm. It is often effective and interpretable as changes in the log value correspond to relative changes in the original scale. We write the transformed series, w_t, as w_t =\\log y_t, where y_t is the original time series.\nThe textbook also mentions power transformations of the form w_t = y_t^p (squarte roots - p=\\frac12, cube roots - p=\\frac13, etc). These are not as common to use, but there are situations where these may be better than the logarithm.\nA family of transformations (including log- and a class of power transformations) is the Box-Cox transform. For any value of \\lambda\\in \\mathbb R, \\begin{equation*}\nw_t = \\begin{cases}\n\\log(y_t),&\\text{if }\\lambda = 0;\\\\\n(y_t^\\lambda -1)/\\lambda, &\\text{otherwise}.\n\\end{cases}\n\\end{equation*} As you can see, if \\lambda = 0 we have a simple natural logarithm transform. This version of the Box-Cox transform is also defined for negative values of y_t as long as \\lambda >0.\nThe book has a very nice shiny app for experimenting with different values of \\lambda on a time series of gas production in Australia. We have borrowed it below, but you find it also here. They write that: ” A good value of \\lambda is one which makes the size of the seasonal variation about the same across the whole series, as that makes the forecasting model simpler.” This pretty much sums up why one does mathematical transformations as a preprocessing step before fitting a model - it makes the model simpler.\n\n\nFor financial assets, such as stocks, it is often better to consider the returns rather than the price series. This is also a mathematical transformation and involves differencing. First order differencing means subtracting the previous observation from the present, i.e. w_t=y_t-y_{t-1}. Taking differences is an effective way of potentially making a non-stationary time series stationary. E.g. if a time series has a linear trend: Y_t = \\alpha t + Z_t, where \\alpha is a real constant and Z_t is a white noise, we get that W_t = Y_t-Y_{t-1} = \\alpha t + Z_t - \\alpha(t-1) - Z_{t-1} = Z_t-Z_{t-1} + \\alpha, effectively removing the trend in the transformed series. We will return to this when considering ARIMA models.\nThere are different definitions of returns, but the most common ones are the standard returns, r_t, and log-returns, \\textrm{lr}_t, defined respectively by \\begin{equation*}\n\\begin{split}\nr_t &= \\frac{y_t-y_{t-1}}{y_{t-1}},\\\\\n\\textrm{lr}_t &= \\log y_t-\\log y_{t-1} = \\log\\frac{y_t}{y_{t-1}}.\n\\end{split}\n\\end{equation*} A daily return series for a stock usually has expectation close to zero and little autocorrelation, which can be convenient in many situations. However, they are typically hetereoskedastic (non-constant variance) and the squared returns are often autocorrelated. We will come back to this, when discussing volatility forecasting towards the end of the course.\n\n\nCode\n\n\n# Package for downloading stock data (primarily from Yahoo! Finance)\nlibrary(quantmod)\n\n# -- Download the data: --\ngetSymbols(\"AAPL\")\n\n[1] \"AAPL\"\n\n# -- Extract the closing price and create a tsibble: --\nclose.price <- tibble(\n  close = as.numeric(AAPL$AAPL.Close),\n  date  = time(AAPL)\n) %>% as_tsibble(index = date) \n\n# -- Plot closing price: --\nclose.price %>% autoplot(close) +\n  labs(title = \"Apple Closing price\", \n       y     = \"US$\")\n\n\n\n# -- Adding transformations : -- \nclose.price <- close.price %>% \n  mutate(logclose  = log(close), # log-transform\n         logreturn = c(NA, diff(logclose)), # log returns\n         return    = c(NA, diff(close)/close[-nrow(close.price)]) # Returns\n         ) \n\n\n# -- Box-cox-transform --\nlambda <- close.price %>%\n  features(close, features = guerrero) %>%\n  pull(lambda_guerrero)\nclose.price <- close.price %>%\n  mutate(boxcox = box_cox(close,lambda))\nclose.price %>%\n  autoplot(boxcox) +\n  labs(y = \"\",\n       title = latex2exp::TeX(paste0(\n         \"Transformed apple closing price with $\\\\lambda$ = \",\n         round(lambda,3))))+\n  # Adding red curve with log-transform\n  geom_line(aes(y=logclose), col = 2) \n\n\n\n# -- Plotting the different transformations --\nclose.price %>%\n  pivot_longer(-date) %>% \n  autoplot(value) + \n  facet_wrap(~name, scales=\"free_y\", strip.position = \"left\")+\n  labs(title = \"Apple Closing Price transformations\") +\n  theme(strip.placement = \"outside\")\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\nset.seed(1344)\nlibrary(tidyverse)\nlibrary(fpp3)\ndat <- tibble(\n  date = seq(as.Date(\"2015-01-01\"), as.Date(\"2020-12-31\"), by = \"1 day\"),\n  t = 1:length(date),\n  Tt = 100+9*t/300 + .5*((t-1000)/200)^3+(.87*cos(2*pi*t/1200)+.42*sin(2*pi*t/600))*11,\n  St = (.94*cos(2*pi*t/365) -1.2*sin(2* pi*t/365))*13,\n  Rt = rnorm(length(date), sd = 15),\n  Yt = Tt + St +  Rt\n) \n\n# -- Plotting Y and its components --\ndat %>%\n  pivot_longer(cols = -c(date,t)) %>% \n  mutate(name = factor(name, levels = c(\"Yt\", \"Tt\", \"St\", \"Rt\"))) %>% # to order the panels\n  ggplot(aes(x=date, y = value, col = name)) + \n  geom_line() + facet_wrap(~name, ncol = 1, scales = \"free_y\", strip.position = \"left\") +\n  theme(strip.placement = \"outside\", axis.title = element_blank(), legend.position = \"none\")\n\n\n\n# -- Plotting seasonally adjusted Y --\nggplot(dat, aes(x = date, y = Yt-St)) + \n  geom_line() +\n  labs(title = \"Seasonally adjusted\", x = \"\")"
  },
  {
    "objectID": "3_decomposition.html",
    "href": "3_decomposition.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "In this chapter we will consider different adjusments and transformations one can do prior to a model task. Then we move on to techniques for decomposing a time series into a trend-cycle, season and remainder component.\n\n\n\n\n\n\n\nIn the example in the video above, we are not interested in a proxy for working days per month, and to avoid the effect of this we use the mean (average) production per working day within each month instead of total production per month. The code to generate the example can be found below:\n\n\nCode for generating examples:\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(fpp3)\n\n# ggplot theme:\ntheme_set(\n  theme_bw() + \n    theme(panel.grid.minor = element_blank(),\n          panel.grid.major = element_blank())\n)\n\n# Daily production: \ndat <- tibble(\n  date = seq(as.Date(\"2015-01-01\"), as.Date(\"2019-12-31\"), by = \"1 day\"),\n  price = pi\n) %>% \n  #Removing the weekends:\n  filter(wday(date, week_start = 1) %in% 1:5) %>%  \n  #Note: We do not remove public holidays, and the worker never takes a day off\n  mutate(YearMonth = yearmonth(date))\n\n# -- TOTAL PRODUCTION FIGURE --\ndat %>%\n  group_by(YearMonth) %>%\n  summarize(`Total production` = sum(price)) %>%\n  as_tsibble(index = \"YearMonth\") %>% \n  ggplot(aes(x = YearMonth, \n             y = `Total production`)) + \n  geom_point(color = \"skyblue\") + \n  geom_line(color = \"skyblue\") + \n  scale_y_continuous(breaks = seq(60, 100, 5),\n                     labels = paste0(\"$\",seq(60, 100, 5),\"k\"), \n                     limits = c(60,75))\n# -- MEAN PRODUCTION FIGURE --\ndat %>% group_by(YearMonth) %>%\n  summarize(`Mean production` = mean(price)) %>%\n  as_tsibble(index = \"YearMonth\") %>% \n  ggplot(aes(x = YearMonth, \n             y = `Mean production`)) + \n  geom_point(color = \"skyblue\") + \n  geom_line(color=\"skyblue\")+ \n  scale_y_continuous(breaks = seq(3, 4, .02), \n                     labels = paste0(\"$\",seq(3, 4, .02),\"k\"))\n\n\n\n\n\nAdjusting for population size is usually a good idea when studying a quantity that is affected by it. The most obvious example is to study GDP (Gross Domestic Product) per capita instead of GDP.\n\n\n\n\nCode for generating examples in the video\n# --- Setting up the data --\nscandinaviaUSA <- global_economy %>% \n  filter(Country %in% c(\"Norway\",\"Sweden\",\"Denmark\", \"United States\"))\nscandinaviaUSA %>% head()\n\n# --- GDP by country in $US --\nscandinaviaUSA %>%\n  autoplot(GDP)+\n  labs(title= \"GDP\", y = \"$US\")\n\n# --- Population by country  --\nscandinaviaUSA %>%\n  autoplot(Population)+\n  labs(title= \"Population\", y = \"Number of people\")\n\n# --- Population by country  (log-scale on y-axis) --\nscandinaviaUSA %>% \n  autoplot(Population)+\n  scale_y_log10()+\n  labs(title= \"Population (log-scale)\",y = \"Number of people\")\n\n# --- GDP per capita by country ---\nscandinaviaUSA %>%\n  autoplot(GDP/Population) +\n  labs(title= \"GDP per capita = GDP / Population\", y = \"$US\")\n\n\n\n\n\nAdjusting for inflation is a simple way of taking into account that 5$ in 1950 would get you much more than 5$ would today. This compensation is usually done by a consumer price index, which is standardized to a specific year (in the video below we show examples with 2010 and 2015 as reference years).\nLet Y_t denote the raw time series and Y_t^\\star the inflation adjusted. Let \\text{CPI}_t denote a relevant consumer price index defined to be 100 in the reference year t^\\star. Then Y_t^\\star = Y_t \\cdot \\frac{100}{\\text{CPI}_t}. More generally, we can choose the reference year t^\\star and write this as Y_t^\\star = Y_t \\cdot \\frac{\\text{CPI}_{t^\\star}}{\\text{CPI}_t}.\nThe inflation adjusted series is then measured in the unit “year t^\\star-money”.\n\n\n\n\nCode for generating examples:\n# --- Inflation adjusted GDP per capita by country ---\nscandinaviaUSA %>%\n  autoplot(GDP/Population *100 / CPI) +\n  labs(title= \"GDP per capita = GDP / Population\", y = \"$US\")\n\n# --- CPI FOR NORWAY (data from Statistics Norway)---\nCPI <- read.csv(\"data/CPI_norway.csv\", sep = \";\") %>% as_tibble() %>%\n  select(1:2) %>%\n  rename(Year = X,CPI = Y.avg2) %>% \n  mutate(Year = as.numeric(Year), CPI = as.numeric(CPI))%>%\n  filter(Year < 2022) %>%\n  as_tsibble(index = Year)\n\n# --- CPI figure ---\nCPI %>% \n  autoplot(CPI, color = \"blue\", lwd = 1.2) +\n  labs(title= \"Consumer Price Index\", y = \"NOK\",\n       subtitle = \"Data source: Statistics Norway\")+\n  geom_hline(yintercept = 100, lty = 2) + geom_vline(xintercept = 2015, lty = 2)+\n  scale_x_continuous(breaks = seq(1925,2025,10))+\n  scale_y_continuous(breaks = seq(0,120,10))\n\n# --- BIG MAC price index ---\nbigMac <- read_csv(\"https://raw.githubusercontent.com/TheEconomist/big-mac-data/master/output-data/big-mac-raw-index.csv\")\nnorBigMac <- bigMac %>% \n  filter(name %in% c(\"Norway\")) %>% \n  mutate(Year = lubridate::year(date)) %>%\n  as_tsibble(index = \"date\")%>%\n  filter(Year <2022) %>%\n  left_join(CPI, by = \"Year\") \n\n# --- BIG MAC price index figure ---\nnorBigMac %>% \n  autoplot(local_price) +\n  labs(title= \"Big Mac price in Norway\", y = \"NOK\",\n       subtitle = \"Data source: The Economist\") +\n  geom_smooth(method = \"lm\", se=FALSE)\n\n#--- Inflation adjusted BIG MAC price index figure ---\nnorBigMac %>%\n  mutate(cpiAdjusted =local_price / CPI * 100)  %>%\n  as_tsibble(index = date) %>%\n  autoplot(cpiAdjusted)+\n  labs(title= \"Inflation adjusted Big Mac price in Norway\", y = \"NOK (2015)\",\n       subtitle = \"Data sources: The Economist (big mac index), Statistics Norway (CPI)\") +\n  geom_smooth(method = \"lm\", se=FALSE)\n\n\n\n\n\nIn many situations it can be necessary to do a mathematical transformation of a time series. There can be different reasons for doing so, but a main one is to make it stationary (or at least more stationary). For instance, if you see that the variation increases or decreases with the level of the series. The most common transformation (for positive time series) is probably using the logarithm. It is often effective and interpretable as changes in the log value correspond to relative changes in the original scale. We write the transformed series, w_t, as w_t =\\log y_t, where y_t is the original time series.\nThe textbook also mentions power transformations of the form w_t = y_t^p (squarte roots - p=\\frac12, cube roots - p=\\frac13, etc). These are not as common to use, but there are situations where these may be better than the logarithm.\nA family of transformations (including log- and a class of power transformations) is the Box-Cox transform. For any value of \\lambda\\in \\mathbb R, \\begin{equation*}\nw_t = \\begin{cases}\n\\log(y_t),&\\text{if }\\lambda = 0;\\\\\n(y_t^\\lambda -1)/\\lambda, &\\text{otherwise}.\n\\end{cases}\n\\end{equation*} As you can see, if \\lambda = 0 we have a simple natural logarithm transform. This version of the Box-Cox transform is also defined for negative values of y_t as long as \\lambda >0.\nThe book has a very nice shiny app for experimenting with different values of \\lambda on a time series of gas production in Australia. We have borrowed it below, but you find it also here. They write that: ” A good value of \\lambda is one which makes the size of the seasonal variation about the same across the whole series, as that makes the forecasting model simpler.” This pretty much sums up why one does mathematical transformations as a preprocessing step before fitting a model - it makes the model simpler.\n\n\nFor financial assets, such as stocks, it is often better to consider the returns rather than the price series. This is also a mathematical transformation and involves differencing. First order differencing means subtracting the previous observation from the present, i.e. w_t=y_t-y_{t-1}. Taking differences is an effective way of potentially making a non-stationary time series stationary. E.g. if a time series has a linear trend: Y_t = \\alpha t + Z_t, where \\alpha is a real constant and Z_t is a white noise, we get that W_t = Y_t-Y_{t-1} = \\alpha t + Z_t - \\alpha(t-1) - Z_{t-1} = Z_t-Z_{t-1} + \\alpha, effectively removing the trend in the transformed series. We will return to this when considering ARIMA models.\nThere are different definitions of returns, but the most common ones are the standard returns, r_t, and log-returns, \\textrm{lr}_t, defined respectively by \\begin{equation*}\n\\begin{split}\nr_t &= \\frac{y_t-y_{t-1}}{y_{t-1}},\\\\\n\\textrm{lr}_t &= \\log y_t-\\log y_{t-1} = \\log\\frac{y_t}{y_{t-1}}.\n\\end{split}\n\\end{equation*} A daily return series for a stock usually has expectation close to zero and little autocorrelation, which can be convenient in many situations. However, they are typically hetereoskedastic (non-constant variance) and the squared returns are often autocorrelated. We will come back to this, when discussing volatility forecasting towards the end of the course.\n\n\nCode on mathematical transformations\n\n\n# Package for downloading stock data (primarily from Yahoo! Finance)\nlibrary(quantmod)\n\n# -- Download the data: --\ngetSymbols(\"AAPL\")\n\n[1] \"AAPL\"\n\n# -- Extract the closing price and create a tsibble: --\nclose.price <- tibble(\n  close = as.numeric(AAPL$AAPL.Close),\n  date  = time(AAPL)\n) %>% as_tsibble(index = date) \n\n# -- Plot closing price: --\nclose.price %>% autoplot(close) +\n  labs(title = \"Apple Closing price\", \n       y     = \"US$\")\n\n\n\n# -- Adding transformations : -- \nclose.price <- close.price %>% \n  mutate(logclose  = log(close), # log-transform\n         logreturn = c(NA, diff(logclose)), # log returns\n         return    = c(NA, diff(close)/close[-nrow(close.price)]) # Returns\n         ) \n\n\n# -- Box-cox-transform --\nlambda <- close.price %>%\n  features(close, features = guerrero) %>%\n  pull(lambda_guerrero)\nclose.price <- close.price %>%\n  mutate(boxcox = box_cox(close,lambda))\nclose.price %>%\n  autoplot(boxcox) +\n  labs(y = \"\",\n       title = latex2exp::TeX(paste0(\n         \"Transformed apple closing price with $\\\\lambda$ = \",\n         round(lambda,3))))+\n  # Adding red curve with log-transform\n  geom_line(aes(y=logclose), col = 2) \n\n\n\n# -- Plotting the different transformations --\nclose.price %>%\n  pivot_longer(-date) %>% \n  autoplot(value) + \n  facet_wrap(~name, scales=\"free_y\", strip.position = \"left\")+\n  labs(title = \"Apple Closing Price transformations\") +\n  theme(strip.placement = \"outside\")\n\n\n\n\n\n\n\n\n\n\n\n\nCode on mathematical transformations\n\n\nset.seed(1344)\nlibrary(tidyverse)\nlibrary(fpp3)\ndat <- tibble(\n  date = seq(as.Date(\"2015-01-01\"), as.Date(\"2020-12-31\"), by = \"1 day\"),\n  t = 1:length(date),\n  Tt = 100+9*t/300 + .5*((t-1000)/200)^3+(.87*cos(2*pi*t/1200)+.42*sin(2*pi*t/600))*11,\n  St = (.94*cos(2*pi*t/365) -1.2*sin(2* pi*t/365))*13,\n  Rt = rnorm(length(date), sd = 15),\n  Yt = Tt + St +  Rt\n) \n\n# -- Plotting Y and its components --\ndat %>%\n  pivot_longer(cols = -c(date,t)) %>% \n  mutate(name = factor(name, levels = c(\"Yt\", \"Tt\", \"St\", \"Rt\"))) %>% # to order the panels\n  ggplot(aes(x=date, y = value, col = name)) + \n  geom_line() + facet_wrap(~name, ncol = 1, scales = \"free_y\", strip.position = \"left\") +\n  theme(strip.placement = \"outside\", axis.title = element_blank(), legend.position = \"none\")\n\n\n\n# -- Plotting seasonally adjusted Y --\nggplot(dat, aes(x = date, y = Yt-St)) + \n  geom_line() +\n  labs(title = \"Seasonally adjusted\", x = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\nlibrary(fpp3)\nlibrary(tidyverse)\n\n# ggplot theme: \ntheme_set(theme_bw() + \n            theme(panel.grid.major = element_blank(),\n                  panel.grid.minor = element_blank()))\n\n# -- Read in data: --\ndat <- readxl::read_excel(\n  \"data/NorwayEmployment_15-74years_bySex.xlsx\") %>%\n  as_tibble() %>%\n  mutate(Quarter = str_replace(Quarter, \"K\",\"Q\"),\n         Quarter = yearquarter(Quarter)) \nnames(dat)[3] <- \"Employed\"\n\n# -- Aggregating from Employed by sex to total --\ndat <- dat %>% \n  group_by(Quarter) %>% \n  summarize(Employed = sum(Employed)) %>%\n  as_tsibble(index = Quarter) # Time series table\ndat %>% \n  autoplot(Employed, lwd = 1, colour = \"blue\")\n\n\n\ndat <- dat %>%\n  mutate(\n    `12-MA` = slider::slide_dbl(Employed, mean,\n                                .before = 5, .after = 6, .complete = TRUE),\n    `2x12-MA` = slider::slide_dbl(`12-MA`, mean,\n                                  .before = 1, .after = 0, .complete = TRUE)\n  )\ndat %>%\n  ggplot(aes(x=Quarter, y =Employed))+\n  geom_line(colour = \"gray\") +\n  geom_line(aes(y = `2x12-MA`), colour = \"#D55E00\") +\n  theme_bw()+\n  labs(y = \"Persons (thousands)\",\n       title = \"Total employment in US retail\")\n\nWarning: Removed 12 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\ndat %>% \n  model(\n    classical_decomposition(Employed, type = \"additive\")\n  ) %>%\n  components() %>%\n  autoplot()\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\ndat %>% \n  model(\n    classical = classical_decomposition(Employed, \n                                        type = \"multiplicative\"),\n    x11 = X_13ARIMA_SEATS(Employed ~ x11()),\n    seats = X_13ARIMA_SEATS(Employed ~ seats())\n  ) %>%\n  components() %>% \n  mutate(random = ifelse(.model == \"classical\", \n                         random, \n                         irregular)) %>%\n  autoplot(lwd = 1)\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\ndat %>%\n  model(\n    STL0 = STL(Employed),\n    STL1 = STL(Employed ~ trend(window = 5) + # default 7\n                 season(window = 19),         # default 11\n               robust = FALSE)\n  ) %>%\n  components() %>%\n  autoplot()\n\n\n\n\n\n\n\n\n\nUse the global_economy data, select a country (e.g. Austria). Plot GDP, GDP per capita and GDP per capita inflation adjusted, GDP inflation adjusted.\n\n\n\nSolution\n\n\nlibrary(fpp3)\nlibrary(tidyverse)\ndat <- global_economy %>% \n  filter(Country == \"Austria\")\ndat %>% autoplot(GDP)\n\n\n\ndat %>% autoplot(GDP/Population)\n\n\n\ndat %>% autoplot(GDP/Population * 100/CPI)\n\n\n\ndat %>% autoplot(GDP * 100/CPI)\n\n\n\n\n\n\nIn the global_economy data set, the CPI has a reference year of 2010. Do the necessary changes to inflation adjust GDP per capita with 1990 as reference year.\n\n\n\nSolution\n\n\nlibrary(fpp3)\nlibrary(tidyverse)\ndat <- global_economy %>% \n  filter(Country == \"Austria\")\n\n# -- Extracting the CPI in 1990: --\ncpi1990 <- dat %>% filter(Year ==1990) %>% pull(CPI)\n\n# -- Transforming such that CPI1990 is 100 in 1990: --\ndat <- dat %>% mutate(CPI1990 = CPI / cpi1990 * 100)\n\n# -- Plotting Inflation adjusted GDP per capita: --\ndat %>% autoplot(GDP/Population * 100/CPI1990) + \n  labs(y = \"Inflation adjusted GDP per capita (1990 US$)\") +\n  geom_line(aes(y= GDP/Population * 100/CPI), col = 2)\n\n\n\n# -- Comparing the two CPIs: --\ndat %>% \n  pivot_longer(cols = c(CPI,CPI1990)) %>%\n  ggplot(aes(x = Year, y = value, col = name)) + geom_line()+\n  geom_hline(yintercept = 100) +\n  scale_color_manual(values = c(\"red\",\"blue\"))+\n  geom_segment(x = 1990,xend = 1990, y = -Inf, yend =100, lty = 2, col = \"blue\")+\n  geom_segment(x = 2010,xend = 2010, y = -Inf, yend =100, lty = 2, col = \"red\")+\n  labs(title = \"Differences between CPI with reference year 1990 and 2010\",\n       y = \"Consumer Price Index\") +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\nUse the quantmod package to download data for another stock. What transformation would you prefer for this stock?\nUse the guerrero feature to select a \\lambda for the Box-Cox transformation on the data from the previous exercise."
  },
  {
    "objectID": "3_decomposition_intro.html",
    "href": "3_decomposition_intro.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Decomposition\nIn this chapter we will consider different adjusments and transformations one can do prior to a model task. Then we move on to techniques for decomposing a time series into a trend-cycle, season and remainder component."
  },
  {
    "objectID": "3_exercises.html",
    "href": "3_exercises.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Exercises\n\n\n\nIn addition to the exercises in Chapter 3.7 of the textbook, here are some additional ones with solution. Try to solve them yourself before looking at the solution. The textbook exercise solutions are published on canvas.\n\nUse the simulated data from here. Compare different methods with the truth from the simulation. Are there large differences?\n\n\n\nSolution\n\nSince the simulated data is daily, we cannot use the x11 and seats methods. We will therefore only compare the STL and classical decomposition.\nWe start by re-running the simulation and creating a tsibble.\n\nset.seed(1344)\nlibrary(fpp3)\ntheme_set(theme_bw())\ndat <- tibble(\n  date = seq(as.Date(\"2015-01-01\"), as.Date(\"2020-12-31\"), by = \"1 day\"),\n  t = 1:length(date),\n  Tt = 100+9*t/300 + .5*((t-1000)/200)^3+(.87*cos(2*pi*t/1200)+.42*sin(2*pi*t/600))*11,\n  St = (.94*cos(2*pi*t/365) -1.2*sin(2* pi*t/365))*13,\n  Rt = rnorm(length(date), sd = 15),\n  Yt = Tt + St +  Rt\n) \n\ndat <- as_tsibble(dat, index = date)\n\nThen we do the decomposition, classical and STL:\n\ndecomp <- dat %>% \n  model(\n    classic = classical_decomposition(Yt~season(period = \"1 year\"), type = \"additive\"),\n    stl = STL(Yt~season(period = \"1 year\"), robust = TRUE)\n) %>% components() %>%\n  # For some reason, the classic and stl uses differnt naming conventions:\n  mutate(random = ifelse(is.na(remainder), random, remainder),\n          seasonal = ifelse(is.na(`season_1 year`), seasonal, `season_1 year`))\n# Adjusting the names of dat to fit with the decomp\nnames(dat) <- c(\"date\", \"t\", \"trend\", \"seasonal\", \"random\", \"Yt\")\ndat$.model = \"simulation\"\n# Classical decomposition: \ndecomp %>% filter(.model == \"classic\") %>% autoplot()+ labs(title= \"Classical\")\n\nWarning: Removed 182 rows containing missing values (`geom_line()`).\n\n\n\n\n# STL: \ndecomp %>% filter(.model == \"stl\") %>% autoplot() + labs(title= \"STL\")\n\n\n\n# Simulation: \ndecomp %>% bind_rows(dat) %>% filter(.model == \"simulation\") %>% autoplot()\n\n\n\n# All\ndecomp %>% bind_rows(dat) %>% autoplot()\n\nWarning: Removed 182 rows containing missing values (`geom_line()`).\n\n\n\n\n\nWith the setting I used here, it does not seem that the decomposition methods manage to get as smooth estimate for the seasonal component as we see in the simulation, but for the trend part the fit is almost perfect.\n\n\nUse the global_economy data, select a country (e.g. Austria). Plot GDP, GDP per capita and GDP per capita inflation adjusted, GDP inflation adjusted.\n\n\n\nSolution\n\n\nlibrary(fpp3)\nlibrary(tidyverse)\ndat <- global_economy %>% \n  filter(Country == \"Austria\")\ndat %>% autoplot(GDP)\n\n\n\ndat %>% autoplot(GDP/Population)\n\n\n\ndat %>% autoplot(GDP/Population * 100/CPI)\n\n\n\ndat %>% autoplot(GDP * 100/CPI)\n\n\n\n\n\n\nIn the global_economy data set, the CPI has a reference year of 2010. Do the necessary changes to inflation adjust GDP per capita with 1990 as reference year.\n\n\n\nSolution\n\n\nlibrary(fpp3)\nlibrary(tidyverse)\ndat <- global_economy %>% \n  filter(Country == \"Austria\")\n\n# -- Extracting the CPI in 1990: --\ncpi1990 <- dat %>% filter(Year ==1990) %>% pull(CPI)\n\n# -- Transforming such that CPI1990 is 100 in 1990: --\ndat <- dat %>% mutate(CPI1990 = CPI / cpi1990 * 100)\n\n# -- Plotting Inflation adjusted GDP per capita: --\ndat %>% autoplot(GDP/Population * 100/CPI1990) + \n  labs(y = \"Inflation adjusted GDP per capita (1990 US$)\") +\n  geom_line(aes(y= GDP/Population * 100/CPI), col = 2)\n\n\n\n# -- Comparing the two CPIs: --\ndat %>% \n  pivot_longer(cols = c(CPI,CPI1990)) %>%\n  ggplot(aes(x = Year, y = value, col = name)) + geom_line()+\n  geom_hline(yintercept = 100) +\n  scale_color_manual(values = c(\"red\",\"blue\"))+\n  geom_segment(x = 1990,xend = 1990, y = -Inf, yend =100, lty = 2, col = \"blue\")+\n  geom_segment(x = 2010,xend = 2010, y = -Inf, yend =100, lty = 2, col = \"red\")+\n  labs(title = \"Differences between CPI with reference year 1990 and 2010\",\n       y = \"Consumer Price Index\") +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\nUse the quantmod package to download data for another stock (e.g. Equinor = EQNR). What kind of transformation would you prefer for your stock?\n\n\n\nSolution\n\nI choose the Norwegian company Equinor.\n\nlibrary(quantmod)\n# Equinor: \ngetSymbols(\"EQNR\")\n\n[1] \"EQNR\"\n\nclose.price <- tibble(\n  close = as.numeric(EQNR$EQNR.Close),\n  date  = time(EQNR)\n) %>% as_tsibble(index = date) \n\n# -- Plot closing price: --\nclose.price %>% autoplot(close) +\n  labs(title = \"Equinor Closing price\", \n       y     = \"US$\")\n\n\n\nclose.price <- close.price %>% \n  mutate(logclose  = log(close), # log-transform\n         logreturn = c(NA, diff(logclose)), # log returns\n         return    = c(NA, diff(close)/close[-nrow(close.price)]) # Returns\n         ) \n\nclose.price %>% \n  pivot_longer(-date) %>%\n  ggplot(aes(x=date, y = value))+ geom_line()+ \n  facet_wrap(~name, scales = \"free_y\")\n\n\n\n\nThe return series seem to be good alternatives, also for this stock! The Box-Cox transformation seem to take away some of the trend-cycles in the raw data.\n\n\nUse the guerrero feature to select a \\lambda for the Box-Cox transformation on the data from the previous exercise.\n\n\n\nSolution\n\n\n# -- Box-cox-transform --\nlambda <- close.price %>%\n  features(close, features = guerrero) %>%\n  pull(lambda_guerrero)\nlambda\n\n[1] 0.4714043\n\nclose.price <- close.price %>%\n  mutate(boxcox = box_cox(close,lambda))\nclose.price %>%\n  autoplot(boxcox) +\n  labs(y = \"\",\n       title = latex2exp::TeX(paste0(\n         \"Transformed Equinor closing price with $\\\\lambda$ = \",\n         round(lambda,3))))+\n  # adding the blue close price\n  geom_line(aes(y=close), col = 4)\n\n\n\n\n\n\nImplement the additive classical decomposition method for the Norwegian Wholesale and Retails sales index. To adjust the seasonal component to sum to zero, use the following transformation S_t^\\star = S_t - |S_t| \\cdot \\frac{\\sum_r S_r}{\\sum_r |S_r|} where S_t is the seasonal component before the adjustment and S_t^\\star is the adjusted such that \\sum_{t}S_t^\\star = 0. Compare your results to the output from the function classical_decomposition().\n\n\n\nSolution\n\n\n# Load the data: \nwholesale  <- readRDS(file= \"data/wholesale_and_retails_index_norway.rds\")\n\n# Home-made classical decomposition: \nclassic_decomp <- wholesale %>% \n  rename(Index = `Wholesale and retail sales index`) %>%\n  mutate(`12-MA` = slider::slide_dbl(Index, mean, .before = 5, .after = 6, .complete = TRUE),\n         trend = slider::slide_dbl(`12-MA`, mean, .before = 1, .after = 0, .complete = TRUE), #`2x12-MA` TREND ESTIMATE\n         detrend = Index - trend,\n         MONTH = month(yearmonth)\n         )  %>% \n  group_by(MONTH) %>%\n  mutate(season = mean(detrend, na.rm=T)) %>% # SEASON ESTIMATE\n  ungroup()\n\nTo adjust the seasonal component to sum-to-zero, we update it as S_t^\\star = S_t - |S_t| \\cdot \\frac{\\sum_r S_r}{\\sum_r |S_r|}.\n\n# Adjust season to sum-to-zero\nS <- sum(classic_decomp$season)\nA <- sum(abs(classic_decomp$season))\nclassic_decomp <- classic_decomp %>% \n  mutate(seasonal = season - abs(season)*S/A) %>% \n# Calculate remainder R = Y - T - S\n  mutate(random = Index - trend - seasonal) %>% # REMAINDER\n  ungroup() %>%\n  select(yearmonth, Index, trend, seasonal, random) %>%\n  mutate(.model = \"homemade\")\n# Plot results: \nclassic_decomp %>% \n  pivot_longer(cols =c(-yearmonth,-.model)) %>%\n  mutate(name = factor(name, \n                       levels = c(\"Index\", \"trend\", \"seasonal\", \"random\"))) %>%\nggplot(aes(x= yearmonth, y = value))+geom_line()+\nfacet_wrap( ~ name, ncol = 1, scales = \"free_y\")\n\n\n\n# Let's compare with the automatic one: \nwholesale %>% \n  rename(Index = `Wholesale and retail sales index`) %>%\n  model(\n    classic = classical_decomposition(Index)\n  ) %>%\n  components() %>% \n  bind_rows(classic_decomp) %>% \n  autoplot()\n\n\n\n\nPerfect match!"
  },
  {
    "objectID": "3_timeseriesdecomposition.html",
    "href": "3_timeseriesdecomposition.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Code\n\n\nlibrary(fpp3)\nlibrary(tidyverse)\n\n# ggplot theme: \ntheme_set(theme_bw() + \n            theme(panel.grid.major = element_blank(),\n                  panel.grid.minor = element_blank()))\n\n# -- Read in data: --\ndat <- readxl::read_excel(\n  \"data/NorwayEmployment_15-74years_bySex.xlsx\") %>%\n  as_tibble() %>%\n  mutate(Quarter = str_replace(Quarter, \"K\",\"Q\"),\n         Quarter = yearquarter(Quarter)) \nnames(dat)[3] <- \"Employed\"\n\n# -- Aggregating from Employed by sex to total --\ndat <- dat %>% \n  group_by(Quarter) %>% \n  summarize(Employed = sum(Employed)) %>%\n  as_tsibble(index = Quarter) # Time series table\ndat %>% \n  autoplot(Employed, colour = \"blue\")\n\n\n\ndat <- dat %>%\n  mutate(\n    `12-MA` = slider::slide_dbl(Employed, mean,\n                                .before = 5, .after = 6, .complete = TRUE),\n    `2x12-MA` = slider::slide_dbl(`12-MA`, mean,\n                                  .before = 1, .after = 0, .complete = TRUE)\n  )\ndat %>%\n  ggplot(aes(x=Quarter, y =Employed))+\n  geom_line(colour = \"gray\") +\n  geom_line(aes(y = `2x12-MA`), colour = \"#D55E00\") +\n  theme_bw()+\n  labs(y = \"Persons (thousands)\",\n       title = \"Total employment in US retail\")\n\nWarning: Removed 12 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\ndat %>% \n  model(\n    classical_decomposition(Employed, type = \"additive\")\n  ) %>%\n  components() %>%\n  autoplot()\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\ndat %>% \n  model(\n    classical = classical_decomposition(Employed, \n                                        type = \"multiplicative\"),\n    x11 = X_13ARIMA_SEATS(Employed ~ x11()),\n    seats = X_13ARIMA_SEATS(Employed ~ seats())\n  ) %>%\n  components() %>% \n  mutate(random = ifelse(.model == \"classical\", \n                         random, \n                         irregular)) %>%\n  autoplot()\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\ndat %>%\n  model(\n    STL0 = STL(Employed),\n    STL1 = STL(Employed ~ trend(window = 5) + # default 7\n                 season(window = 19),         # default 11\n               robust = FALSE)\n  ) %>%\n  components() %>%\n  autoplot()"
  },
  {
    "objectID": "4_exercises.html",
    "href": "4_exercises.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Exercises\n\n\n\n\nExercise 1 in Chapter 4.6.\nExercise 2 in Chapter 4.6.\nExercise 3 in Chapter 4.6."
  },
  {
    "objectID": "4_features.html",
    "href": "4_features.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Features\nHere we will go through how to calculate features in R and show some examples of features you can calculate for time series using the feasts package. You find more details in Chapter 4.1-4.3 of Hyndman and Athanasopoulos(2021). The code used in this lecture is an adaptation of what you find in these sections.\n\n\n\n\nCode used in video\n\n\n# --- Time series features ---\nlibrary(fpp3)\nlibrary(tidyverse)\ntheme_set(theme_bw() +\n            theme(panel.grid.minor = element_blank(),\n                  panel.grid.major = element_blank(),\n                  strip.background = element_rect(fill =\"white\",\n                                                  color = \"transparent\")))\n \n# -- Features: mean --\ntourism %>% \n  features(Trips, list(mean = mean)) %>%\n  arrange(mean)\n\n# A tibble: 304 × 4\n   Region          State              Purpose   mean\n   <chr>           <chr>              <chr>    <dbl>\n 1 Kangaroo Island South Australia    Other    0.340\n 2 MacDonnell      Northern Territory Other    0.449\n 3 Wilderness West Tasmania           Other    0.478\n 4 Barkly          Northern Territory Other    0.632\n 5 Clare Valley    South Australia    Other    0.898\n 6 Barossa         South Australia    Other    1.02 \n 7 Kakadu Arnhem   Northern Territory Other    1.04 \n 8 Lasseter        Northern Territory Other    1.14 \n 9 Wimmera         Victoria           Other    1.15 \n10 MacDonnell      Northern Territory Visiting 1.18 \n# … with 294 more rows\n\n# -- Features: mean, sd, 2.5 and 97.5 percentiles --\ntourism %>% \n  features(Trips, list(mean = mean,\n                       sd   = sd,\n                       p    = ~quantile(., probs = c(0.025, 0.975)))) %>%\n  arrange(desc(sd))\n\n# A tibble: 304 × 7\n   Region                 State             Purpose   mean    sd p_2.5…¹ p_97.…²\n   <chr>                  <chr>             <chr>    <dbl> <dbl>   <dbl>   <dbl>\n 1 South Coast            New South Wales   Holiday   495. 170.    285.     812.\n 2 North Coast NSW        New South Wales   Holiday   588. 117.    404.     815.\n 3 Sydney                 New South Wales   Business  602. 117.    402.     884.\n 4 Great Ocean Road       Victoria          Holiday   281. 116.    135.     545.\n 5 Melbourne              Victoria          Holiday   507. 103.    354.     736.\n 6 Peninsula              Victoria          Holiday   185.  96.7    70.8    458.\n 7 Australia's South West Western Australia Holiday   309.  95.3   179.     541.\n 8 Melbourne              Victoria          Visiting  619.  93.6   472.     807.\n 9 Brisbane               Queensland        Visiting  493.  90.6   344.     663.\n10 Sydney                 New South Wales   Visiting  747.  89.6   564.     916.\n# … with 294 more rows, and abbreviated variable names ¹​`p_2.5%`, ²​`p_97.5%`\n\n# -- dplyr equivalent: --\ntourism  %>% \n  as_tibble() %>% \n  group_by(Region,State,Purpose) %>%\n  summarize(mean = mean(Trips)) %>%\n  arrange(mean)\n\n# A tibble: 304 × 4\n# Groups:   Region, State [76]\n   Region          State              Purpose   mean\n   <chr>           <chr>              <chr>    <dbl>\n 1 Kangaroo Island South Australia    Other    0.340\n 2 MacDonnell      Northern Territory Other    0.449\n 3 Wilderness West Tasmania           Other    0.478\n 4 Barkly          Northern Territory Other    0.632\n 5 Clare Valley    South Australia    Other    0.898\n 6 Barossa         South Australia    Other    1.02 \n 7 Kakadu Arnhem   Northern Territory Other    1.04 \n 8 Lasseter        Northern Territory Other    1.14 \n 9 Wimmera         Victoria           Other    1.15 \n10 MacDonnell      Northern Territory Visiting 1.18 \n# … with 294 more rows\n\n# -- ACF features --\ntourism %>% \n  features(Trips, feat_acf) \n\n# A tibble: 304 × 10\n   Region   State Purpose     acf1 acf10 diff1…¹ diff1…² diff2…³ diff2…⁴ seaso…⁵\n   <chr>    <chr> <chr>      <dbl> <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 Adelaide Sout… Busine…  0.0333  0.131  -0.520   0.463  -0.676   0.741  0.201 \n 2 Adelaide Sout… Holiday  0.0456  0.372  -0.343   0.614  -0.487   0.558  0.351 \n 3 Adelaide Sout… Other    0.517   1.15   -0.409   0.383  -0.675   0.792  0.342 \n 4 Adelaide Sout… Visiti…  0.0684  0.294  -0.394   0.452  -0.518   0.447  0.345 \n 5 Adelaid… Sout… Busine…  0.0709  0.134  -0.580   0.415  -0.750   0.746 -0.0628\n 6 Adelaid… Sout… Holiday  0.131   0.313  -0.536   0.500  -0.716   0.906  0.208 \n 7 Adelaid… Sout… Other    0.261   0.330  -0.253   0.317  -0.457   0.392  0.0745\n 8 Adelaid… Sout… Visiti…  0.139   0.117  -0.472   0.239  -0.626   0.408  0.170 \n 9 Alice S… Nort… Busine…  0.217   0.367  -0.500   0.381  -0.658   0.587  0.315 \n10 Alice S… Nort… Holiday -0.00660 2.11   -0.153   2.11   -0.274   1.55   0.729 \n# … with 294 more rows, and abbreviated variable names ¹​diff1_acf1,\n#   ²​diff1_acf10, ³​diff2_acf1, ⁴​diff2_acf10, ⁵​season_acf1\n\n# -- STL features --\ntourism %>%\n  features(Trips, feat_stl) %>%\n  ggplot(aes(x = trend_strength, y=seasonal_strength_year,\n             color = Purpose)) + \n  geom_point() + \n  facet_wrap(vars(State))\n\n\n\n# -- Time series with strongest seasonal component: --\ntourism %>%\n  features(Trips, feat_stl) %>%\n  filter(\n    seasonal_strength_year == max(seasonal_strength_year)\n  ) %>%\n  left_join(tourism, by = c(\"Region\",\"State\",\"Purpose\")) %>%\n  ggplot(aes(x=Quarter, y = Trips)) + geom_line() + \n  facet_grid(vars(Region,State,Purpose))"
  },
  {
    "objectID": "4_timeseriesfeatures.html",
    "href": "4_timeseriesfeatures.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Time series features\nThis chapter is maybe mostly about learning how to use features in time series analysis. It has mostly a technical side that you should learn to master. Calculating features such a means, standard deviation or quantiles is maybe not new to you, but the ACF and STL features I would imagine are. Using features to detect outlyers among many time series is a nice application of the material you should learn in this chapter."
  },
  {
    "objectID": "4_tourismexample.html",
    "href": "4_tourismexample.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Exploring Australian tourism example\nYou find the example in the textbook here: Chapter 4.5. This is a somewhat complicated example to follow perhaps, and in the video below we go through the code line by line (almost) and explain what is going on. You may watch the video or simply study it in the book. The code is not added here, since it is the same as in the book (more or less)."
  },
  {
    "objectID": "5_forecasterstoolbox.html",
    "href": "5_forecasterstoolbox.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Forecasters toolbox"
  },
  {
    "objectID": "6_judgementalforecast.html",
    "href": "6_judgementalforecast.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "In this chapter, we go through chapter 6 of the textbook (Hyndman and Athanasopoulos, 2021). This material is mostly for you to be aware of and be able do discuss different aspects of the judgmental forecasting methods. We will through short videos describe some general aspects and specific methods. Particularly, the Delphi method and forecasting by analogy will be described in some detail, while other methods are mentioned. For more details, see the referred chapter 6 in the textbook."
  },
  {
    "objectID": "7_regressionmodels.html",
    "href": "7_regressionmodels.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Regression models"
  },
  {
    "objectID": "8_ARIMAmodels.html",
    "href": "8_ARIMAmodels.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "The following shiny app has been developed by Sondre Hølleland and being administred at https://sholleland.shinyapps.io/ban430_shinyapps. Due to restrictions relating to available computing hours on the free shinyapps account, the app may not work. You may then copy the code below to run the shiny app locally on your own computer. Remember that understanding the details of this code is not necessary.\n\n\nShiny app code:\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(fpp3)\nlibrary(ggpubr)\nsbp.width <- 3\n\ntheme_set(theme_bw() + theme(panel.grid.major = element_blank(),\n                             panel.grid.minor = element_blank()))\n# Define UI for application that draws a histogram\nui <- fluidPage(\n\n    # Application title\n    titlePanel(\"ARMA models\"),\n    tabsetPanel(\n      tabPanel(\"AR(1)\", \n               \n               sidebarLayout(\n                 sidebarPanel(width = sbp.width,\n                              uiOutput(\"artext\"),\n                   sliderInput(\"arphi\",\n                               \"phi:\",\n                               min = -.99,\n                               max = .99,\n                               value = .7,\n                               step = .01),\n                   sliderInput(\"arsigma\",\n                               \"sigma:\",\n                               min = 0,\n                               max = 10,\n                               value = 1,\n                               step = .2),\n                   numericInput(\"arseed\",\n                                \"Seed:\",\n                                value = 1234,\n                                min = 0, \n                                max = 9999,\n                                step = 1),\n                   numericInput(\"arn\",\n                                \"Sample size:\",\n                                value = 500,\n                                min = 100, \n                                max = 5000,\n                                step = 100)\n                 ),\n                 \n                 # Show a plot of the generated distribution\n                 mainPanel(\n                   plotOutput(\"arPlot\", height = \"600px\")\n                 )\n               )),\n      tabPanel(\"MA(1)\",  \n               \n               sidebarLayout(\n                 sidebarPanel(width = sbp.width,\n                              uiOutput(\"matext\"),\n                   sliderInput(\"matheta\",\n                               \"theta:\",\n                               min = -1,\n                               max = 1,\n                               value = .7,\n                               step = .01),\n                   sliderInput(\"masigma\",\n                               \"sigma:\",\n                               min = 0,\n                               max = 10,\n                               value = 1,\n                               step = .2),\n                   numericInput(\"maseed\",\n                                \"Seed:\",\n                                value = 1234,\n                                min = 0, \n                                max = 9999,\n                                step = 1),\n                   numericInput(\"man\",\n                                \"Sample size:\",\n                                value = 500,\n                                min = 100, \n                                max = 5000,\n                                step = 100)\n                 ),\n                 \n                 # Show a plot of the generated distribution\n                 mainPanel(\n                   plotOutput(\"maPlot\", height = \"600px\")\n                 )\n               )),\n      tabPanel(\"ARMA(1,1)\", \n              \n               sidebarLayout(\n                 sidebarPanel(width = sbp.width,\n                              uiOutput(\"armatext\"),\n                   sliderInput(\"armaphi\",\n                               \"phi:\",\n                               min = -.99,\n                               max = .99,\n                               value = .7,\n                               step = .01),\n                   sliderInput(\"armatheta\",\n                               \"theta:\",\n                               min = -1,\n                               max = 1,\n                               value = .7,\n                               step = .01),\n                   sliderInput(\"armasigma\",\n                               \"sigma:\",\n                               min = 0,\n                               max = 10,\n                               value = 1,\n                               step = .2),\n                   numericInput(\"armaseed\",\n                                \"Seed:\",\n                                value = 1234,\n                                min = 0, \n                                max = 9999,\n                                step = 1),\n                   numericInput(\"arman\",\n                                \"Sample size:\",\n                                value = 500,\n                                min = 100, \n                                max = 5000,\n                                step = 100)\n                 ),\n                 \n                 # Show a plot of the generated distribution\n                 mainPanel(\n                   plotOutput(\"armaPlot\", height = \"600px\")\n                 )\n               ))))\n    # Sidebar with a slider input for number of bins \n   \n\n\n# Define server logic required to draw a histogram\nserver <- function(input, output) {\n    output$artext <- renderUI({\n      withMathJax(\"Model: $$Y_t=\\\\phi \\\\,Y_{t-1}+Z_t,\\\\quad Z_t\\\\sim \\\\text{i.i.d N}(0,\\\\sigma^2)$$\")\n    })\n    output$matext <- renderUI({\n      withMathJax(\"Model: $$Y_t=\\\\theta\\\\, Z_{t-1}+Z_t,\\\\quad Z_t\\\\sim \\\\text{i.i.d N}(0,\\\\sigma^2)$$\")\n    })\n    output$armatext <- renderUI({\n      withMathJax(\"Model: $$Y_t=\\\\phi\\\\, Y_{t-1}+\\\\theta \\\\,Z_{t-1}+Z_t,\\\\quad Z_t\\\\sim \\\\text{i.i.d N}(0,\\\\sigma^2)$$\")\n    })\n    output$arPlot <- renderPlot({\n      set.seed(input$arseed)\n      burnin <- 200\n      x <- rnorm(input$arn+burnin, sd = input$arsigma)\n      for(i in 2:length(x))\n        x[i] <- input$arphi*x[i-1]+rnorm(1, sd = input$arsigma)\n      df <- tsibble(x = x[burnin+1:(length(x)-burnin)],\n                    t = 1:(length(x)-burnin),\n                    index = \"t\")\n      # Theoretical acf: \n      \n      theoretical.correlations <- \n        tibble(lag = 1:29,\n               acf = ARMAacf(ar = c(input$arphi), lag.max = 29, pacf =FALSE)[-1],\n               pacf = ARMAacf(ar = c(input$arphi), lag.max = 29, pacf =TRUE))\n      # generate bins based on input$bins from ui.R\n       ggarrange(\n         df %>% autoplot() + scale_y_continuous(\"AR(1) series\")+\n           scale_x_continuous(\"Time index\", expand = c(0,0)),\n         ggarrange(df %>% ACF() %>% autoplot()+\n                     scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                     ylab(\"Sample PACF\"),\n         df %>% PACF() %>% autoplot()+\n           scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n           ylab(\"Sample PACF\"), \n         theoretical.correlations %>% ggplot(aes(x=lag))+\n           geom_segment(aes(x=lag,xend=lag,y=0,yend=acf))+geom_hline(yintercept=0)+\n           scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n           scale_y_continuous(\"Theoretical ACF\"),\n         theoretical.correlations %>% ggplot(aes(x=lag))+\n           geom_segment(aes(x=lag,xend=lag,y=0,yend=pacf))+geom_hline(yintercept=0)+\n           scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n           scale_y_continuous(\"Theoretical PACF\"),\n         nrow = 2, ncol = 2),\n         ncol = 1, nrow = 2, heights = c(1,2))\n    })\n\n    output$maPlot <- renderPlot({\n      set.seed(input$maseed)\n      burnin <- 200\n      z <- rnorm(input$man+burnin, sd = input$masigma)\n      x <- numeric(input$man+burnin)\n      for(i in 2:length(x))\n        x[i] <- input$matheta*z[i-1]+z[i]\n      df <- tsibble(x = x[burnin+1:(length(x)-burnin)],\n                    t = 1:(length(x)-burnin),\n                    index = \"t\")\n      \n      theoretical.correlations <- \n        tibble(lag = 1:29,\n               acf = ARMAacf(ma = c(input$matheta), lag.max = 29, pacf =FALSE)[-1],\n               pacf = ARMAacf(ma = c(input$matheta), lag.max = 29, pacf =TRUE))\n      # generate bins based on input$bins from ui.R\n      ggarrange(\n        df %>% autoplot() + scale_y_continuous(\"MA(1) series\")+\n          scale_x_continuous(\"Time index\", expand = c(0,0)),\n        ggarrange(df %>% ACF() %>% autoplot()+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                                         ylab(\"Sample ACF\"),\n                  df %>% PACF() %>% autoplot()+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                                         ylab(\"Sample PACF\"), \n                  theoretical.correlations %>% ggplot(aes(x=lag))+\n                    geom_segment(aes(x=lag,xend=lag,y=0,yend=acf))+geom_hline(yintercept=0)+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                    scale_y_continuous(\"Theoretical ACF\"),\n                  theoretical.correlations %>% ggplot(aes(x=lag))+\n                    geom_segment(aes(x=lag,xend=lag,y=0,yend=pacf))+geom_hline(yintercept=0)+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                    scale_y_continuous(\"Theoretical PACF\"),\n                  nrow = 2, ncol = 2),\n        ncol = 1, nrow = 2, heights = c(1,2))\n    })\n    output$armaPlot <- renderPlot({\n      set.seed(input$armaseed)\n      burnin <- 200\n      #sdZ = sqrt(input$armasigma *(1-input$armaphi^2)/(1+2*input$armaphi*input$armatheta + input$armatheta^2))\n      z <- rnorm(input$arman+burnin, sd = input$armasigma)\n      x <- numeric(input$arman+burnin)\n      for(i in 2:length(x))\n        x[i] <- input$armaphi*x[i-1]+input$armatheta*z[i-1]+z[i]\n      df <- tsibble(x = x[burnin+1:(length(x)-burnin)],\n                    t = 1:(length(x)-burnin),\n                    index = \"t\")\n      \n      theoretical.correlations <- \n        tibble(lag = 1:29,\n               acf = ARMAacf(ar = c(input$armaphi), ma = c(input$armatheta), lag.max = 29, pacf =FALSE)[-1],\n               pacf = ARMAacf(ar = c(input$armaphi), ma = c(input$armatheta), lag.max = 29, pacf =TRUE))\n      # generate bins based on input$bins from ui.R\n      ggarrange(\n        df %>% autoplot() + scale_y_continuous(\"MA(1) series\")+\n          scale_x_continuous(\"Time index\", expand = c(0,0)),\n        ggarrange(df %>% ACF() %>% autoplot()+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                                         ylab(\"Sample ACF\"),\n                  df %>% PACF() %>% autoplot()+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                                         ylab(\"Sample PACF\"), \n                  theoretical.correlations %>% ggplot(aes(x=lag))+\n                    geom_segment(aes(x=lag,xend=lag,y=0,yend=acf))+geom_hline(yintercept=0)+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                    scale_y_continuous(\"Theoretical ACF\"),\n                  theoretical.correlations %>% ggplot(aes(x=lag))+\n                    geom_segment(aes(x=lag,xend=lag,y=0,yend=pacf))+geom_hline(yintercept=0)+\n                    scale_x_continuous(\"Lag\", expand = c(0,0),limit=c(0.5,29.5), breaks = seq(0,30,5))+\n                    scale_y_continuous(\"Theoretical PACF\"),\n                  nrow = 2, ncol = 2),\n        ncol = 1, nrow = 2, heights = c(1,2))\n    })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "9_Volatitiliy_forecasting.html",
    "href": "9_Volatitiliy_forecasting.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Volatility forecasting"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This course and website has been developed by Sondre Hølleland, assistant professor at Norwegian School of Economics, Department of Business and Management Science. More about me will be added."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Welcome to the website for BAN430 Forecasting. We will use this website as a supplement to lectures. The website is an ongoing development, so not all subjects will have content yet. Below you will find a detailed (preliminary) lecture plan, link to the textbook and curriculum.\n\n\n\n\n\n\n \n  \n    Monday 14:15-16:00 \n    Thursday 12:15-14:00 \n  \n \n\n  \n    16.01.2023 \n    19.01.2023 \n  \n  \n    Introduction lecture \n    R recap \n  \n  \n    23.01.2023 \n    26.01.2023 \n  \n  \n    Time series graphics \n    Time series decomposition (summary) \n  \n  \n    30.01.2023 \n    02.02.2023 \n  \n  \n    Forecasters toolbox \n    Forecasters toolbox \n  \n  \n    06.02.2023 \n    09.02.2023 \n  \n  \n    Regression models \n    Regression models \n  \n  \n    13.02.2023 \n    16.02.2023 \n  \n  \n    Exponential smoothing \n    Exponential smoothing \n  \n  \n    20.02.2023 \n    23.02.2023 \n  \n  \n    No lecture: Selfstudy time series features (website) \n    No lecture: Selfstudy judgmental forecasts (website) \n  \n  \n    27.02.2023 \n    02.03.2023 \n  \n  \n    ARIMA \n    ARIMA \n  \n  \n    06.03.2023 \n    09.03.2023 \n  \n  \n    ARIMA \n    No lecture / selfstudy \n  \n  \n    13.03.2023 \n    16.03.2023 \n  \n  \n    ARIMA \n    Dynamic regresion models \n  \n  \n    20.03.2023 \n    23.03.2023 \n  \n  \n    Dynamic regression models \n    No lecture / selfstudy \n  \n  \n    27.03.2023 \n    30.03.2023 \n  \n  \n    Volatility forecasting \n    Volatility forecasting \n  \n  \n    03.04.2023 \n    06.04.2023 \n  \n  \n    Easter holiday? \n    Easter holiday \n  \n  \n    10.04.2023 \n    13.04.2023 \n  \n  \n    Easter holiday \n    Summary lecture \n  \n  \n    Study period prior to exam \n     \n  \n  \n    08.05.2023 \n     \n  \n  \n    8 hour home exam \n     \n  \n\n\n\n\n\n\n\n\n\n\n\nFPP Third edition\n\n\nWe will use the textbook Forecasting: Principles and Practice, 3rd edition, by Hyndman and Athanasopoulos, i.e. the online version which can be accessed at https://otexts.com/fpp3/.\n\n\n\nTextbook Hyndman and Athanasopoulos(2021) chapters 1-10 and 13. Additional notes by lecturer on volatility forecasting. All the material on this website."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "W1_traffic.html",
    "href": "W1_traffic.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "In this workshop we will build a model for forecasting traffic in Bergen’s busiest intersection Danmarksplass. The data is publicly available at vegvesen.no/trafikkdata and published by The Norwegian Public Roads Administration. Forecasting traffic amount could be useful for anyone wanting to avoid traffic jams, but also important for government agencies with responsibilities related to road planning or public transport.\nAs you may imagine the amount of traffic varies depending on things such as time of day, which day of the week it is and public holidays. The peak hours are mostly associated with people traveling to or home from work, while the least busy hours will be in the middle of the night. In this exercise you will use raw observations from The Norwegian Public Roads Administration to forecast traffic passing through one of Bergen’s traffic bottlenecks.\nHopefully you will learn about\n\nGraphical presentation of a time series\nBuilding a linear time series regression model\nUseful transformations\nUsing dummy variables for seasonality\nUsing fourier sequences for seasonality\nExternal predictor variables\n\n\n\n\nThe data you will be using consists of the two columns\n\ndatetime: date and hour of observation\nvehicles: the number of vehicles that has passed a sensor in the road the previous hour\n\nThe observations are hourly and start from January 2018 to February 1st 2023 and are all from Danmarksplass (near the charging station). We add another location as predictor for Danmarksplass later in the case study to see if this may inform our forecasting model, but this data has the same structure (except that the vehicle column has a different name).\n\n\n\n\nLoad the data into your working environment. Make sure the columns are correctly formatted.\n\n\ndanmarksplass <- readr::read_csv(\n  \"https://raw.githubusercontent.com/holleland/BAN430/master/data/traffic_danmarksplass.csv\")\n\n\n\n\n\nConvert the data to a tsibble and make a time plot.\n\n\n\n\n\nCreate a training data set. We will use January 2023 as our test set. Make a time plot with one panel per year for the training data. Can you detect any particular patterns or deviations from the patterns?\n\n\n\nCode hint:\ntrain <- danmarksplass %>% filter(year(datetime) < 2023)\n\n\n\nUsing different graphics, visualize the training data. Look for trends and seasons on multiple scales.\nIn particular, create this graphic. What do you detect here?\n\n\n\nCode solution:\ntrain %>% mutate(wday = wday(datetime, label =T, abbr = FALSE, week_start=1), \n                 hour = hour(datetime),\n                 date=date(datetime)) %>%\n  ggplot(aes(x = hour, y = vehicles, col = date, group = date)) + geom_line() +\n  facet_wrap( ~ wday)+ scale_color_date(low = \"blue\", high = \"magenta\")\n\n\n\n\n\n\nAggregate the data to total traffic by year-week and do a STL decomposition. Do you have an plausible explanation for the estimated trend? Try reducing the trend window (default is 91).\n\n\n\nCode hint:\ntrain %>% \n  index_by(date = ~yearweek(.)) %>% \n  summarize(vehicles = sum(vehicles,na.rm=T))%>% \n  model(stl = STL(vehicles ~ trend(window =91))\n\n\n\nFit a TSLM with weekly season as predictor (dummy variables for every hour of the week). Fit one model without transforming and one where you log-transform the forecast variable. Forecast for the first week of 2023 and plot it for the two models. Would you prefer to use the log-transform or no transform? Explain why.\n\n\n\nCode hint\nlog(vehicle) ~ season(period = \"week\")\n\n\n\nIn the model above, we used 24\\cdot 7= 168 predictors. We can do this, because we have so many data points. However, could we achieve something similar by using a fourier series? If K<168/2 = 84, this will reduce the number of parameters. Use the same transformation as you favored above and compare the model with a model using fourier predictors for the weekly season. Try different values of K and forecast for the first week of 2023. Evaluate the accuracy of the two models with AIC, AICc and CV.\n\n\n\nCode hint:\nvehicles ~ fourier(K = 65, period = \"week\")\n\n\n\nBased on the number of observations here (T\\approx 45\\,000), which of AIC, AICc or CV would you prefer to use and why?\nOne issue you might see is that the model over-estimates the traffic on January 1st (which in 2023 is a Sunday, but it is also a public holiday). Could there be a public holiday effect? By loading the csv linked below, you will have a list of Norwegian holidays from 2008-Jan 2023. Add a dummy variable to your model of choice for public holiday (1 if holiday, 0 otherwise). Does this improve the model fit measures (AIC)?\n\n\nholidays <- read_csv(\n  \"https://raw.githubusercontent.com/holleland/BAN430/master/data/NorwegianHolidays_2008-jan2023.csv\")$x\n\n\n\nCode hint:\nholiday <- readr::read_csv(\n  \"https://raw.githubusercontent.com/holleland/BAN430/master/data/NorwegianHolidays_2008-jan2023.csv\")$x\ndanmarksplass <- danmarksplass %>% mutate(\n  holiday = factor(ifelse(date(datetime) %in% holidays, 1, 0))\n)\ntrain <- danmarksplass %>% filter(year(datetime) < 2023)\ntrain %>% \n  model(holiday = TSLM(log(vehicle) ~ factor(holiday) + ...))\n\n\n\nAre there other dummies you could think of adding? Special events that might impact the traffic on a certain day or during a period (spike dummy) or permanently changing it from a certain point in time (step dummy)? Discuss.\nFit a seasonal naive as benchmark model and compare it to the best model so far using RMSE, MAE, MAPE and MASE on the test set.\n\n\n\nCode hint:\nfit <- model(\n  m1 = ...,\n  snaive = SNAIVE(...)\n)\naccuracy(fit)\n\n\n\nOne hypothesis is that some of the traffic at Danmarkplass can be explained by the arrival of ferries at Halhjem, creating a peak in the number of cars arriving. This will only contribute to the traffic heading North, but let us test by adding a predictor for traffic at Moberg v/Lekven (a station on the E39 immediately after the ferry). We will only use traffic towards Bergen here. The traffic heading North at Moberg can be found in traffic_moberg.csv (link below). Join it with the danmarksplass data (one tsibble with both columns). At Moberg there are some zero measurement. It may be useful to use log(1+moberg). Since it takes some time to drive from Moberg to Danmarksplass it may also be useful to use lagged observations at Moberg (hint: Use lag() function). Evaluate whether adding Moberg improves AIC.\n\n\nmoberg <- readr::read_csv(\"https://raw.githubusercontent.com/holleland/BAN430/master/data/traffic_moberg.csv\") \n\n\n\nCode hint:\ndanmarksplass <- left_join(danmarksplass,\n                           moberg, \n                           by = \"datetime\")\ntrain <- danmarksplass %>% filter(year(datetime) < 2023)\ntrain %>% \n  model(holiday = TSLM(log(vehicle) ~ lag(log(1+moberg)) + factor(holiday) + ...))\n\n\n\nThe way you have implemented the Moberg traffic in your forecast model, is it an ex ante or ex post forecast?\nLet us have a look at the residuals. Comment on potential issues.\nIncrease the forecast interval to include all of January 2023. Does this change any of the conclusions about the models? How does the forecast look?\nCan you think of other predictors to add to the model. Can you improve the model performance further?"
  },
  {
    "objectID": "W2_electricityprices.html",
    "href": "W2_electricityprices.html",
    "title": "BAN430 Forecasting",
    "section": "",
    "text": "Traditionally, Norwegian electricity consumers have been blessed with low electricity prices due to the vast access to hydro power from the many Norwegian waterfalls. This has been a competitive advantage for Norwegian industry. But some time ago this changed and we have seen a large increase in electricity prices. This has led the Norwegian government to implement a monetary support scheme for Norwegian private households. The scheme is based on the monthly average price and covers 90% above 0.70 NOK/kWh excluding taxes (mva). The problem with this scheme is that the monthly average price is not known before the month has ended, so at the time of consumption, the consumers of electricity do not know how much governmental support will be and there how expensive the power is. Since the scheme does not take into account the spot price there can be times with negative prices after taking into account the governmental support. Say the monthly average is 2.7 NOK, then the support will be 0.90\\cdot (2.7-0.7) = 1.80 NOK/kWh.\nIt can therefore be useful to have a forecast model to have an idea of what the governmental support will be. Towards the end of the month we will have a quite good idea of what the average price will be, but at the beginning of the month it will be quite uncertain.\nMedia houses such as VG and Aftenposten present estimates of the support by simple moving averages (so far in the month), but Martin Jullum, a statistician at the Norwegian Computing Center, has developed a more sophisticated forecasting model presented at dinstrompris.no (only in Norwegian). The approach he is using is based on dynamic regression model with ARIMA errors that simulates the rest of the month 10 000 times and calculates an average based on that. In this exercise, we will end up with a version of his model.\nAlthough we may have negative electricity prices (free electricity!) for a few hours in a day, this is highly unlikely to happen on the daily average price and has not happened in the data we will be using. You may therefore assume non-negative daily electricity price if you find that necessary.\n\n\n\nWe download the data from Martin Jullum’s github repository using the following code\n\nelprices <- readr::read_csv(\n  \"https://raw.githubusercontent.com/martinju/stromstotte/master/data/database_nordpool_daily.csv\")\n\nNote that the data is updated daily. The data contains three columns:\n\narea: Nordpool price region (NO1, NO2, NO3, NO4 or NO5)\ndate: date in %Y-%m-%d format\nprice: Average daily price in NOK per kWh\n\nWestern Norway is pricing region NO5.\nThe goal of this workshop is to build a model that can forecast the monthly average electricity price for February 2023 using daily observations. From that, we can also give an estimate of the Governmental support scheme. We will use January 2023 as our test set (for comparing different model classes) and February 2023 is the month we will apply our forecast model on.\n\nLoad the data and select only pricing area NO5. Create a training set containing observations up to and including Dec 31st 2022 and a test set with observations in January 2023. Also create a set called feb23 containing February 2023. All these data sets should be tsibble objects.\n\n\n\n\n\nExplore the data using suitable graphics. Look for trends and season and comment on what you find.\nDo a decomposition using a suitable decomposition method with weekly seasonality. Compare different options for the decomposition method. Comment on the trend and seasonal component. Are they reasonable? Why? Or why not?\n\nWe know that electricity demand is correlated with temperature. On colder days we need more heating which increase demand. Increased demand should also increase the price of electricity.\n\nCould daily average temperature in Bergen (at Florida) be a potential predictor candidate for the electricity price? Below is a scatterplot with price on the y-axis and Bergen average temperature on the x-axis. Comment on the relationship. Would forecasting with temperature as predictor be an ex ante or ex post forecast? Elaborate.\n\n\n\n\n\n\n\n\n\nIn this exercise we focus on ETS and ARIMA models and use AICc as our model selection criteria within each model family. We first seek to find the best ETS model and do ARIMA afterwards.\n\n\n\nUse the automated ETS model selection in the fable package and print the report. Write up the model equations with estimated values.\nPlot the residuals ACF and histogram. Are there any problems with these residuals? Do a Ljung-Box test including lags up to 10.\n\n\n\n\n\nEstimate the optimal \\lambda parameter using the Guerero method. Based on the estimated \\lambda parameter, is the box-cox transformation far from a log-transform? Make a plot including the box-cox transformed and log transformed. Include also the log(1+price) transformation.\n\nWe could have compared the different transformations by evaluating their performance on a test set, but for simplicity we stick to the log(1+price) transform. What is the main consequence for the forecasting model based on this assumption?\n\nWe have decided on a suitable transformation. Is the transformed time series stationary? Is differencing necessary? Use the unitroot_nsdiffs and unitroot_ndiffs to select the number of differences. Plot the differenced series and visually confirm that the differenced and transformed series is stationary using gg_tsdisplay() with plot_type = “partial”.\nSuggest some model candidates based on the ACF and PACF plots in the previous exercise. Fit the model candidates along with three automatically selected models:\n\n\nStepwise = TRUE and approx = TRUE\nStepwise = FALSE and approx = TRUE\nStepwise = FALSE and approx = FALSE\n\nCompare the methods using AICc as criteria. Which model would you choose? Create a fit object with only this model and call it fit.\n\nCheck the model assumptions using the gg_tsresiduals() function on the fit object from the selected model. Perform a Ljung-Box test and conclude. Are the assumptions fulfilled? What are the required and “nice-to-have” assumptions? Would you assume normality for producing prediction intervals in this case?\nPrint the report for your selected model. Based on the report, set up the model equation using the backshift notation.\nBased on the equation in the previous exercise, find an expression for the first point forecast (\\widehat y_{T+1|T}).\nMartin Jullumn’s model on dinstrompris.no is a dynamic regression model with a categorical weekday dummy and ARIMA errors. Fit such a model a compare it with the ARIMA model of choice so far using an accuracy measure of choice (e.g. RMSE). Use a 4 weeks forecast horizon (same as length of February). Jullum do not use any transformations in his model (as far as I can see) so our model will be different. See code hint below for how this model can be implemented. Check the residual plots for the best model.\n\n\n\nCode hint:\nmodel(\n  jullum = ARIMA(log(1+price) ~ wday(date, label = TRUE), approx = FALSE, stepwise = FALSE)\n)\n\n\n\n\n\nWe will now set up a Monte Carlo simulation for forecasting the average monthly electricity price for February. We simulate daily February prices from the best model and caluclate the monthly average price. This gives us a simulated sample of monthly average price of February and we can estimate the forecast distribution of that variable.\n\nWe continue with the best model based on the test set performance above. We have found our forecasting model, now it is time to set it into action. Imagine that it is January 31^{\\text{st}} 2023 today, and we want to forecast the monthly average electricity price for February 2023. We will complicate this further in the next question, but start refitting the selected model using observations including Jan 31st 2023. Plot a forecast for daily prices in February. Does the forecast behave as you expect? Generate 500 potential February prices from the fitted model. Calculate the monthly average price for each of the simulations (should give 500 average February prices). Present the mean, standard deviation and 95% prediction interval for the monthly average price.\n\n\n\nCode hint:\nfit %>% forecast(h = 28) %>% autoplot()\n# Simulate: \nsims <- fit %>%  generate(h = 28, times = 500)\nsims %>% \n  index_by(month = ~yearmonth(.)) %>% \n  group_by(.rep) %>%  \n  # Calculate monthly average price for the simulations\n  summarize(average = mean(.sim)) %>% \n  ungroup() %>% \n  # Monte Carlo estimates of the distribution: \n  summarize(\n    mean = mean(average),\n    sd   = sd(average),\n    lwr  = quantile(average, prob = 0.025),\n    upr  = quantile(average, prob = 0.975)\n  )\n\n\n\nIn this final exercise, we pretend that the we are doing the same thing as dinstrompris.no. We start from Jan 31st of 2023 with the mean, standard deviation and 95% prediction interval from exercise 14 (i.e. the forecast of the average price for February). Then a day goes by, it is now Feb 1st and we know the daily average price of Feb 1st, so we update our model with the new information and produce a new forecast for the remaining 27 days of the month and produce a mean, sd, prediction interval for the average price of the full month, where 1 day is observed and the rest is simulated. Continue adding days and updating the forecast until all days of February are observed. Create a nice graphic similar to the one below. Explain the behavior of the forecast we see in the figure.\n\nPS: The last exercise is extra challenging from a programming point of view. If you don’t know how to attack this, you may just comment on the figure.\n The red dashed line is the actual average price of February and the date on the x-axis is the first day of the forecasting horizon (Feb 1st means data up to Jan 31st is used)."
  }
]